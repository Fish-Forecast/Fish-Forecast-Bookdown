## Metrics

How to we quantify the difference between the forecast and the actual values in the test data set?

Let's take the example of a training set/test set.

```{r echo=FALSE}
traindat <- window(anchovyts, 1964, 1987)
testdat <- window(anchovyts, 1988, 1989)
fit1 <- forecast::auto.arima(traindat)
fr <- forecast::forecast(fit1, h=2)
```

The forecast errors are the difference between the test data and the forecasts.

```{r}
fr.err <- testdat - fr$mean
fr.err
```

---

### `accuracy()` function

The `accuracy()` function in forecast provides many different metrics such as mean error, root mean square error, mean absolute error, mean percentage error, mean absolute percentage error.  It requires a forecast object and a test data set that is the same length.

```{r}
accuracy(fr, testdat)
```

The metrics are:

#### ME Mean err {-}

```{r}
me <- mean(fr.err)
me
```

#### RMSE Root mean squared error {-}

```{r}
rmse <- sqrt(mean(fr.err^2))
rmse
```

#### MAE Mean absolute error {-}

```{r}
mae <- mean(abs(fr.err))
mae
```


#### MPE Mean percentage error {-}

```{r}
fr.pe <- 100*fr.err/testdat
mpe <- mean(fr.pe)
mpe
```

#### MAPE Mean absolute percentage error {-}

```{r}
mape <- mean(abs(fr.pe))
mape
```

```{r}
accuracy(fr, testdat)[,1:5]
```

```{r}
c(me, rmse, mae, mpe, mape)
```

---

### Test multiple models

Now that you have some metrics for forecast accuracy, you can compute these for all the models in your candidate set.

```{r}
# The model picked by auto.arima
fit1 <- forecast::Arima(traindat, order=c(0,1,1))
fr1 <- forecast::forecast(fit1, h=2)
test1 <- forecast::accuracy(fr1, testdat)[2,1:5]

# AR-1
fit2 <- forecast::Arima(traindat, order=c(1,1,0))
fr2 <- forecast::forecast(fit2, h=2)
test2 <- forecast::accuracy(fr2, testdat)[2,1:5]

# Naive model with drift
fit3 <- forecast::rwf(traindat, drift=TRUE)
fr3 <- forecast::forecast(fit3, h=2)
test3 <- forecast::accuracy(fr3, testdat)[2,1:5]
```

#### Show a summary {-}

```{r results='asis', echo=FALSE}
sum.tests <- rbind(test1, test2, test3)
row.names(sum.tests) <- c("(0,1,1)","(1,1,0)","Naive")
sum.tests <- format(sum.tests, digits=3)
knitr::kable(sum.tests, format="html")
```

---

### Cross-validation

Computing forecast errors and performance metrics with time series cross-validation is similar to the training set/test test approach.

The first step to using the `tsCV()` function is to define the function that returns a forecast for your model.  Your function needs to take `x`, a time series, and `h` the length of the forecast.  You can also have other arguments if needed.  Here is an example function for a forecast from an ARIMA model.

```{r}
fun <- function(x, h, order){
  forecast::forecast(Arima(x, order=order), h=h)
}
```

We pass this into the `tsCV()` function.  `tsCV()` requires our dataset and our forecast function.  The arguments after the forecast function are those we included in our `fun` definition.  `tsCV()` returns a time series of errors.

```{r}
e <- forecast::tsCV(traindat, fun, h=1, order=c(0,1,1))
```

We then can compute performance metrics from these errors.

```{r}
tscv1 <- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE))
tscv1
```

#### Cross-validation farther in future {-}

Compare accuracy of forecasts 1 year out versus 4 years out.  If `h` is greater than 1, then the errors are returned as a matrix with each `h` in a column.  Column 4 is the forecast, 4 years out.

```{r cv.sliding.4}
e <- forecast::tsCV(traindat, fun, h=4, order=c(0,1,1))[,4]
#RMSE
tscv4 <- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE))
rbind(tscv1, tscv4)
```

As we would expect, forecast errors are higher when we make forecasts farther into the future.

#### Cross-validation with a fixed window {-}

Compare accuracy of forecasts with a fixed 10-year window and 1-year out forecasts.

```{r fixed.cv.1}
e <- forecast::tsCV(traindat, fun, h=1, order=c(0,1,1), window=10)
#RMSE
tscvf1 <- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE))
tscvf1
```

#### All the forecasts tests together {-}

Here are all 4 types of forecasts tests together.  There is not right approach.  Time series cross-validation has the advantage that you test many more forecasts and use all your data.

```{r results='asis'}
comp.tab <- rbind(train.test=test1[c("ME","RMSE","MAE")],
      tsCV.variable1=tscv1,
      tsCV.variable4=tscv4,
      tsCV.fixed1=tscvf1)
knitr::kable(comp.tab, format="html")
```



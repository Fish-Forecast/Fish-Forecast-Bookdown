[
["6-4-r-markdown.html", "6.4 R Markdown", " 6.4 R Markdown Create some simulated data for a linear regression against one predictor variable: $$y_i = + x_i + e_i \\ e_i N(0,) $$ The errors \\(e_i\\) are i.i.d. These \\(y_i\\) perfectly fit the assumption in a generic linear regression model. n &lt;- 100 alpha &lt;- 1 beta &lt;- 2 cov &lt;- rnorm(n) err &lt;- rnorm(n, 0, 1) y &lt;- alpha + beta*cov + err plot(cov, y) Now fit a linear regression and look at the standard errors for the parameter estimates. dat &lt;- data.frame(y=y, x=cov) fit &lt;- lm(y~x, data=dat) summary(fit) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3600 -0.7156 0.1617 0.7416 2.1261 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8809 0.1093 8.057 1.92e-12 *** ## x 1.8752 0.1039 18.049 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.092 on 98 degrees of freedom ## Multiple R-squared: 0.7687, Adjusted R-squared: 0.7664 ## F-statistic: 325.8 on 1 and 98 DF, p-value: &lt; 2.2e-16 Let’s imagine that each \\(i\\) is a site where we have measured \\(y_i\\) and \\(x_i\\). Let’s imagine that instead of measuring \\(y\\) and \\(x\\) once at each site, we measure \\(y\\) and \\(x\\) three times. Let’s imagine also that the error \\(e\\) is site dependent, i.e. that there is a \\(e_i\\) for each site, and the \\(x\\) are the same at each site. So that \\(y_{i,1}=y_{i,2}=y_{i,3}\\). Obviously, the \\(y\\) are not independent of each other now. The \\(y_{i,j}\\) are all exactly the same. Let’s see what happens when we estimate the parameters. We’ll create our data by replicating \\(y\\) and \\(x\\) three times. We’ll randomize the order too. ord &lt;- sample(3*n) #random order y3 &lt;- c(y,y,y)[ord] cov3 &lt;- c(cov, cov, cov)[ord] dat &lt;- data.frame(y=y3, x=cov3) fit &lt;- lm(y~x, data=dat) summary(fit) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3600 -0.7156 0.1617 0.7416 2.1261 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.88086 0.06270 14.05 &lt;2e-16 *** ## x 1.87520 0.05958 31.47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.084 on 298 degrees of freedom ## Multiple R-squared: 0.7687, Adjusted R-squared: 0.768 ## F-statistic: 990.6 on 1 and 298 DF, p-value: &lt; 2.2e-16 The standard errors of our parameters have gone down. But that is not correct since we do not actually have \\(3\\times n\\) data points. We really have \\(n\\) data points. We can see the problem in a histogram of the residuals and a plot of residuals against covariate value. par(mfrow=c(1,2)) resids &lt;- residuals(fit) hist(resids, main=&quot;not normal&quot;) plot(resids[order(cov3)], main=&quot;resids vs cov&quot;) The residuals also do not pass a normality test—although that can happen for many reasons. shapiro.test(residuals(fit)) ## ## Shapiro-Wilk normality test ## ## data: residuals(fit) ## W = 0.97787, p-value = 0.0001364 This was an extreme example where \\(e_{i,1}=e_{i,2}=e_{i,3}\\), but the same problem would arise if the \\(e_i\\) were less than perfectly correlated. "],
["6-5-linear-regression-of-autoregressive-data.html", "6.5 Linear regression of autoregressive data", " 6.5 Linear regression of autoregressive data A similar problem arises when our data are a time-series and are autocorrelated—in addition to being a function of the covariate. \\[y_t = \\alpha + \\beta x_t + \\phi y_{t-1} + e_t\\] If \\(\\phi\\) is close to 0, then there is not much autocorrelation and we don’t see a problem in the ACF. phi &lt;- 0.1 yar &lt;- y[2:n]+phi*y[1:(n-1)] acf(yar) If \\(\\phi\\) is close to 1, then the autocorrelation is apparent. phi &lt;- 0.8 yar &lt;- y[2:n]+phi*y[1:(n-1)] acf(yar) If \\(\\phi\\) is close to 1, our relation between \\(y\\) and \\(x\\) also looks peculiar and non-linear. par(mfrow=c(1,2)) plot(y,cov, main=&quot;uncorrelated y&quot;) plot(yar, cov[2:n], main=&quot;correlated y&quot;) Now fit: datar &lt;- data.frame(y=yar, x=cov[2:n]) fitar &lt;- lm(y ~ x, data=datar) summary(fitar) ## ## Call: ## lm(formula = y ~ x, data = datar) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.6383 -1.4889 0.2652 1.3790 4.9608 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.7075 0.2208 7.734 9.82e-12 *** ## x 1.7425 0.2089 8.341 5.03e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.193 on 97 degrees of freedom ## Multiple R-squared: 0.4177, Adjusted R-squared: 0.4117 ## F-statistic: 69.58 on 1 and 97 DF, p-value: 5.027e-13 Let’s fit to \\(y_{1:n-1}\\) and \\(yar\\) "]
]

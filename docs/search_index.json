[
["index.html", "Forecasting Catch Time Series Preface", " Forecasting Catch Time Series Elizabeth Holmes 2018-10-11 Preface In this course, you will learn how to use R to model and forecast catch time series using a variety of different forecasting models. Time-varying regression Box-Jenkins (ARMA) Models Exponential smoothing Modelling time series with seasonality Multivaritate regression with ARMA errors ARMA models with covariates (ARMAX) Forecast diagnostics and accuracy metrics "],
["1-introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction There are many approaches for forecasting from time series alone–meaning without any covariates or exogenous variables. Examples are the approaches used in the following papers. Stergiou and Christou 1996 Time-varying regression Box-Jenkins models, aka ARIMA models Multivariate time-series approaches Harmonic regression Dynamic regression Vector autoregression (MAR) Exponential smoothing (2 variants) Exponential surplus yield model (FOX) Georgakarakos et al. 2006 Box-Jenkins models, aka ARIMA models Artificial neural networks (ANNs) Bayesian dynamic models Lawer 2016 Box-Jenkins models, aka ARIMA models Artificial neural networks (ANNs) Exponential Smoothing (6 variants) This course will focus on three of these methods: time-varying regression, ARIMA models and Exponential smoothing models. These will be shown with and without seasonality. Methods which use covariates, or exogenous variables, will also be addressed. "],
["1-1-stergiou-and-christou-1996.html", "1.1 Stergiou and Christou 1996", " 1.1 Stergiou and Christou 1996 These three methods will be demonstrated by replicating the work in Stergiou and Christou (1996) Modelling and forecasting annual fisheries catches: comparison of regression, univariate and multivariate time series methods. Fisheries Research 25: 105-136. Stergiou and Christou 1996 1.1.1 Hellenic landings data We will use the annual landings data from Hellenic (Greek) waters that were used in Stergiou and Christou (1996). Stergiou and Christou analyzed 16 species. We will look two of the species: Anchovy and Sardine. Stergiou and Christou used the data from 1964-1989. We have the data up to 2007, but will focus mainly on 1964-1989 (the first half of the time series) to replicate Stergiou and Christou’s analyses. Area where data were collected Statistical Reports Statistical Report Table IV "],
["1-2-the-landings-data-landings-rdata.html", "1.2 The landings data, landings.RData", " 1.2 The landings data, landings.RData The landings data object contains the following: landings The 1964 to 2007 total landings data multiple species. It is stored as a data frame, not ts object, with a year column, a species column and columns for landings in metric tons and log metric tons. anchovy and sardine A data frame for the landings (in log metric tons) of these species. These are the example catch time series used in the chapters. The data are 1964-2007, however Stergiou and Christou used 1964-1989 and the time series are subsetted to this time period for the examples. These data frames have only year and log.metric.tons columns. anchovyts and sardinets A ts object for the yearly landings (in log metric tons) of these species. anchovy87 and sardine87 A subsetted data frame with Year &lt;= 1987. This is the training data used in Stergiou and Christou. anchovy87ts and sardine87ts A ts object for the yearly landings (in log metric tons) of these species for 1964-1987. covsmean.mon and covsmean.year The covariates air temperature, pressure, sea surface temperature, vertical wind, and wind speed cubed average monthly and yearly over three 1 degree boxes in the study area. See the appendix in the chapter on covariates. Load the data as follows and use only the 1964-1989 landings. We use subset() to subset the landings data frame. Not window() as that is a function for subsetting ts objects. load(&quot;landings.RData&quot;) landings89 = subset(landings, Year &lt;= 1989) ggplot(landings89, aes(x=Year, y=log.metric.tons)) + geom_line() + facet_wrap(~Species) "],
["1-3-ts-objects.html", "1.3 ts objects", " 1.3 ts objects A ts object in R is a time series, univariate or multivariate, that has information on the major time step value (e.g. year) and the period of the minor time step, if any. For example, if your data are monthly then the major time step is year, the minor time step is month and the period is 12 (12 months a year). If you have daily data collected hourly then you major time step is day, minor time step is hour and period is 24 (24 hours per day). If you have yearly data collected yearly, your major time step is year, your minor time step is also year, and the period is 1 (1 year per year). You cannot have multiple minor time steps, for example monthly data collected hourly with daily and hourly periods specified. The data in a ts object cannot have any missing time steps. For example, if your data were in a data frame with a column for year, you could have a missing year, say no row for year 1988, and the data sense would still ‘make sense’. The data in a ts object cannot have any missing ‘rows’. If there is no data for a particular year or year/month (if your data are monthly), then that data point must be entered as a NA. You do not need a time step (e.g. year/month) column(s) for a ts object. You only need the starting major time step and the starting minor time step (if not 1) and the period. All the time values from each data point can be computed from those 2 pieces of information if there are no gaps in your time series. Missing data are fine; they just have to be entered with a NA. All the non-seasonal examples shown will work on a plain vector of numbers, and it it is not necessary to convert a non-seasonal time series into a ts object. That said, if you do not convert to a ts object, you will miss out on all the plotting and subsetting functions that are written for ts objects. Also when you do multivariate regression with covariates, having your data and covariates stored as a ts object will make regressing against lagged covariates (covariate values in the past) easier. 1.3.1 ts() function To convert a vector of numbers to a ts object, we use the ts() function. ts(data = NA, start = 1, end = numeric(), frequency = 1) start is a two number vector with the first major time step and the first minor time step. If you only pass in one number, then it will use 1 (first minor time step) as the 2nd number in start. end is specified in exactly the same way and you only need to specified start or end, not both. frequency is the number of minor time steps per major time step. If you do not pass this in, it will assume that frequency=1, i.e. no periods or season in your data. If you specify frequency=4, it will assume that the period is quarterly. If you specify that frequency=12, it will assume that period is monthly. This just affects the labeling of the minor time step columns and will print your data with 4 or 12 columns. For other frequencies, the data will not be printed with columns for the minor time steps, but the information is there and plotting will use the major steps. Examples Quarterly data aa &lt;- ts(1:24, start=c(1960,1), frequency=4) aa ## Qtr1 Qtr2 Qtr3 Qtr4 ## 1960 1 2 3 4 ## 1961 5 6 7 8 ## 1962 9 10 11 12 ## 1963 13 14 15 16 ## 1964 17 18 19 20 ## 1965 21 22 23 24 plot(aa, type=&quot;p&quot;) Monthly data aa &lt;- ts(1:24, start=c(1960,1), frequency=12) aa ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1960 1 2 3 4 5 6 7 8 9 10 11 12 ## 1961 13 14 15 16 17 18 19 20 21 22 23 24 plot(aa, type=&quot;p&quot;) Biennial data aa &lt;- ts(1:24, start=c(1960,1), frequency=2) aa ## Time Series: ## Start = c(1960, 1) ## End = c(1971, 2) ## Frequency = 2 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 plot(aa, type=&quot;p&quot;) 1.3.2 ggplot and ts objects In some ways, plotting ts object is easy. Just use plot() or autoplot() and it takes care of the time axis. In other ways, it can be frustrating if you want to alter the defaults. autoplot() autoplot() is a ggplot of the ts object. aa &lt;- ts(1:24, start=c(1960,1), frequency=12) autoplot(aa) and you have access to the usual gglot functions. autoplot(aa) + geom_point() + ylab(&quot;landings&quot;) + xlab(&quot;&quot;) + ggtitle(&quot;Anchovy landings&quot;) Adding minor tick marks in ggplot is tedious (google if you want that) but adding vertical lines at your minor ticks is easy. aa &lt;- ts(1:24, start=c(1960,1), frequency=12) vline_breaks &lt;- seq(1960, 1962, by=1/12) autoplot(aa) + geom_vline(xintercept = vline_breaks, color =&quot;blue&quot;) + geom_point() 1.3.3 Plotting using a data frame Often it is easier to work with a data frame (or a tibble) with columns for your major and minor time steps. That way you are not locked into whatever choices the plotting and printing functions use for ts objects. Many plotting functions work nicely with this type of data frame and you have full control over plotting and summarizing your data. To plot the x-axis, we need to add a date column in date format. Knowing the right format to use for as.Date() will take some sleuthing on the internet. The default is 1960-12-31 so if you get stuff you can always write your date in that format and use the default. Here I use 1960Jan01 and specify the format for that. I have used the date_format() function in the scales package to help format the dates on the x-axis. aa &lt;- data.frame( year=rep(1960:1961,each=12), month = rep(month.abb,2), val=1:24) aa$date &lt;- as.Date(paste0(aa$year,aa$month,&quot;01&quot;),&quot;%Y%b%d&quot;) ggplot(aa, aes(x=date, y=val)) + geom_point() + scale_x_date(labels=scales::date_format(&quot;%b-%Y&quot;)) + ylab(&quot;landings&quot;) + xlab(&quot;&quot;) "],
["1-4-packages.html", "1.4 Packages", " 1.4 Packages We will be using the forecast (Hyndman et al. 2018) and tseries (Trapletti and Hornik 2018) packages, with the MARSS (Holmes et al. 2018) package to implement ARMAX models. However we will also use a variety of other packages. So that you can keep track of what package a function come from, I will use the :: notation for functions that are not from the following standard packages: base R stats ggplot2 Thus to call function fun1 from package pack1, I will use pack1::fun1(). This will make the code more verbose but you will be able to keep track of function comes from what package. Make sure you have the following packages installed. All are available on CRAN. ggplot2, gridExtra, grid forecast tseries urca MARSS tidyverse and piping I will minimize the use of tidyverse and piping. Although the latter can create much more concise code, for beginner R users and programmers, I think it will interfere with learning. I may add the piped versions of the code later. I am not going to be doing much ‘data-wrangling’. I will assume that your data are in the tidyverse format, though I will not be using tibbles explicitly. Our data are quite simple, so this is not hard. See the chapter on inputting your data. plotting packages I will use a combination of base plotting and ggplot2 (Wickham et al. 2018) plotting. Doing a tutorial on basic plotting with ggplot2 may be helpful for the material. "],
["1-5-references.html", "1.5 References", " 1.5 References We will be using classic methods for catch forecasting discussed in the following reference papers: We are replicating the work in Stergio and Christou 1996. See their ResearchGate page for PDF reprint. These methods are also discussed in Lawer 2016. Open-Source PDF. And in Georgakarakos et al 2006. A PDF reprint is available on the Hellenic Center for Marine Research website. The chapter on modeling seasonal catch data will use models discussed in Stergiou et al 1997. See their ResearchGate page for PDF reprint. "],
["2-time-varying-regression.html", "Chapter 2 Time-varying regression", " Chapter 2 Time-varying regression Time-varying regression is simply a linear regression where time is the explanatory variable: \\[log(catch) = \\alpha + \\beta t + \\beta_2 t^2 + \\dots + e_t\\] The error term ( \\(e_t\\) ) was treated as an independent Normal error ( \\(\\sim N(0, \\sigma)\\) ) in Stergiou and Christou (1996). If that is not a reasonable assumption, then it is simple to fit a non-Gausian error model in R. Order of the time polynomial The order of the polynomial of \\(t\\) determines how the time axis (x-axis) relates to the overall trend in the data. A 1st order polynomial (\\(t\\)) will allow a linear relationship only. A 2nd order polynomial(_1 \\(t\\) + _2 \\(t^2\\)) will allow a convex or concave relationship with one peak. 3rd and 4th orders will allow more flexible relationships with more peaks. "],
["2-1-fitting-time-varying-regressions.html", "2.1 Fitting time-varying regressions", " 2.1 Fitting time-varying regressions Fitting a time-varying regression is done with the lm() function. For example, here is how to fit a 4th-order polynomial for time to the anchovy data. We are fitting this model: \\[log(Anchovy) = \\alpha + \\beta t + \\beta_2 t^2 + \\beta_3 t^3 + \\beta_4 t^4 + e_t\\] First load in the data. landings.RData includes a number of data objects. anchovy is a data frame with year and log.metric.tons columns. anchovy87 is the same data frame but with the years 1964 to 1987. These are the years that Stergio and Christou use for fitting their models. They hold out 1988 and 1989 for forecast evaluation. load(&quot;landings.RData&quot;) We need to add on a column for \\(t\\) (and \\(t^2\\), \\(t^3\\), \\(t^4\\)) where the first year is \\(t=1\\). We could regress against year (so 1964 to 1987), but by convention, one regresses against 1 to the number of years or 0 to the number of years minus 1. Stergiou and Christou did the former. anchovy87$t = anchovy87$Year-1963 anchovy87$t2 = anchovy87$t^2 anchovy87$t3 = anchovy87$t^3 anchovy87$t4 = anchovy87$t^4 model &lt;- lm(log.metric.tons ~ t + t2 + t3 + t4, data=anchovy87) All our covariates are functions of \\(t\\), so we do not actually need to add on the \\(t^2\\), \\(t^3\\) and \\(t^4\\) to our data frame. We can use the I() function. This function is useful whenever you want to use a transformed value of a column of your data frame in your regression. anchovy87$t = anchovy87$Year-1963 model &lt;- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=anchovy87) Let’s look at the fit. summary(model) ## ## Call: ## lm(formula = log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), ## data = anchovy87) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.26951 -0.09922 -0.01018 0.11777 0.20006 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.300e+00 1.953e-01 42.498 &lt;2e-16 *** ## t 1.751e-01 1.035e-01 1.692 0.107 ## I(t^2) -2.182e-02 1.636e-02 -1.333 0.198 ## I(t^3) 1.183e-03 9.739e-04 1.215 0.239 ## I(t^4) -1.881e-05 1.934e-05 -0.972 0.343 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1458 on 19 degrees of freedom ## Multiple R-squared: 0.9143, Adjusted R-squared: 0.8962 ## F-statistic: 50.65 on 4 and 19 DF, p-value: 7.096e-10 2.1.1 Orthogonal polynomials None of the time effects are significant despite an obvious linear temporal trend to the data. What’s going on? Well \\(t\\), \\(t^2\\), \\(t^3\\) and \\(t^4\\) are all highly correlated. Fitting a linear regression with multiple highly correlated covariates will not get you anywhere unless perhaps all the covariates are needed to explain the data. We will see the latter case for the sardine. In the anchovy case, multiple of the covariates could explain the linear-ish trend. You could try fitting the first degree model \\(x_t = \\alpha + \\beta t + e_t\\), then the second \\(x_t = \\alpha + \\beta_1 t + \\beta_2 t^2 + e_t\\), then the third. This would reveal that in the first and second order fits, we get significant effects of time in our model. However the correct way to do this would be to use orthogonal polynomials. 2.1.1.1 poly() function The poly() function creates orthogonal covariates for your polynomial. What does that mean? Let’s say you want to fit a model with a 2nd order polynomial of \\(t\\). It has \\(t\\) and \\(t^2\\), but using these as covariates directly lead to using two covariates that are highly correlated. Instead we want a covariate that explains \\(t\\) and another that explains the part of \\(t^2\\) that cannot be explained by \\(t\\). poly() creates these orthogonal covariates. The poly() function creates covariates with mean zero and identical variances. Covariates with different means and variances makes it hard to compare the estimated effect sizes. T1 = 1:24; T2=T1^2 c(mean(T1),mean(T2),cov(T1, T2)) ## [1] 12.5000 204.1667 1250.0000 T1 = poly(T1,2)[,1]; T2=poly(T1,2)[,2] c(mean(T1),mean(T2),cov(T1, T2)) ## [1] 4.921826e-18 2.674139e-17 -4.949619e-20 Using poly() to fit the anchovy data We saw in the anchovy fit that using \\(t\\), \\(t^2\\), \\(t^3\\) and \\(t^4\\) directly in the fit resulted in no significant estimated time effect despite a clear temporal trend in the data. If we fit with poly() so that we do not use correlated time covariates, we see a different picture. model &lt;- lm(log.metric.tons ~ poly(t,4), data=anchovy87) summary(model) ## ## Call: ## lm(formula = log.metric.tons ~ poly(t, 4), data = anchovy87) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.26951 -0.09922 -0.01018 0.11777 0.20006 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.08880 0.02976 305.373 &lt; 2e-16 *** ## poly(t, 4)1 1.97330 0.14581 13.534 3.31e-11 *** ## poly(t, 4)2 0.54728 0.14581 3.753 0.00135 ** ## poly(t, 4)3 0.30678 0.14581 2.104 0.04892 * ## poly(t, 4)4 -0.14180 0.14581 -0.972 0.34302 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1458 on 19 degrees of freedom ## Multiple R-squared: 0.9143, Adjusted R-squared: 0.8962 ## F-statistic: 50.65 on 4 and 19 DF, p-value: 7.096e-10 2.1.2 Residual diagnostics We want to test if our residuals are temporally independent. We can do this with the Ljung-Box test as Stergio and Christou do. For the Ljung-Box test Null hypothesis is that the data are independent Alternate hypothesis is that the data are serially correlated Example of the Ljung-Box test Box.test(rnorm(100), type=&quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: rnorm(100) ## X-squared = 1.0053, df = 1, p-value = 0.316 The null hypothesis is not rejected. These are not serially correlated. Stergio and Christou appear to use a lag of 14 for the test (this is a bit large for 24 data points). The degrees of freedom is lag minus the number of estimated parameters in the model. So for the Anchovy data, \\(df = 14 - 2\\). x &lt;- resid(model) Box.test(x, lag = 14, type = &quot;Ljung-Box&quot;, fitdf=2) ## ## Box-Ljung test ## ## data: x ## X-squared = 14.627, df = 12, p-value = 0.2625 Compare to the values in the far right column in Table 4. The null hypothesis of independence is rejected. Breusch-Godfrey test Although Stergiou and Christou use the Ljung-Box test, the Breusch-Godfrey test is more standard for regression residuals. The forecast package has the checkresiduals() function which will run this test and some diagnostic plots. forecast::checkresiduals(model) ## ## Breusch-Godfrey test for serial correlation of order up to 8 ## ## data: Residuals ## LM test = 12.858, df = 8, p-value = 0.1168 2.1.3 Compare to Stergiou and Christou Stergiou and Christou (1996) fit time-varying regressions to the 1964-1987 data and show the results in Table 4. Table 4 Compare anchovy fit to Stergiou and Christou Stergiou and Christou use a first order polynomial, linear relationship with time, for the anchovy data. They do not state how they choose this over a 2nd order polynomial which also appears supported (see fit with poly() fit to the anchovy data). anchovy87$t = anchovy87$Year-1963 model &lt;- lm(log.metric.tons ~ t, data=anchovy87) The coefficients and adjusted R2 are similar to that shown in their Table 4. The coefficients are not identical so there may be some differences in the data I extracted from the Greek statistical reports and those used in Stergiou and Christou. c(coef(model), summary(model)$adj.r.squared) ## (Intercept) t ## 8.36143085 0.05818942 0.81856644 Compare sardine fit to Stergiou and Christou For the sardine (bottom row in Table 4), Stergio and Christou fit a 4th order polynomial. With poly(), a 4th order time-varying regression model is fit to the sardine data as: sardine87$t = sardine87$Year-1963 model &lt;- lm(log.metric.tons ~ poly(t,4), data=sardine87) This indicates support for the 2nd, 3rd, and 4th orders but not the 1st (linear) part. summary(model) ## ## Call: ## lm(formula = log.metric.tons ~ poly(t, 4), data = sardine87) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.115300 -0.053090 -0.008895 0.041783 0.165885 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.31524 0.01717 542.470 &lt; 2e-16 *** ## poly(t, 4)1 0.08314 0.08412 0.988 0.335453 ## poly(t, 4)2 -0.18809 0.08412 -2.236 0.037559 * ## poly(t, 4)3 -0.35504 0.08412 -4.220 0.000463 *** ## poly(t, 4)4 0.25674 0.08412 3.052 0.006562 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08412 on 19 degrees of freedom ## Multiple R-squared: 0.6353, Adjusted R-squared: 0.5586 ## F-statistic: 8.275 on 4 and 19 DF, p-value: 0.0004846 Stergiou and Christou appear to have used a raw polynomial model using \\(t\\), \\(t^2\\), \\(t^3\\) and \\(t^4\\) as the covariates instead of orthogonal polynomials. To fit the model that they did, we use model &lt;- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=sardine87) Using a model fit with the raw time covariates, the coefficients and adjusted R2 are similar to that shown in Table 4. c(coef(model), summary(model)$adj.r.squared) ## (Intercept) t I(t^2) I(t^3) I(t^4) ## 9.672783e+00 -2.443273e-01 3.738773e-02 -1.983588e-03 3.405533e-05 ## ## 5.585532e-01 The test for autocorrelation of the residuals is x &lt;- resid(model) Box.test(x, lag = 14, type = &quot;Ljung-Box&quot;, fitdf=5) ## ## Box-Ljung test ## ## data: x ## X-squared = 32.317, df = 9, p-value = 0.0001755 fitdf specifies the number of parameters estimated by the model. In this case it is 5, intercept and 4 coefficients. The p-value is less than 0.05 indicating that the residuals are temporally correlated. 2.1.4 Summary Why use time-varying regression? It looks there is a simple time relationship. If a high-order polynomial is required, that is a bad sign. Easy and fast Easy to explain You are only forecasting a few years ahead No assumptions required about ‘stationarity’ Why not to use time-varying regression? Autocorrelation is not modeled. That autocorrelation may hold information for forecasting. You are only using temporal trend for forecasting (mean level). If you use a high-order polynomial, you might be modeling noise from a random walk. That means interpreting the temporal pattern as having information when in fact it has none. Is time-varying regression used? It seems pretty simple. Is this used? All the time. Most “trend” analyses are a variant of time-varying regression. If you fit a line to your data and report the trend or percent change, that’s a time-varying regression. "],
["2-2-forecasting-with-a-time-varying-regression-model.html", "2.2 Forecasting with a time-varying regression model", " 2.2 Forecasting with a time-varying regression model Forecasting is easy in R once you have a fitted model. Let’s say for the anchovy, we fit the model \\[C_t = \\alpha + \\beta t + e_t\\] where \\(t\\) starts at 1 (so 1964 is \\(t=1\\) ). To predict, predict the catch in year t, we use \\[C_t = \\alpha + \\beta t + e_t\\] Model fit: load(&quot;landings.RData&quot;) anchovy87$t &lt;- anchovy87$Year-1963 model &lt;- lm(log.metric.tons ~ t, data=anchovy87) coef(model) ## (Intercept) t ## 8.36143085 0.05818942 For anchovy, the estimated \\(\\alpha\\) (Intercept) is 8.3614309 and \\(\\beta\\) is 0.0581894. We want to use these estimates to forecast 1988 ( \\(t=25\\) ). So the 1988 forecast is 8.3614309 + 0.0581894 \\(\\times\\) 25 : coef(model)[1]+coef(model)[2]*25 ## (Intercept) ## 9.816166 log metric tons. 2.2.1 The forecast package The forecast package in R makes it easy to create forecasts with fitted models and to plot (some of) those forecasts. For a TV Regression model, our forecast() call looks like fr &lt;- forecast::forecast(model, newdata = data.frame(t=25:29)) The dark grey bands are the 80% prediction intervals and the light grey are the 95% prediction intervals. plot(fr) Anchovy forecasts from a higher order polynomial can similarly be made. Let’s fit a 4-th order polynomial. \\[C_t = \\alpha + \\beta_1 t + \\beta_2 t^2 + \\beta_3 t^3 + \\beta_4 t^4 + e_t\\] To forecast with this model, we fit the model to estimate the \\(\\beta\\)’s and then replace \\(t\\) with \\(24\\): \\[C_{1988} = \\alpha + \\beta_1 24 + \\beta_2 24^2 + \\beta_3 24^3 + \\beta_4 24^4 + e_t\\] This is how to do that in R: model &lt;- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=anchovy87) fr &lt;- forecast::forecast(model, newdata = data.frame(t=24:28)) fr ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1 10.05019 9.800941 10.29944 9.657275 10.44310 ## 2 10.18017 9.856576 10.50377 9.670058 10.69028 ## 3 10.30288 9.849849 10.75591 9.588723 11.01704 ## 4 10.41391 9.770926 11.05689 9.400315 11.42750 ## 5 10.50839 9.609866 11.40691 9.091963 11.92482 Unfortunately, forecast’s plot() function for forecast objects does not recognize that there is only one predictor \\(t\\) thus we cannot use forecast’s plot function. If you do this in R, it throws an error. try(plot(fr)) Error in plotlmforecast(x, PI = PI, shaded = shaded, shadecols = shadecols, : Forecast plot for regression models only available for a single predictor I created a function that you can use to plot time-varying regressions with polynomial \\(t\\). You will use this function in the lab. plotforecasttv(model, ylims=c(8,17)) A feature of a time-varying regression with many polynomials is that it fits the data well, but the forecast quickly becomes uncertain due to uncertainty regarding the polynomial fit. A simpler model can give forecasts that do not become rapidly uncertain. The flip-side is that the simpler model may not capture the short-term trends very well and may suffer from autocorrelated residuals. model &lt;- lm(log.metric.tons ~ t + I(t^2), data=anchovy87) plotforecasttv(model, ylims=c(8,17)) 2.2.2 Summary Time-varying regression is a simple approach to forecasting that allows a non-linear trend. The uncertainty in your forecast is determined by how much error there is between the fit an the data. Fit must be balanced against prediction uncertainty. R allows you to quickly fit models and compute the prediction intervals. Careful thought must be given to selecting the polynomial order. Standard methods are available in R for order selection Using different orders for different data sets has prediction consequences "],
["3-arima-models.html", "Chapter 3 ARIMA Models", " Chapter 3 ARIMA Models The basic idea in an ARMA model is that past values in the time series have information about the current state. An AR model, the first part of ARMA, models the current state as a linear function of past values: \\[x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + e_t\\] Species Plot "],
["3-1-overview.html", "3.1 Overview", " 3.1 Overview 3.1.1 Components of an ARIMA model You will commonly see ARIMA models referred to as Box-Jenkins models. This model has 3 components (p, d, q): AR autoregressive \\(y_t\\) depends on past values. The AR level is maximum lag \\(p\\). \\[x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + e_t\\] I differencing \\(x_t\\) may be a difference of the observed time series. The number of differences is denoted \\(d\\). First difference is \\(d=1\\): \\[x_t = y_t - y_{t-1}\\] MA moving average The error \\(e_t\\) can be a sum of a time series of independent random errors. The maximum lag is denoted \\(q\\). \\[e_t = \\eta_t + \\theta_1 \\eta_{t-1} + \\theta_2 \\eta_{t-2} + ... + \\theta_q \\eta_{t-q},\\quad \\eta_t \\sim N(0, \\sigma)\\] Create some data from an AR(2) Model \\[x_t = 0.5 x_{t-1} + 0.3 x_{t-2} + e_t\\] dat = arima.sim(n=1000, model=list(ar=c(.5,.3))) plot(dat) abline(h=0, col=&quot;red&quot;) Compare AR(2) and random data. AR(2) is auto-correlated Plot the data at time \\(t\\) against the data at time \\(t-1\\) 3.1.2 Box-Jenkins Method This refers to a step-by-step process of selecting a forecasting model. You need to go through the steps otherwise you could end up fitting a nonsensical model or using fitting a sensible model with an algorithm that will not work on your data. A. Model form selection Evaluate stationarity and seasonality Selection of the differencing level (d) Selection of the AR level (p) Selection of the MA level (q) B. Parameter estimation C. Model checking 3.1.3 ACF and PACF functions The ACF function The auto-correlation function (ACF) is the correlation between the data at time \\(t\\) and \\(t+1\\). This is one of the basic diagnostic plots for time series data. acf(dat[1:50]) The ACF simply shows the correlation between all the data points that are lag \\(p\\) apart. Here are the correlations for points lag 1 and lag 10 apart. cor() is the correlation function. cor(dat[2:TT], dat[1:(TT-1)]) ## [1] 0.6997968 cor(dat[11:TT], dat[1:(TT-10)]) ## [1] 0.159927 The values match what we see in the ACF plot. ACF for independent data Temporally independent data shows no significant autocorrelation. PACF function In the ACF for the AR(2), we see that \\(x_t\\) and \\(x_{t-3}\\) are correlated even those the model for \\(x_t\\) does not include \\(x_{t-3}\\). \\(x_{t-3}\\) is correlated with \\(x_t\\) indirectly because \\(x_{t-3}\\) is directly correlated with \\(x_{t-2}\\) and \\(x_{t-1}\\) and these two are in turn directly correlated with \\(x_t\\). The partial autocorrelation function removes this indirect correlation. Thus the only significant lags in the PACF should be the lags that appear in the process model. For example, if the model is Partial ACF for AR(2) \\[x_t = 0.5 x_{t-1} + 0.3 x_{t-2} + e_t\\] then only the first two lags should be significant in the PACF. pacf(dat) Partial ACF for AR(1) Similarly if the process model is \\[x_t = 0.5 x_{t-1} + e_t\\] The PACF should only have significant values at lag 1. dat &lt;- arima.sim(TT, model=list(ar=c(.5))) pacf(dat) "],
["3-2-stationarity.html", "3.2 Stationarity", " 3.2 Stationarity The first two steps of the Box-Jenkins Method have to do with evaluating for stationarity and correcting for lack of stationarity in your data: A. Model form selection Evaluate stationarity and seasonality Selection of the differencing level (d) Selection of the AR level (p) Selection of the MA level (q) B. Parameter estimation C. Model checking 3.2.1 Definition Stationarity means ‘not changing in time’ in the context of time-series models. Typically we test the trend and variance, however more generally all statistical properties of a time-series is time-constant if the time series is ‘stationary’. Many ARMA models exhibit stationarity. White noise is one type: \\[x_t = e_t, e_t \\sim N(0,\\sigma)\\] ## Loading required package: gridExtra ## Loading required package: reshape2 An AR-1 process with \\(b&lt;1\\) \\[x_t = b x_{t-1} + e_t\\] is also stationary. Stationarity around a trend The processes shown above have mean 0 and a flat level. We can also have stationarity around an non-zero level or stationarity around an linear trend. If \\(b=0\\), we have white noise and if \\(b&lt;1\\) we have AR-1. Non-zero mean: \\(x_t = \\mu + b x_{t-1} + e_t\\) Linear trend: \\(x_t = \\mu + at + b x_{t-1} + e_t\\) 3.2.2 Non-stationarity One of the most common forms of non-stationarity that is tested for is ‘unit root’, which means that the process is a random walk: \\[x_t = x_{t-1} + e_t\\] . Non-stationarity with a trend Similar to the way we added an intecept and linear trend to the stationarity processes, we can do the same to the random walk. Non-zero mean or intercept: \\(x_t = \\mu + x_{t-1} + e_t\\) Linear trend: \\(x_t = \\mu + at + x_{t-1} + e_t\\) The effects are fundamentally different however. The addition of \\(\\mu\\) leads to a upward mean linear trend while the addition of \\(at\\) leads to exponential growth. 3.2.3 Stationarity tests Why is evaluating stationarity important? Many AR models have a flat level or trend and time-constant variance. If your data do not have those properties, you are fitting a model that is fundamentally inconsistent with your data. Many standard algorithms for fitting ARIMA models assume stationarity. Note, you can fit ARIMA models without making this assumption, but you need to use the appropriate algorithm. We will discuss three common approaches to evaluating stationarity: Visual test (Augmented) Dickey-Fuller test KPSS test Visual test The visual test is simply looking at a plot of the data versus time. Look for Change in the level over time. Is the time series increasing or decreasing? Does it appear to cycle? Change in the variance over time. Do deviations away from the mean change over time, increase or decrease? Here is a plot of the anchovy and sardine in Greek waters from 1965 to 1989. The anchovies have an obvious non-stationary trend during this period. The mean level is going up. The sardines have a roughly stationary trend. The variance (deviations away from the mean) appear to be roughly stationary, neither increasing or decreasing in time. Although the logged anchovy time series is increasing, it appears to have an linear trend. Dickey-Fuller test The Dickey=Fuller test (and Augmented Dickey-Fuller test) look for evidence that the time series has a unit root. The null hypothesis is that the time series has a unit root, that is, it has a random walk component. The alternative hypothesis is some variation of stationarity. The test has three main verisons. Visually, the null and alternative hypotheses for the three Dickey-Fuller tests are the following. It is hard to see but in the panels on the left, the variance around the trend is increasing and on the right, it is not. Mathematically, here are the null and alternative hypotheses. In each, we are testing if \\(\\delta=0\\). Null is a random walk with no drift \\(x_t = x_{t-1}+e_t\\) Alternative is a mean-reverting (stationary) process with zero mean. \\(x_t = \\delta x_{t-1}+e_t\\) Null is a random walk with drift (linear STOCHASTIC trend) \\(x_t = \\mu + x_{t-1} + e_t\\) Alternative is a mean-reverting (stationary) process with non-zero mean and no trend. \\(x_t = \\mu + \\delta x_{t-1} + e_t\\) Null is a random walk with exponential trend \\(x_t = \\mu + at + x_{t-1} + e_t\\) Alternative is a mean-reverting (stationary) process with non-zero mean and linear DETERMINISTIC trend. \\(x_t = \\mu + at + \\delta x_{t-1} + e_t\\) Example: Dickey-Fuller test using adf.test() adf.test() in the tseries package will apply the Augemented Dickey-Fuller and report the p-value. We want to reject the Dickey=Fuller null hypothesis of non-stationarity. We will set k=0 to apply the Dickey-Fuller test which tests for AR(1) stationarity. The Augmented Dickey-Fuller tests for more general lag-p stationarity. adf.test(x, alternative = c(&quot;stationary&quot;, &quot;explosive&quot;), k = trunc((length(x)-1)^(1/3))) x is the time-series data in vector or ts form. Here is how to apply this test to the anchovy data tseries::adf.test(anchovy87ts, k=0) ## ## Augmented Dickey-Fuller Test ## ## data: anchovy87ts ## Dickey-Fuller = -2.8685, Lag order = 0, p-value = 0.2415 ## alternative hypothesis: stationary # or tseries::adf.test(anchovy87$log.metric.tons, k=0) The null hypothesis is not rejected. That is not what we want. Example: Dickey-Fuller test using ur.df() The urca R package can also be used to apply the Dickey-Fuller tests. Use lags=0 for Dickey-Fuller which tests for AR(1) stationarity. We will set type=&quot;trend&quot; to deal with the trend seen in the anchovy data. Note, adf.test() uses this type by default. ur.df(y, type = c(&quot;none&quot;, &quot;drift&quot;, &quot;trend&quot;), lags = 0) test = urca::ur.df(anchovy87ts, type=&quot;trend&quot;, lags=0) test ## ## ############################################################### ## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # ## ############################################################### ## ## The value of the test statistic is: -2.8685 4.0886 4.7107 ur.df() will report the test statistic. You can look up the values of the test statistic for different \\(\\alpha\\) levels using summary(test) or attr(test, &quot;cval&quot;). If the test statistic is less than the critical value for \\(\\alpha\\)=0.05 (‘5pct’ in cval), it means the null hypothesis of non-stationarity is rejected. For the Dickey-Fuller test, you do want to reject the null hypothesis. The test statistic is attr(test, &quot;teststat&quot;) ## tau3 phi2 phi3 ## statistic -2.86847 4.088559 4.71069 and the critical value at \\(\\alpha = 0.05\\) is attr(test,&quot;cval&quot;) ## 1pct 5pct 10pct ## tau3 -4.38 -3.60 -3.24 ## phi2 8.21 5.68 4.67 ## phi3 10.61 7.24 5.91 The statistic is larger than the critical value and thus the null hypothesis of non-stationarity is not rejected. That’s not what we want. Augmented Dickey-Fuller test The Dickey-Fuller test assumes that the stationary process is AR(1) (autoregressive lag-1). The Augmented Dickey-Fuller test allows a general stationary process. The idea of the test however is the same. We can apply the Augmented Dickey-Fuller test with the ur.df() function or the adf.test() function in the tseries package. adf.test(x, alternative = c(&quot;stationary&quot;, &quot;explosive&quot;), k = trunc((length(x)-1)^(1/3))) The alternative is either stationary like \\(x_t = \\delta x_{t-1} + \\eta_t\\) with \\(\\delta&lt;1\\) or ‘explosive’ with \\(\\delta&gt;1\\). k is the number of lags which determines the number of time lags allowed in the autoregression. k is generally determined by the length of your time series. Example: Augmented Dickey-Fuller tests with adf.test() With the tseries package, we apply the Augmented Dickey-Fuller test with adf.test(). This function uses the test where the alternative model is stationary around a linear trend: \\(x_t = \\mu + at + \\delta x_{t-1} + e_t\\). tseries::adf.test(anchovy87ts) ## ## Augmented Dickey-Fuller Test ## ## data: anchovy87ts ## Dickey-Fuller = -0.57814, Lag order = 2, p-value = 0.9685 ## alternative hypothesis: stationary In both cases, we do not reject the null hypothesis that the data have a random walk. Thus there is not support for these time series being stationary. Example: Augmented Dickey-Fuller tests with ur.df() With the urca package, we apply the Augmented Dickey-Fuller test with ur.df(). The defaults for ur.df() are different than for adf.test(). ur.df() allows you to specify which of the 3 alternative hypotheses you want: none (stationary around 0), drift (stationary around a non-zero intercept), trend (stationary around a linear trend). Another difference is that by default, ur.df() uses a fixed lag of 1 while by default adf.test() selects the lag based on the length of the time series. We will specify “trend” to make the test similar to adf.test(). We will set the lags like adf.test() does also. k = trunc((length(anchovy87ts)-1)^(1/3)) test = urca::ur.df(anchovy87ts, type=&quot;trend&quot;, lags=k) test ## ## ############################################################### ## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # ## ############################################################### ## ## The value of the test statistic is: -0.5781 3.2816 0.8113 The test statistic values are the same, but we need to look up the critical values with summary(test). KPSS test In the Dickey-Fuller test, the null hypothesis is the unit root, i.e. random walk. Often times, there is not enough power to reject the null hypothesis. A null hypothesis is accepted unless there is strong evidence against it. The Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test has as the null hypothesis that a time series is stationary around a level trend (or a linear trend). The alternative hypothesis for the KPSS test is a random walk. The stationarity assumption is general; it does not assume a specific type of stationarity such as white noise. If both KPSS and Dickey-Fuller tests support non-stationarity, then the stationarity assumption is not supported. Example: KPSS tests tseries::kpss.test(anchovy87ts, null=&quot;Trend&quot;) ## ## KPSS Test for Trend Stationarity ## ## data: anchovy87ts ## KPSS Trend = 0.22597, Truncation lag parameter = 1, p-value = 0.01 Here null=&quot;Trend&quot; was included to account for the increasing trend in the data. The null hypothesis of stationarity is rejected. Thus both the KPSS and Dickey-Fuller tests support the hypothesis that the anchovy time series is non-stationary. That’s not what we want. 3.2.4 Differencing the data Differencing the data is used to correct non-stationarity. Differencing means to create a new time series \\(z_t = x_t - x_{t-1}\\). First order differencing means you do this once (so \\(z_t\\)) and second order differencing means you do this twice (so \\(z_t - z_{t-1}\\)). The diff() function takes the first difference: x &lt;- diff(c(1,2,4,7,11)) x ## [1] 1 2 3 4 The second difference is the first difference of the first difference. diff(x) ## [1] 1 1 1 Here is a plot of the anchovy data and its first difference. par(mfrow=c(1,2)) plot(anchovy87ts, type=&quot;l&quot;) title(&quot;Anchovy&quot;) plot(diff(anchovy87ts), type=&quot;l&quot;) title(&quot;Anchovy first difference&quot;) Let’s test the anchovy data with one difference using the KPSS test. diff.anchovy = diff(anchovy87ts) tseries::kpss.test(diff.anchovy) ## Warning in tseries::kpss.test(diff.anchovy): p-value greater than printed ## p-value ## ## KPSS Test for Level Stationarity ## ## data: diff.anchovy ## KPSS Level = 0.18309, Truncation lag parameter = 1, p-value = 0.1 The null hypothesis of stationairity is not rejected. That is good. Let’s test the first difference of the anchovy data using the Augmented Dickey-Fuller test. We do the default test and allow it to chose the number of lags. tseries::adf.test(diff.anchovy) ## ## Augmented Dickey-Fuller Test ## ## data: diff.anchovy ## Dickey-Fuller = -4.2126, Lag order = 2, p-value = 0.01584 ## alternative hypothesis: stationary The null hypothesis of non-stationarity is rejected. That is what we want. However, we differenced which removed the trend thus we are testing against a more general model than we need. Let’s test with an alternative hypothesis that has a non-zero mean but not trend. We can do this with ur.df() and type='drift'. test &lt;- urca::ur.df(diff.anchovy, type=&quot;drift&quot;, lags=2) The null hypothesis of NON-stationairity IS rejected. That is good. The test statistic is attr(test, &quot;teststat&quot;) ## tau2 phi1 ## statistic -3.492685 6.099778 and the critical value at \\(\\alpha = 0.05\\) is attr(test,&quot;cval&quot;) ## 1pct 5pct 10pct ## tau2 -3.75 -3.00 -2.63 ## phi1 7.88 5.18 4.12 3.2.5 Summary Test stationarity before you fit a ARMA model. Visual test: is the time series flutuating about a level or a linear trend? Yes or maybe? Apply a “unit root” test. (Augmented) Dickey-Fuller test KPSS test No or fails the unit root test. Apply differencing again and re-test. Still not passing? Try a second difference. Still not passing? ARMA model might not be the best choice. Or you may need to an adhoc detrend. "],
["3-3-model-structure.html", "3.3 Model Structure", " 3.3 Model Structure We are now at step A3 and A4 of the Box-Jenkins Method. Note we did not address seasonality since we are working with yearly data. A. Model form selection Evaluate stationarity and seasonality Selection of the differencing level (d) Selection of the AR level (p) Selection of the MA level (q) B. Parameter estimation C. Model checking Much of this will be automated when we use the forecast package 3.3.1 AR and MA lags Step A3 is to determine the number of \\(p\\) lags in the AR part of the model: \\[x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + e_t\\] Step A4 is to determine the number of \\(q\\) lags in the MA part of the model: \\[e_t = \\eta_t + \\theta_1 \\eta_{t-1} + \\theta_2 \\eta_{t-2} + ... + \\theta_q \\eta_{t-q},\\quad \\eta_t \\sim N(0, \\sigma)\\] 3.3.2 Model order For an ARIMA model, the number of AR lags, number of differences, and number of MA lags is called the model order or just order. Examples. Note \\(e_t \\sim N(0,\\sigma)\\) order (0,0,0) white noise \\[x_t = e_t\\] order (1,0,0) AR-1 process \\[x_t = \\phi x_{t-1} + e_t\\] order (0,0,1) MA-1 process \\[x_t = e_t + \\theta e_{t-1}\\] order (1,0,1) AR-1 MA-1 process \\[x_t = \\phi x_{t-1} + e_t + \\theta e_{t-1}\\] order (0,1,0) random walk \\[x_t - x_{t-1} = e_t\\] which is the same as \\[x_t = x_{t-1} + e_t\\] 3.3.3 Choosing the AR and MA levels Method #1 use the ACF and PACF functions The ACF plot shows you how the correlation between \\(x_t\\) and \\(x_{t+p}\\) decrease as \\(p\\) increases. The PACF plot shows you the same but removes the autocorrelation due to lags less that \\(p\\). If your ACF and PACF look like the top panel, it is AR-p. The first lag where the PACF is below the dashed lines is the \\(p\\) lag for your model. If it looks like the middle panel, it is MA-p. The first lag where the ACF is below the dashed lines is the \\(q\\) lag for your model. If it looks like the bottom panel, it is ARMA and this approach doesn’t work. Method #2 Use formal model selection This weighs how well the model fits against how many parameters your model has. We will use this approach. The auto.arima() function in the forecast package in R allows you to easily estimate the \\(p\\) and \\(q\\) for your ARMA model. We will use the first difference of the anchovy data since our stationarity diagnostics indicated that a first difference makes our time series stationary. anchovy.diff1 = diff(anchovy87$log.metric.tons) forecast::auto.arima(anchovy.diff1) ## Series: anchovy.diff1 ## ARIMA(0,0,1) with non-zero mean ## ## Coefficients: ## ma1 mean ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 The output indicates that the ‘best’ model is a MA-1 with a non-zero mean. “non-zero mean” means that the mean of our data (anchovy.diff1) is not zero. auto.arima() will also estimate the amount of differencing needed. forecast::auto.arima(anchovy87ts) ## Series: anchovy87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 The output indicates that the ‘best’ model is a MA-1 with first difference. “with drift” means that the mean of our data (anchovy87) is not zero. This is the same model but the jargon regarding the mean is different. More examples Let’s try fitting to some simulated data. We will simulate with arima.sim(). We will specify no differencing. set.seed(100) a1 = arima.sim(n=100, model=list(ar=c(.8,.1))) forecast::auto.arima(a1, seasonal=FALSE, max.d=0) ## Series: a1 ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## 0.6928 -0.5343 ## s.e. 0.0732 0.2774 ## ## sigma^2 estimated as 0.7703: log likelihood=-128.16 ## AIC=262.33 AICc=262.58 BIC=270.14 The ‘best-fit’ model is simpler than the model used to simulate the data. How often is the ‘true’ model is chosen? Let’s fit 100 simulated time series and see how often the ‘true’ model is chosen. By far the correct type of model is selected, AR-p, but usually a simpler model of AR-1 is chosen over AR-2 (correct) most of the time. save.fits = rep(NA,100) for(i in 1:100){ a1 = arima.sim(n=100, model=list(ar=c(.8,.1))) fit = forecast::auto.arima(a1, seasonal=FALSE, max.d=0, max.q=0) save.fits[i] = paste0(fit$arma[1], &quot;-&quot;, fit$arma[2]) } table(save.fits) ## save.fits ## 1-0 2-0 3-0 4-0 ## 74 20 5 1 3.3.4 Trace = TRUE You can see what models that auto.arima() tried using trace=TRUE. The models are selected on AICc by default and the AICc value is shown next to the model. forecast::auto.arima(anchovy87ts, trace=TRUE) ## ## ARIMA(2,1,2) with drift : 0.9971438 ## ARIMA(0,1,0) with drift : -1.582738 ## ARIMA(1,1,0) with drift : -3.215851 ## ARIMA(0,1,1) with drift : -5.727702 ## ARIMA(0,1,0) : -1.869767 ## ARIMA(1,1,1) with drift : -2.907571 ## ARIMA(0,1,2) with drift : -3.219136 ## ARIMA(1,1,2) with drift : -1.363802 ## ARIMA(0,1,1) : -1.425496 ## ## Best model: ARIMA(0,1,1) with drift ## Series: anchovy87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 3.3.5 stepwise=FALSE By default, step-wise selection is used and an approximation is used for the models tried in the model selection step. For a final model selection, you should turn these off. forecast::auto.arima(anchovy87ts, stepwise=FALSE, approximation=FALSE) ## Series: anchovy87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 3.3.6 Summary Once you have dealt with stationarity, you need to determine the order of the model: the AR part and the MA part. Although you could simply use auto.arima(), it is best to run acf() and pacf() on your data to understand it better. Does it look like a pure AR process? Also evaluate if there are reasons to assume a particular structure. Are you using an established model form, from say another paper? Are you fitting to a process that is fundamentally AR only or AR + MA? "],
["3-4-fitting-arima-models.html", "3.4 Fitting ARIMA Models", " 3.4 Fitting ARIMA Models We are now at step B of the Box-Jenkins Method. A. Model form selection Evaluate stationarity and seasonality Selection of the differencing level (d) Selection of the AR level (p) Selection of the MA level (q) B. Parameter estimation C. Model checking 3.4.1 Fitting with auto.arima() auto.arima() (in the forecast package) has many arguments. auto.arima(y, d = NA, D = NA, max.p = 5, max.q = 5, max.P = 2, max.Q = 2, max.order = 5, max.d = 2, max.D = 1, start.p = 2, start.q = 2, start.P = 1, start.Q = 1, stationary = FALSE, seasonal = TRUE, ic = c(&quot;aicc&quot;, &quot;aic&quot;, &quot;bic&quot;), stepwise = TRUE, trace = FALSE, approximation = (length(x) &gt; 150 | frequency(x) &gt; 12), truncate = NULL, xreg = NULL, test = c(&quot;kpss&quot;, &quot;adf&quot;, &quot;pp&quot;), seasonal.test = c(&quot;seas&quot;, &quot;ocsb&quot;, &quot;hegy&quot;, &quot;ch&quot;), allowdrift = TRUE, allowmean = TRUE, lambda = NULL, biasadj = FALSE, parallel = FALSE, num.cores = 2, x = y, ...) When just getting started, we will focus just on a few of these. trace To print out the models that were tested. stepwise and approximation To use slower but better estimation when selecting model order. test The test to use to select the amount of differencing. Load the data Load the data. load(&quot;landings.RData&quot;) anchovy87ts is a ts object of the log metric tons for 1964-1987. We will use this for auto.arima() however we could also use anchovy87$log.metric.tons. anchovy87ts is just anchovy87ts &lt;- ts(anchovy87, start=1964) Fit to the anchovy data using auto.arima() fit &lt;- forecast::auto.arima(anchovy87ts) Here are the values for anchovy in Table 8 of Stergiou and Christou. Model \\(\\theta_1\\) drift (c) R\\(^2\\) BIC LB (0,1,1) 0.563 0.064 0.83 1775 5.4 Here is the equivalent values from the best fit from auto.arima(): Model theta1 drift R2 BIC LB (0,1,1) 0.5731337 0.0640889 0.8402976 -3.584377 5.372543 Where do we find each of the components of Stergiou and Christou’s Table 8? The parameter estimates We can extract the parameter estimates from a fitted object in R using coef(). coef(fit) ## ma1 drift ## -0.5731337 0.0640889 The ma1 is the same as \\(\\theta_1\\) except its negative because of the way Stergiou and Christou write their MA models. They write it as \\[e_t = \\eta_t - \\theta_1 \\eta_{t-1}\\] instead of the form that auto.arima() uses \\[e_t = \\eta_t + \\theta_1 \\eta_{t-1}\\] Computing R2 This is not output as part of a arima fitted object so we need to compute it. res &lt;- resid(fit) dat &lt;- anchovy87$log.metric.tons meany &lt;- mean(dat, na.rm=TRUE) r2 &lt;- 1- sum(res^2,na.rm=TRUE)/sum((dat-meany)^2,na.rm=TRUE) Ljung-Box statistic LB &lt;- Box.test(res, type=&quot;Ljung-Box&quot;, lag=12, fitdf=2)$statistic fitdf=2 is from the number of parameters estimated. BIC BIC is in fit$BIC. Why is BIC different? Because there is a missing constant, which is fairly common. The absolute value of BIC is unimportant. Only its value relative to other models that you tested is important. 3.4.2 Outputting the models tested Pass in trace=TRUE to see a list of the models tested in auto.arima()’s search. By default auto.arima() uses AICc for model selection and the AICc values are shown. Smaller is better for AICc and AICc values that are different by less than 2 have similar data support. Look for any models with similar AICc to the best selected model. You should consider that model also. forecast::auto.arima(anchovy87ts, trace=TRUE) ## ## ARIMA(2,1,2) with drift : 0.9971438 ## ARIMA(0,1,0) with drift : -1.582738 ## ARIMA(1,1,0) with drift : -3.215851 ## ARIMA(0,1,1) with drift : -5.727702 ## ARIMA(0,1,0) : -1.869767 ## ARIMA(1,1,1) with drift : -2.907571 ## ARIMA(0,1,2) with drift : -3.219136 ## ARIMA(1,1,2) with drift : -1.363802 ## ARIMA(0,1,1) : -1.425496 ## ## Best model: ARIMA(0,1,1) with drift ## Series: anchovy87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 3.4.3 Repeat with the sardine data Stergiou and Christou sardine model (Table 8) is ARIMA(0,1,0): \\[x_t = x_{t-1}+e_t\\] The model selected by auto.arima() is ARIMA(0,0,1): \\[x_t = e_t + \\theta_1 e_{t-1}\\] forecast::auto.arima(sardine87ts) ## Series: sardine87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 Why? Stergiou and Christou used the Augmented Dickey-Fuller test to determine the amount of differencing needed while the default for auto.arima() is to use the KPSS test. Repeat using test='adf' Now the selected model is the same. fit &lt;- auto.arima(sardine87ts, test=&quot;adf&quot;) fit ## Series: sardine87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 Compare the estimated values in Stergiou and Christou Table 8: Model \\(\\theta_1\\) drift (c) R2 BIC LB (0,1,0) NA NA 0.00 1396 22.2 versus from auto.arima() ## Warning in mean.default(sardine, na.rm = TRUE): argument is not numeric or ## logical: returning NA ## Warning in Ops.factor(left, right): &#39;-&#39; not meaningful for factors Model theta1 drift R2 BIC LB (0,1,0) 0.5731337 0.0640889 -Inf -3.584377 5.372543 3.4.4 Missing values These functions work fine with missing values. Missing values are denoted NA. anchovy.miss &lt;- anchovy87ts anchovy.miss[10:14] &lt;- NA fit &lt;- auto.arima(anchovy.miss) fit ## Series: anchovy.miss ## ARIMA(1,1,0) with drift ## ## Coefficients: ## ar1 drift ## -0.5622 0.067 ## s.e. 0.2109 0.022 ## ## sigma^2 estimated as 0.02245: log likelihood=6.35 ## AIC=-6.71 AICc=-5.45 BIC=-3.3 3.4.5 Fit a specific ARIMA model Sometimes you don’t want to search, but rather fit an ARIMA model with a specific order. Say you wanted to fit this model: \\[x_t = \\beta_1 x_{t-1} + \\beta_2 x_{t-2} + e_t\\] For that you can use Arima() in the forecast package: fit.AR2 &lt;- forecast::Arima(anchovy87ts, order=c(2,0,0)) fit.AR2 ## Series: anchovy87ts ## ARIMA(2,0,0) with non-zero mean ## ## Coefficients: ## ar1 ar2 mean ## 0.6912 0.2637 9.2353 ## s.e. 0.2063 0.2142 0.5342 ## ## sigma^2 estimated as 0.0511: log likelihood=2.1 ## AIC=3.81 AICc=5.91 BIC=8.52 3.4.6 Model Checking Plot your data Is the plot long-tailed (Chl, some types of fish data)? Take the logarithm. Fit model. Plot your residuals Check your residuals for stationarity, normality, and independence Ideally your response variable will be unimodal. If not, you are using an ARIMA model that doesn’t produce data like yours. While you could change the assumptions about the error distribution in the model, it will be easier to transform your data. Look at histograms of your data: Use checkresiduals() to do basic diagnostics. fit &lt;- forecast::auto.arima(anchovy87ts) checkresiduals(fit) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,1,1) with drift ## Q* = 1.4883, df = 3, p-value = 0.685 ## ## Model df: 2. Total lags used: 5 3.4.7 Workflow for non-seasonal data Go through Box-Jenkins Method to evaluate stationarity Plot the data and make decisions about transformations to make the data more unimodal Make some decisions about differencing and any other data transformations via the stationarity tests Use auto.arima(data, trace=TRUE) to evaluate what ARMA models best fit the data. Fix the differencing if needed. Determine a set of candidate models. Include a null model in the candidate list. naive and naive with drift are typical nulls. Test candidate models for forecast performance with cross-validation (next lecture). 3.4.8 Stepwise vs exhaustive model selection Stepwise model selection is fast and useful if you need to explore many models and it takes awhile to fit each model. Our models fit quickly and we don’t have season in our models. Though it will not make a difference for this particular dataset, in general set stepwise=FALSE to do a more thorough model search. forecast::auto.arima(anchovy87ts, stepwise=FALSE, approximation=FALSE) ## Series: anchovy87ts ## ARIMA(0,1,1) with drift ## ## Coefficients: ## ma1 drift ## -0.5731 0.0641 ## s.e. 0.1610 0.0173 ## ## sigma^2 estimated as 0.03583: log likelihood=6.5 ## AIC=-6.99 AICc=-5.73 BIC=-3.58 3.4.9 Summary auto.arima() in the forecast package is a good choice for selection and fitting of ARIMA models. Arima() is a good choice when you know the order (structure) of the model. You (may) need to know whether the mean of the data should be zero and whether it is stationary around a linear line. include.mean=TRUE means the mean is not zero include.drift=TRUE means fit a model that fluctuates around a trend (up or down) "],
["3-5-forecasting.html", "3.5 Forecasting", " 3.5 Forecasting The basic idea of forecasting with an ARIMA model is the same as forecasting with a time-varying regressiion model. We estimate a model and the parameters for that model. For example, let’s say we want to forecast with ARIMA(2,1,0) model: \\[y_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + e_t\\] where \\(y_t\\) is the first difference of our anchovy data. Let’s estimate the \\(\\beta\\)’s: fit &lt;- Arima(anchovy, order=c(2,1,0)) coef(fit) ## ar1 ar2 ## -0.3347994 -0.1453928 So we will forecast with this model: \\[y_t = -0.3348 y_{t-1} - 0.1454 y_{t-2} + e_t\\] So to get our forecast for 1988, we do this \\[(y_{1988}-y_{1987}) = -0.3348 (y_{1987}-y_{1986}) - 0.1454 (y_{1986}-y_{1985})\\] Thus \\[y_{1988} = y_{1987}-0.3348 (y_{1987}-y_{1986}) - 0.1454 (y_{1986}-y_{1985})\\] Here is R code to do that: anchovy[24]+coef(fit)[1]*(anchovy[24]-anchovy[23])+ coef(fit)[2]*(anchovy[23]-anchovy[22]) ## ar1 ## 10.00938 3.5.1 Forecasting with forecast() forecast(fit, h=h) automates the forecast calculations for us. forecast() takes a fitted object, fit, from arima() and output forecasts for h time steps forward. The upper and lower prediction intervals are also computed. fit &lt;- forecast::auto.arima(sardine87ts, test=&quot;adf&quot;) fr &lt;- forecast::forecast(fit, h=5) fr ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1988 10.03216 9.789577 10.27475 9.661160 10.40317 ## 1989 10.09625 9.832489 10.36001 9.692861 10.49964 ## 1990 10.16034 9.876979 10.44370 9.726977 10.59371 ## 1991 10.22443 9.922740 10.52612 9.763035 10.68582 ## 1992 10.28852 9.969552 10.60749 9.800701 10.77634 We can plot our forecast with prediction intervals. Here is the sardine forecast: plot(fr, xlab=&quot;Year&quot;) Forecast for anchovy spp=&quot;Anchovy&quot; dat &lt;- subset(landings, Species==spp &amp; Year&lt;=1987)$log.metric.tons dat &lt;- ts(dat, start=1964) fit &lt;- forecast::auto.arima(dat) fr &lt;- forecast::forecast(fit, h=5) plot(fr) Forecasts for chub mackerel We can repeat for other species using the same code. spp=&quot;Chub.mackerel&quot; dat &lt;- subset(landings, Species==spp &amp; Year&lt;=1987)$log.metric.tons dat &lt;- ts(dat, start=1964) fit &lt;- forecast::auto.arima(dat) fr &lt;- forecast::forecast(fit, h=5) plot(fr, ylim=c(6,10)) 3.5.2 Missing values Missing values are allowed for arima() and we can product forecasts with the same code. anchovy.miss &lt;- anchovy87ts anchovy.miss[10:14] &lt;- NA fit &lt;- forecast::auto.arima(anchovy.miss) fr &lt;- forecast::forecast(fit, h=5) plot(fr) 3.5.3 Null forecast models Whenever we are testing a forecast model or procedure we have developed, we should test against ‘null’ forecast models. These are standard ‘competing’ forecast models. The Naive forecast The Naive forecast with drift The mean or average forecast The “Naive” forecast The “naive” forecast is simply the last value observed. If we want to prediction landings in 2019, the naive forecast would be the landings in 2018. This is a difficult forecast to beat! It has the advantage of having no parameters. In forecast, we can fit this model with the naive() function. Note this is the same as the rwf() function. fit.naive &lt;- forecast::naive(anchovy87ts) fr.naive &lt;- forecast::forecast(fit.naive, h=5) plot(fr.naive) The “Naive” forecast with drift The “naive” forecast is equivalent to a random walk with no drift. So this \\[x_t = x_{t-1} + e_t\\] As you saw with the anchovy fit, it doesn’t allow an upward trend. Let’s make it a little more flexible by add drift. This means we estimate one term, the trend. \\[x_t = \\mu + x_{t-1} + e_t\\] fit.rwf &lt;- forecast::rwf(anchovy, drift=TRUE) fr.rwf &lt;- forecast::forecast(fit.rwf, h=5) plot(fr.rwf) The “mean” forecast The “mean” forecast is simply the mean of the data. If we want to prediction landings in 2019, the mean forecast would be the average of all our data. This is a poor forecast typically. It uses no information about the most recent values. In forecast, we can fit this model with the Arima() function and order=c(0,0,0). This will fit this model: \\[x_t = e_t\\] where \\(e_t \\sim N(\\mu, \\sigma)\\). fit.mean &lt;- forecast::Arima(anchovy, order=c(0,0,0)) fr.mean &lt;- forecast::forecast(fit.mean, h=5) plot(fr.mean) "],
["4-exponential-smoothing-models.html", "Chapter 4 Exponential Smoothing Models", " Chapter 4 Exponential Smoothing Models Reference. Rob J Hyndman (lead on the forecast package) and George Athanasopoulos have an excellent online text on practical forecasting and exponential smoothing. Read their chapter on exponential smoothing to learn more about these models and how to use them. "],
["4-1-naive-forecast.html", "4.1 Naive forecast", " 4.1 Naive forecast For a naive forecast of the anchovy landings in 1988, we just use the 1987 landings. \\[\\hat{x}_{1988} = x_{1987}\\] Which is the same as saying that we put 100% of the ‘weight’ on the most recent value and no weight on any value prior to that. \\[\\hat{x}_{1988} = 1 \\times x_{1987} + 0 \\times x_{1986} + 0 \\times x_{1985} + \\dots\\] Past values in the time series have information about the current state, but only the most recent past value. That’s a bit extreme. Often the values prior to the last value also have some information about future states. But the ‘information content’ should decrease the farther in the past that we go. "],
["4-2-simple-ets.html", "4.2 Simple ETS", " 4.2 Simple ETS Simple exponential smoothing uses this type of weighting that falls off exponentially and the objective to estimate the best weighting ( \\(\\alpha\\) ): "],
["4-3-fitting-with-ets.html", "4.3 Fitting with ets()", " 4.3 Fitting with ets() The forecast package will fit a wide variety of exponential smoothing models. The main fitting function is ets(): ets(y, model = &quot;ZZZ&quot;, &lt; + many other arguments &gt;) y : your data. A time series of responses. model: what type of exponential smoothing model. We are going to use ets() to fit three simple types of exponential smoothing models: model “ZZZ” alternate function exponential smoothing no trend “ANN” ses() exponential smoothing with trend “AAN” holt() exponential smoothing choose trend “AZN” NA The alternate function does exactly the same fitting. It is just a ‘shortcut’. 4.3.1 Exponential smoothing with no trend This is like the naive model that just uses the last value to make the forecast, but instead of only using the last value it will use values farther in the past also. The weighting fall off exponentially. Load the data and forecast package. load(&quot;landings.RData&quot;) fit &lt;- forecast::ets(anchovy87ts, model=&quot;ANN&quot;) fr &lt;- forecast::forecast(fit, h=5) plot(fr) Look at the estimates fit ## ETS(A,N,N) ## ## Call: ## forecast::ets(y = anchovy87ts, model = &quot;ANN&quot;) ## ## Smoothing parameters: ## alpha = 0.7065 ## ## Initial states: ## l = 8.5553 ## ## sigma: 0.2166 ## ## AIC AICc BIC ## 6.764613 7.964613 10.298775 4.3.2 The weighting function 4.3.3 Produce forecast using a previous fit Say you want to estimate a forecasting model from one dataset and use that model to forecast another dataset or another area. Here is how to do that. This is the fit to the 1964-1987 data: fit1 &lt;- forecast::ets(anchovy87ts, model=&quot;ANN&quot;) Use that model with the 2000-2007 data and produce a forecast: dat &lt;- subset(landings, Species==&quot;Anchovy&quot; &amp; Year&gt;=2000 &amp; Year&lt;=2007) dat &lt;- ts(dat$log.metric.tons, start=2000) fit2 &lt;- forecast::ets(dat, model=fit1) ## Model is being refit with current smoothing parameters but initial states are being re-estimated. ## Set &#39;use.initial.values=TRUE&#39; if you want to re-use existing initial values. fr2 &lt;- forecast::forecast(fit2, h=5) plot(fr2) 4.3.4 Naive model with drift Fit a model that uses the last observation as the forecast but includes a trend estimated from ALL the data. This is what the naive model with drift does. fit.rwf &lt;- forecast::Arima(anchovy87ts, order=c(0,1,0), include.drift=TRUE) fr.rwf &lt;- forecast::forecast(fit.rwf, h=5) plot(fr.rwf) The trend seen in the blue line is estimated from the overall trend in ALL the data. coef(fit.rwf) ## drift ## 0.06577281 The trend from all the data is (last-first)/(number of steps). mean(diff(anchovy87ts)) ## [1] 0.06577281 So we only use the latest data to choose the level for our forecast but use all the data to choose the trend? It would make more sense to weight the more recent trends more heavily. 4.3.5 Exponential smoothing model with trend The exponential smoothing model with trend does this. The one-year trend is \\[x_t - x_{t-1}\\] That is how much the data increased or decreased. plot(diff(anchovy87ts),ylim=c(-0.3,.3)) abline(h=0, col=&quot;blue&quot;) abline(h=mean(diff(anchovy87ts)),col=&quot;red&quot;) title(&quot;0 means no change&quot;) If we take the average of all \\(x_t - x_{t-1}\\) we are using the average trend like the naive model with drift. We put an equal weighting on all trends. But we could use a weighting that falls off exponentially so that we more recent trends affect the forecast more than trends in the distant past. That is what an exponential smoothing model with trend does. 4.3.6 Naive model with trend If your training data are length \\(T\\), then a forecast for \\(T+h\\) is \\[\\hat{x}_{T+h} = l_T + h \\bar{b}\\] where \\(\\hat{b}\\) is the mean of the the yearly changes in \\(x\\), so the mean of \\(x_t - x_{t-1}\\). \\[\\hat{b} = \\sum_{t=2}^T (x_t - x_{t-1})\\] 4.3.7 Exponential smoothing model with trend \\[\\hat{x}_{T+h} = l_T + h b_T\\] where \\(b_T\\) is a weighted average with the more recent trends given more weight. \\[b_t = \\sum_{t=2}^T \\beta (1-\\beta)^{t-2}(x_t - x_{t-1})\\] Fit exponential smoothing with trend fit &lt;- forecast::ets(anchovy87ts, model=&quot;AAN&quot;) fr &lt;- forecast::forecast(fit, h=5) plot(fr) 4.3.8 Decomposing your model fit Sometimes you would like to see the smoothed level and smoothed trend that the model estimated. You can see that with plot(fit) or autoplot(fit). autoplot(fit) 4.3.9 Forecast performance We can evaluate the forecast performance with forecasts of our test data or we can use all the data and use time-series cross-validation. Let’s start with the former. 4.3.10 Test forecast performance Test against a test data set We will fit an an exponential smoothing model with trend to the training data and make a forecast for the years that we ‘held out’. fit1 &lt;- forecast::ets(traindat, model=&quot;AAN&quot;) h=length(testdat) fr &lt;- forecast::forecast(fit1, h=h) plot(fr) points(length(traindat)+1:h, testdat, pch=2, col=&quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;forecast&quot;,&quot;actual&quot;), pch=c(20,2), col=c(&quot;blue&quot;,&quot;red&quot;)) We can calculate a variety of forecast error metrics with forecast::accuracy(fr, testdat) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0155561 0.1788989 0.1442712 0.1272938 1.600532 0.7720807 ## Test set -0.5001701 0.5384355 0.5001701 -5.1678506 5.167851 2.6767060 ## ACF1 ## Training set -0.008371542 ## Test set NA We would now repeat this for all the models in our candidate set and choose the model with the best forecast performance. Test using time-series cross-validation Another approach is to use all the data and test a series of forecasts made by fitting the model to different lengths of the data. In this approach, we don’t have test data. Instead we will use all the data for fitting and for forecast testing. We will redefine traindat as all our Anchovy data. tsCV() function We will use the tsCV() function. We need to define a function that returns a forecast. far2 &lt;- function(x, h, model){ fit &lt;- ets(x, model=model) forecast(fit, h=h) } Now we can use tsCV() to run our far2() function to a series of training data sets. We will specify that a NEW ets model be estimated for each training set. We are not using the weighting estimated for the whole data set but estimating the weighting new for each set. The e are our forecast errors for all the forecasts that we did with the data. e &lt;- forecast::tsCV(traindat, far2, h=1, model=&quot;AAN&quot;) e ## Time Series: ## Start = 1 ## End = 26 ## Frequency = 1 ## [1] -0.245378390 0.366852341 0.419678595 -0.414861770 -0.152727933 ## [6] -0.183775208 -0.013799590 0.308433377 -0.017680471 -0.329690537 ## [11] -0.353441463 0.266143346 -0.110848616 -0.005227309 0.157821831 ## [16] 0.196184446 0.008135667 0.326024067 0.085160559 0.312668447 ## [21] 0.246437781 0.117274740 0.292601670 -0.300814605 -0.406118961 ## [26] NA Let’s look at the first few e so we see exactly with tsCV() is doing. e[2] ## [1] 0.3668523 This uses training data from \\(t=1\\) to \\(t=2\\) so fits an ets to the first two data points alone. Then it creates a forecast for \\(t=3\\) and compares that forecast to the actual value observed for \\(t=3\\). TT &lt;- 2 # end of the temp training data temp &lt;- traindat[1:TT] fit.temp &lt;- forecast::ets(temp, model=&quot;AAN&quot;) fr.temp &lt;- forecast::forecast(fit.temp, h=1) traindat[TT+1] - fr.temp$mean ## Time Series: ## Start = 3 ## End = 3 ## Frequency = 1 ## [1] 0.3668523 e[3] ## [1] 0.4196786 This uses training data from \\(t=1\\) to \\(t=2\\) so fits an ets to the first two data points alone. Then it creates a forecast for \\(t=3\\) and compares that forecast to the actual value observed for \\(t=3\\). TT &lt;- 3 # end of the temp training data temp &lt;- traindat[1:TT] fit.temp &lt;- forecast::ets(temp, model=&quot;AAN&quot;) fr.temp &lt;- forecast::forecast(fit.temp, h=1) traindat[TT+1] - fr.temp$mean ## Time Series: ## Start = 4 ## End = 4 ## Frequency = 1 ## [1] 0.4196786 4.3.11 Testing a specific ets model By specifying model=&quot;AAN&quot;, we estimated a new ets model (meaning new weighting) for each training set used. We might want to specify that we use only the weighting we estimated for the full data set. We do this by passing in a fit to model. The e are our forecast errors for all the forecasts that we did with the data. fit1 below is the ets estimated from all the data 1964 to 1989. Note, the code will produce a warning that it is estimating the initial value and just using the weighting. That is what we want. fit1 &lt;- forecast::ets(traindat, model=&quot;AAN&quot;) e &lt;- forecast::tsCV(traindat, far2, h=1, model=fit1) e ## Time Series: ## Start = 1 ## End = 26 ## Frequency = 1 ## [1] NA 0.576663901 1.031385937 0.897828249 1.033164616 ## [6] 0.935274283 0.958914499 1.265427119 -0.017241938 -0.332751184 ## [11] -0.330473144 0.255886314 -0.103926617 0.031206730 0.154727479 ## [16] 0.198328366 -0.020605522 0.297475742 0.005297401 0.264939892 ## [21] 0.196256334 0.129798648 0.335887872 -0.074017535 -0.373267163 ## [26] NA 4.3.12 Forecast accuracy metrics Now we can compute forecast accuracy metrics from the forecast errors (e). RMSE: root mean squared error rmse &lt;- sqrt(mean(e^2, na.rm=TRUE)) MAE: mean absolute error mae &lt;- mean(abs(e), na.rm=TRUE) "],
["5-testing-forecast-accuracy.html", "Chapter 5 Testing forecast accuracy", " Chapter 5 Testing forecast accuracy Once you have found a set of possible forecast models, you are ready to compare forecasts from a variety of models and choose a forecast model. The are multiple metric you can use to quantify forecast accuracy and different approaches you can take for creating the forecasts that you test. "],
["5-1-measures-of-forecast-accuracy.html", "5.1 Measures of forecast accuracy", " 5.1 Measures of forecast accuracy To measure the forecast fit, we fit a model to training data and test a forecast against data in a test set. We ‘held out’ the test data and did not use it at all in our fitting. Stergiou and Christou used 1964-1987 as their training data and tested their forecasts against 1988 and 1989. This is a training set/test set approach to forecast performance evaluation. 5.1.0.1 Forecast versus actual We will fit to the training data and make a forecast for the test data. We can then compare the forecast to the actual values in the test data. fit1 &lt;- forecast::auto.arima(traindat) fr &lt;- forecast::forecast(fit1, h=2) fr ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 25 10.03216 9.789577 10.27475 9.661160 10.40317 ## 26 10.09625 9.832489 10.36001 9.692861 10.49964 Plot the forecast and compare to the actual values in 1988 and 1989. plot(fr) points(25:26, testdat, pch=2, col=&quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;forecast&quot;,&quot;actual&quot;), pch=c(20,2), col=c(&quot;blue&quot;,&quot;red&quot;)) 5.1.1 Metrics How to we quantify the difference between the forecast and the actual values in the test data set? fr.err &lt;- testdat - fr$mean fr.err ## Time Series: ## Start = 25 ## End = 26 ## Frequency = 1 ## [1] -0.1704302 -0.4944778 The accuracy() function in forecast provides many different metrics such as mean error, root mean square error, mean absolute error, mean percentage error, mean absolute percentage error. ME Mean err me &lt;- mean(fr.err) me ## [1] -0.332454 RMSE Root mean squared error rmse &lt;- sqrt(mean(fr.err^2)) rmse ## [1] 0.3698342 MAE Mean absolute error mae &lt;- mean(abs(fr.err)) mae ## [1] 0.332454 MPE Mean percentage error fr.pe &lt;- 100*fr.err/testdat mpe &lt;- mean(fr.pe) mpe ## [1] -3.439028 MAPE Mean absolute percentage error mape &lt;- mean(abs(fr.pe)) mape ## [1] 3.439028 accuracy(fr, testdat)[,1:5] ## ME RMSE MAE MPE MAPE ## Training set -0.00473511 0.1770653 0.1438523 -0.1102259 1.588409 ## Test set -0.33245398 0.3698342 0.3324540 -3.4390277 3.439028 c(me, rmse, mae, mpe, mape) ## [1] -0.3324540 0.3698342 0.3324540 -3.4390277 3.4390277 5.1.2 Test multiple models Now that you have some metrics for forecast accuracy, you can compute these for all the models in your candidate set. # The model picked by auto.arima fit1 &lt;- forecast::Arima(traindat, order=c(0,1,1)) fr1 &lt;- forecast::forecast(fit1, h=2) test1 &lt;- forecast::accuracy(fr1, testdat)[2,1:5] # AR-1 fit2 &lt;- forecast::Arima(traindat, order=c(1,1,0)) fr2 &lt;- forecast::forecast(fit2, h=2) test2 &lt;- forecast::accuracy(fr2, testdat)[2,1:5] # Naive model with drift fit3 &lt;- forecast::rwf(traindat, drift=TRUE) fr3 &lt;- forecast::forecast(fit3, h=2) test3 &lt;- forecast::accuracy(fr3, testdat)[2,1:5] Show a summary ME RMSE MAE MPE MAPE (0,1,1) -0.293 0.320 0.293 -3.024 3.024 (1,1,0) -0.309 0.341 0.309 -3.200 3.200 Naive -0.483 0.510 0.483 -4.985 4.985 5.1.3 Cross-Validation An alternate approach to testing a model’s forecast accuracy is to use cross-validation. This approach uses windows or shorter segments of the whole time series to make a series of single forecasts. We can use either a sliding or a fixed window. For example for the Anchovy time series, we could fit the model 1964-1973 and forecast 1974, then 1964-1974 and forecast 1975, then 1964-1975 and forecast 1976, and continue up to 1964-1988 and forecast 1989. This would create 16 forecasts to test. The window is ‘sliding’ because the length of the time series used for fitting the model, keeps increasing by 1. Another approach uses a fixed window. For example, a 10-year window. 5.1.3.1 Time-series cross-validation with tsCV() far2 &lt;- function(x, h, order){ forecast::forecast(Arima(x, order=order), h=h) } e &lt;- forecast::tsCV(traindat, far2, h=1, order=c(0,1,1)) tscv1 &lt;- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE)) tscv1 ## ME RMSE MAE ## 0.1128788 0.2261706 0.1880392 Compare to RMSE from just the 2 test data points. test1[c(&quot;ME&quot;,&quot;RMSE&quot;,&quot;MAE&quot;)] ## ME RMSE MAE ## -0.2925326 0.3201093 0.2925326 Cross-validation farther in future Compare accuracy of forecasts 1 year out versus 4 years out. If h is greater than 1, then the errors are returned as a matrix with each h in a column. Column 4 is the forecast, 4 years out. e &lt;- forecast::tsCV(traindat, far2, h=4, order=c(0,1,1))[,4] #RMSE tscv4 &lt;- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE)) rbind(tscv1, tscv4) ## ME RMSE MAE ## tscv1 0.1128788 0.2261706 0.1880392 ## tscv4 0.2839064 0.3812815 0.3359689 Cross-validation with a fixed window Compare accuracy of forecasts with a fixed 10-year window and 1-year out forecasts. e &lt;- forecast::tsCV(traindat, far2, h=1, order=c(0,1,1), window=10) #RMSE tscvf1 &lt;- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE)) tscvf1 ## ME RMSE MAE ## 0.1387670 0.2286572 0.1942840 All the forecasts tests together comp.tab &lt;- rbind(test1=test1[c(&quot;ME&quot;,&quot;RMSE&quot;,&quot;MAE&quot;)], slide1=tscv1, slide4=tscv4, fixed1=tscvf1) knitr::kable(comp.tab, format=&quot;html&quot;) ME RMSE MAE test1 -0.2925326 0.3201093 0.2925326 slide1 0.1128788 0.2261706 0.1880392 slide4 0.2839064 0.3812815 0.3359689 fixed1 0.1387670 0.2286572 0.1942840 "],
["5-2-testing-multiple-models.html", "5.2 Testing multiple models", " 5.2 Testing multiple models For each model, you will do the same steps: Fit the model Create forecasts for a test data set or use cross-validation Compute forecast accuracy metrics for the forecasts Note when you compare models, you can use both ‘training data/test data’ and use time-series cross-validation, but report the metrics in separate columns. Example, ‘RMSE from tsCV’ and ‘RMSE from test data’. 5.2.1 Example candidate model set for anchovy Exponential smoothing model with trend fit &lt;- forecast::ets(traindat, model=&quot;AAN&quot;) fr &lt;- forecast::forecast(fit, h=1) Exponential smoothing model no trend fit &lt;- forecast::ets(traindat, model=&quot;ANN&quot;) fr &lt;- forecast::forecast(fit, h=1) ARIMA(0,1,1) with drift (best) fit &lt;- forecast::Arima(traindat, order(0,1,1), include.drift=TRUE) fr &lt;- forecast::forecast(fit, h=1) ARIMA(2,1,0) with drift (within 2 AIC of best) fit &lt;- forecast::Arima(traindat, order(2,1,0), include.drift=TRUE) fr &lt;- forecast::forecast(fr) Time-varying regression with linear time traindat$t &lt;- 1:24 fit &lt;- lm(log.metric.tons ~ t, data=traindat) fr &lt;- forecast::forecast(fit, newdata=data.frame(t=25)) Null models Naive no trend fit &lt;- forecast::Arima(traindat, order(0,1,0)) fr &lt;- forecast::forecast(fit, h=1) # or simply fr &lt;- forecast::rwf(traindat) Naive with trend fit &lt;- forecast::Arima(traindat, order(0,1,0), include.drift=TRUE) fr &lt;- forecast::forecast(fit) # or simply fr &lt;- forecast::rwf(traindat, drift=TRUE) Average or mean fit &lt;- forecast::Arima(traindat, order(0,0,0)) fr &lt;- forecast::forecast(fit) "],
["6-covariates.html", "Chapter 6 Covariates", " Chapter 6 Covariates Often we want to explain the variability in our data using covariates or exogenous variables. We may want to do this in order to create forecasts using information from the covariates in time step \\(t-1\\) or \\(t\\) to help forecast at time \\(t\\). Or we may want to understand what causes variability in our data in order to help understand the underlying process. We can include covariates in the time-varying regression model and the ARIMA models. We cannot include covariates in an exponential smoothing model. That doesn’t make sense as a exponential model is a type of filter of the data not a ‘process’ model. "],
["6-1-multivariate-linear-regression-with-arma-errors.html", "6.1 Multivariate linear regression with ARMA errors", " 6.1 Multivariate linear regression with ARMA errors The stats::arima() and forecast::auto.arima() functions with argument xreg fit a multivariate linear regression with ARMA errors. Note, this is not what is termed a ARMAX model. ARMAX models will be addressed separately. The model fitted when xreg is passed in is: \\[\\begin{equation} \\begin{gathered} x_t = \\alpha + \\phi_1 c_{t,1} + \\phi_2 c_{t,2} + \\dots + z_t \\\\ z_t = \\beta_1 z_{t-1} + \\dots + \\beta_p z_{t-p} + e_t + \\theta_1 e_{t-1} + \\dots + \\theta_q e_{t-q}\\\\ e_t \\sim N(0,\\sigma) \\end{gathered} \\end{equation}\\] where xreg is matrix with \\(c_{t,1}\\) in column 1, \\(c_{t-2}\\) in column 2, etc. \\(z_t\\) are the ARMA errors. 6.1.1 Example: anchovy 6.1.2 MREG of first or second differences In the multivariate regression with ARMA errors, the response variable \\(x_t\\) is not necessarily stationary since the covariates \\(c_t\\)’s need not be stationary. If we wish to model the first or second differences of \\(x_t\\), then we are potentially modeling a stationary process if differencing leads to a stationary process. We need to think carefully about how we set up a multivariate regression if our response variable is stationary. One recommendation is if \\(x_t\\) is differenced, the same differencing is applied to the covariates. The idea is if the response variable is stationary, we want to make sure that the independent variables are also stationary. However, in a fisheries application \\(x_t - x_{t-1}\\) often has a biological meaning, the yearly (or monthly or hourly) rate of change, and that rate of change is what one is trying explain with a covariate. One would not necessarily expect the first difference to be stationary and one is trying to explain any trend in the one-step rate of change with some set of covariates. On the other hand, if the response variable, the raw data or the first or second difference, is stationary then trying to explain its variability via a non-stationary covariate will clearly lead to the effect size of the covariates being zero. We don’t need to fit a model to tell us that. \\(x_t - x_{t-1}\\). "],
["7-seasonality.html", "Chapter 7 Seasonality", " Chapter 7 Seasonality To work with seasonal data, we need to turn our data into a ts object, which is a “time-series” object in R. This will allow us to specify the seasonality. It is important that we do not leave out any data in our time series. You data should look like so Year Month metric.tons 2018 1 1 2018 2 2 2018 3 3 ... 2019 1 4 2019 2 6 2019 3 NA The months are in order and the years are in order. 7.0.1 Load the chinook salmon data set load(&quot;chinook.RData&quot;) head(chinook) Year Month Species log.metric.tons metric.tons 1990 Jan Chinook 3.4&nbsp; 29.9 1990 Feb Chinook 3.81 45.1 1990 Mar Chinook 3.51 33.5 1990 Apr Chinook 4.25 70&nbsp;&nbsp; 1990 May Chinook 5.2&nbsp; 181&nbsp;&nbsp; 1990 Jun Chinook 4.37 79.2 The data are monthly and start in January 1990. To make this into a ts object do chinookts &lt;- ts(chinook$log.metric.tons, start=c(1990,1), frequency=12) start is the year and month and frequency is the number of months in the year. If we had quarterly data that started in 2nd quarter of 1990, our call would be ts(chinook, start=c(1990,2), frequency=4) If we had daily data starting on hour 5 of day 10 and each row was an hour, our call would be ts(chinook, start=c(10,5), frequency=24) Use ?ts to see more examples of how to set up ts objects. 7.0.2 Plot seasonal data Now that we have specified our seasonal data as a ts object, it is easy to plot because R knows what the season is. plot(chinookts) "],
["7-1-seasonal-exponential-smoothing-model.html", "7.1 Seasonal Exponential Smoothing Model", " 7.1 Seasonal Exponential Smoothing Model Now we add a few more lines to our ETS table of models: model “ZZZ” alternate function exponential smoothing no trend “ANN” ses() exponential smoothing with trend “AAN” holt() exponential smoothing with season no trend “ANA” NA exponential smoothing with season and trend “AAA” NA estimate best trend and season model “ZZZ” NA Unfortunately ets() will not handle missing values and will find the longest continuous piece of our data and use that. library(forecast) traindat &lt;- window(chinookts, c(1990,1), c(1999,12)) fit &lt;- forecast::ets(traindat, model=&quot;AAA&quot;) ## Warning in forecast::ets(traindat, model = &quot;AAA&quot;): Missing values ## encountered. Using longest contiguous portion of time series fr &lt;- forecast::forecast(fit, h=24) plot(fr) points(window(chinookts, c(1996,1), c(1996,12))) 7.1.1 Force seasonality to evolve more If we plot the decomposition, we see the the seasonal component is not changing over time, unlike the actual data. The bar on the right, alerts us that the scale on the 3rd panel is much smaller. autoplot(fit) Pass in a high gamma (the season weighting) to force the seasonality to evolve. fit &lt;- forecast::ets(traindat, model=&quot;AAA&quot;, gamma=0.4) ## Warning in forecast::ets(traindat, model = &quot;AAA&quot;, gamma = 0.4): Missing ## values encountered. Using longest contiguous portion of time series autoplot(fit) "],
["7-2-seasonal-arima-model.html", "7.2 Seasonal ARIMA model", " 7.2 Seasonal ARIMA model auto.arima() will recognize that our data has season and fit a seasonal ARIMA model to our data by default. Let’s use the data that ets() used. This is shorter than our training data and is Oct 1990 to Dec 1995. The data used by ets() is returned in fit$x. We will redefine the training data to be the longest segment with no missing values. traindat &lt;- window(chinookts, c(1990,10), c(1995,12)) testdat &lt;- window(chinookts, c(1996,1), c(1996,12)) fit &lt;- forecast::auto.arima(traindat) fr &lt;- forecast::forecast(fit, h=12) plot(fr) points(testdat) "],
["7-3-missing-values-2.html", "7.3 Missing values", " 7.3 Missing values Unlike for an exponential smoothing model, missing values are ok when fitting a seasonal ARIMA model fulldat &lt;- window(chinookts, c(1990,1), c(1999,12)) fit &lt;- forecast::auto.arima(fulldat) fr &lt;- forecast::forecast(fit, h=12) plot(fr) "],
["7-4-forecast-evaluation.html", "7.4 Forecast evaluation", " 7.4 Forecast evaluation We can compute the forecast performance metrics as usual. fit.ets &lt;- forecast::ets(traindat, model=&quot;AAA&quot;) fr &lt;- forecast::forecast(fit.ets, h=12) Look at the forecast so you know what years and months to include in your test data. Pull those 12 months out of your data using the window() function. testdat &lt;- window(chinookts, c(1996,1), c(1996,12)) Use accuracy() to get the forecast error metrics. forecast::accuracy(fr, testdat) ## ME RMSE MAE MPE MAPE ## Training set -0.0001825075 0.5642326 0.4440532 -9.254074 25.40106 ## Test set 0.3143200919 0.7518660 0.6077172 65.753096 81.38568 ## MASE ACF1 Theil&#39;s U ## Training set 0.7364593 0.07490341 NA ## Test set 1.0078949 0.05504107 0.4178409 We can do the same for the ARIMA model. fit &lt;- forecast::auto.arima(traindat) fr &lt;- forecast::forecast(fit, h=12) forecast::accuracy(fr, testdat) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.009977889 0.5677655 0.3965956 0.4645923 27.40975 0.6577511 ## Test set 0.819202544 0.9458134 0.8192025 22.0199537 55.91641 1.3586419 ## ACF1 Theil&#39;s U ## Training set -0.06137508 NA ## Test set -0.02803914 0.6046339 "],
["references-1.html", "References", " References Holmes, Eli, Eric Ward, Mark Scheuerell, Kellie Wills, NOAA, Seattle, and USA. 2018. MARSS: Multivariate Autoregressive State-Space Modeling. https://CRAN.R-project.org/package=MARSS. Hyndman, Rob, George Athanasopoulos, Christoph Bergmeir, Gabriel Caceres, Leanne Chhay, Mitchell O’Hara-Wild, Fotios Petropoulos, Slava Razbash, Earo Wang, and Farah Yasmeen. 2018. Forecast: Forecasting Functions for Time Series and Linear Models. https://CRAN.R-project.org/package=forecast. Trapletti, Adrian, and Kurt Hornik. 2018. Tseries: Time Series Analysis and Computational Finance. https://CRAN.R-project.org/package=tseries. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, and Kara Woo. 2018. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. "],
["A-inputting-data.html", "A Inputting data", " A Inputting data This chapter will illustrate how to input data that is stored in csv files in various common formats. one response variable If your data look like this: Year Species metric.tons 2018, Fish1, 1 2019, Fish1, 2 2018, Fish2, 3 2019, Fish2, 4 2018, Fish3, 6 2019, Fish4, NA with this code: test &lt;- read.csv(&quot;Data/test.csv&quot;, stringsAsFactors = FALSE) save(test, file=&quot;test.RData&quot;) Many response variables Read in a file where the data are in columns. If your data look like this with each species (or site) across the columns: Year,Anchovy,Sardine,Chub mackerel,Horse mackerel,Mackerel,Jack Mackerel 1964,5449.2,12984.4,1720.7,4022.4,NA,NA 1965,4263.5,10611.1,1278.5,4158.3,NA,NA 1966,5146.4,11437.8,802.6,3012.1,NA,NA Use this code: library(reshape2) test &lt;- read.csv(&quot;Data/test.csv&quot;, stringsAsFactors = FALSE) melt(test, id=&quot;Year&quot;, value.name=&quot;metric.tons&quot;, variable.name=&quot;Species&quot;) save(test, file=&quot;test.RData&quot;) Many response variables, two time variables If your data also have, say, a month (or qtr) column, use this code: Year,Month,Anchovy,Sardine,Chub mackerel,Horse mackerel,Mackerel,Jack Mackerel 1964,1,5449.2,12984.4,1720.7,4022.4,NA,NA 1964,2,4263.5,10611.1,1278.5,4158.3,NA,NA 1964,3,5146.4,11437.8,802.6,3012.1,NA,NA Use this code: library(reshape2) test &lt;- read.csv(&quot;Data/test.csv&quot;, stringsAsFactors = FALSE) melt(test, id=c(&quot;Year&quot;,&quot;Month&quot;), value.name=&quot;metric.tons&quot;, variable.name=&quot;Species&quot;) save(test, file=&quot;test.RData&quot;) One response variable, multiple explanatory variables Year, Anchovy, SST, Mackerel 1964, 5449.2, 24.4, 1720.7 1965, 4263.5, 30.1, 1278.5 1966, 5146.4, 23.8, 802.6 Use this code: test &lt;- read.csv(&quot;Data/test.csv&quot;, stringsAsFactors = FALSE) save(test, file=&quot;test.RData&quot;) Use this lm() model (or gam() etc): fit &lt;- lm(Anchovy ~ SST + Mackerel, data=test) "],
["B-downloading-icoads-covariates.html", "B Downloading ICOADS covariates", " B Downloading ICOADS covariates The covariates are those in Stergiou and Christou except that NS winds might not be vertical wind. I used the ICOADS data not the COADSs. The boxes are 1 degree but on 1 degree centers not 0.5 centers. Thus box is 39.5-40.5 not 39-40. The following is the code used to download the covariates from the NOAA ERDDAP server. It creates a list with the monthly data for each box. library(RCurl) library(XML) library(stringr) lat &lt;- c(39,39,40) lon &lt;- c(24,25,25) covs &lt;- list() for(i in 1:3){ loc &lt;- paste0(&quot;[(&quot;,lat[i],&quot;.5):1:(&quot;,lat[i],&quot;.5)][(&quot;,lon[i],&quot;.5):1:(&quot;,lon[i],&quot;.5)]&quot;) url &lt;- paste0(&quot;https://coastwatch.pfeg.noaa.gov/erddap/griddap/esrlIcoads1ge.htmlTable?air[(1964-01-01):1:(2018-08-01T00:00:00Z)]&quot;,loc,&quot;,slp[(1964-01-01):1:(2018-08-01T00:00:00Z)]&quot;,loc,&quot;,sst[(1964-01-01):1:(2018-08-01T00:00:00Z)]&quot;,loc,&quot;,vwnd[(1964-01-01):1:(2018-08-01T00:00:00Z)]&quot;,loc,&quot;,wspd3[(1964-01-01):1:(2018-08-01T00:00:00Z)]&quot;,loc) doc &lt;- getURL(url) cov &lt;- readHTMLTable(doc, which=2, stringsAsFactors=FALSE) coln &lt;- paste0(colnames(cov),&quot;.&quot;,cov[1,]) coln &lt;- str_replace(coln, &quot;\\n&quot;, &quot;&quot;) coln &lt;- str_replace_all(coln, &quot;[*]&quot;, &quot;&quot;) cov &lt;- cov[-1,] colnames(cov) &lt;- coln cov[,1] &lt;- as.Date(cov[,1]) for(j in 2:dim(cov)[2]) cov[,j] &lt;- as.numeric(cov[,j]) covs[[i]] &lt;- cov } Now create the monthly and yearly means. covsmean &lt;- covs[[1]] for(j in 2:dim(cov)[2]) covsmean[,j] &lt;- apply(cbind(covs[[1]][,j], covs[[2]][,j], covs[[3]][,j]),1,mean,na.rm=TRUE) covsmean &lt;- covsmean[,c(-2,-3)] covsmean$Year &lt;- as.factor(format(cov[,1],&quot;%Y&quot;)) covsmean.mon &lt;- covsmean covsmean.year &lt;- data.frame(Year=unique(covsmean$Year)) for(j in 2:(dim(covsmean)[2]-1)) covsmean.year &lt;- cbind(covsmean.year, tapply(covsmean[,j], covsmean$Year, mean, na.rm=TRUE)) colnames(covsmean.year) &lt;- c(&quot;Year&quot;,colnames(covsmean)[2:(dim(covsmean)[2]-1)]) "]
]

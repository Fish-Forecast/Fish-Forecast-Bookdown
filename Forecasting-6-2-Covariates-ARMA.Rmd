## Multivariate linear regression with ARMA errors

```{r f62-load_packages, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
```


The `stats::arima()` and `forecast::auto.arima()` functions with argument `xreg` fit a multivariate linear regression with ARMA errors.  Note, this is not what is termed a ARMAX model. ARMAX models will be addressed separately.  

The model fitted when `xreg` is passed in is:

\begin{equation}
\begin{gathered}
x_t = \alpha + \phi_1 c_{t,1} + \phi_2 c_{t,2} + \dots + z_t \\
z_t = \beta_1 z_{t-1} + \dots + \beta_p z_{t-p} + e_t + \theta_1 e_{t-1} + \dots + \theta_q e_{t-q}\\
e_t \sim N(0,\sigma)
\end{gathered}
\end{equation}
where `xreg` is  matrix with $c_{t,1}$ in column 1, $c_{t-2}$ in column 2, etc.  $z_t$ are the ARMA errors.

### Covariates used in Stergiou and Christou

Stergiou and Christou used five environmental covariates: air temperature (air), sea-level pressure (slp), sea surface temperature (sst), vertical wind speed (vwnd), and wind speed cubed (wspd3).  Monthly values for these covariates in three 1 degree boxes were taken from the COADS database, and then a yearly average over all months in the three boxes was used to compute a yearly average.

These yearly average environmental covariates are in `covsmean.year`, which is part of `landings.RData`.

```{r f37-load_data}
load("landings.RData")
colnames(covsmean.year)
```

The covariates are those in Stergiou and Christou with the following differences. I used the ICOADS data not the COADS.  The boxes are 1 degree but on 1 degree centers not 0.5 centers.  Thus box is 39.5-40.5 not 39-40.  ICOADS does not include 'vertical wind'.  I used NS winds which may be different.  The code to download the ICOADS data is in the appendix.

In addition to the environmental covariates, Stergiou and Christou used many covariates of fishing effort for trawlers, purse seiners, beach seiners, other coastal boats and demersal (sum of trawlers, beach seiners and other coastal boats).  For each fishery type, they used data on number of fishers, number of boats, total engine horse power, total boat tonnage.  They also used an economic variable: value of catch for trawlers, purse seiners, beach seiners, other coastal boats.

```{r fish.cov}
colnames(fish.cov)
```
For anchovy, the fishery effort metrics from the purse seine fishery were used.  

Lastly, *Trachurus*, presumably Mediterranean horse mackerel, is used as a covariate for anchovy, at least.

### Prepare the data

We will start by creating a dataframe with our response variable and independent variables.  We will model anchovy landings as the response variable.  The covariates are lagged by one year, following Stergiou and Christou.  This means that the catch in year $t$ is regressed against the covariates in year $t-1$. 

We set up our data frame as follows. We will use the 1965 to 1987 catch data as the response. We use the 1964 to 1986 covariate data as the response. We use $t$ 1 to 23 as a "year" covariate.

```{r cov.dataframe}
df <- data.frame(Year=sardine$Year, 
                 sardine=sardine$log.metric.tons,
                 anchovy=anchovy$log.metric.tons)
df <- subset(df, Year>=1965 & Year<=1987)
covsmean.year[,"vwnd.m/s"]<- abs(covsmean.year[,"vwnd.m/s"])
df.env <- log(subset(covsmean.year, Year>=1964 & Year<=1986)[,-1])
df.fish <- log(subset(fish.cov, Year>=1964 & Year<=1986)[,-1])
purse.cols <- stringr::str_detect(colnames(df.fish),"Purse.seiners")
df.fish <- df.fish[,purse.cols]
Trachurus <- subset(landings, Year>=1964 & Year<=1986 & Species=="Horse.mackerel")$log.metric.tons
df <- data.frame(
            df, df.env, df.fish, Trachurus
            )
df$Year <- df$Year-df$Year[1]+1
colnames(df) <- sapply(colnames(df), function(x){rev(str_split(x,"Purse.seiners.")[[1]])[1]})
colnames(df) <- sapply(colnames(df), function(x){str_split(x,"[.]")[[1]][1]})
df <- df[,colnames(df)!="VAP"]
```

### Removing collinear variables

In total, there are 13 covariates and 23 years of data. Many of the fishing effort covariates are highly correlated,

```{r pairs.cov}
pairs(df[,c(1,9:13)])
```
Stergiou and Christou do not state if any of the highly collinear variables were excluded.  However it seems likely that they were.  The value and tonnage of the purse seiner catch are almost completely collinear with Year.  These are both also correlated with the horsepower. 
The Hmisc library has a redundancy function that can help identify which variables to remove.  We will take out sardine and anchovy since we always want those in our model.

```{r Hmisc.redun}
library(Hmisc)
a <- redun(~ .,data=df[,-c(2:3)], nk=0)
a$Out
```

This suggests removing TOP and HPP.  I will save the full data frame with all variables in `df.full`.

```{r pairs.cov}
df.full <- df
col.rm <- which(colnames(df) %in% c("TOP", "HPP"))
df <- df[,-1*col.rm]
```

### Model selection with stepwise variable selection

<!--
http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/
-->

Stergiou and Christou state that the covariates to include were selected with stepwise variable selection.  Stepwise variable selection is a type of automatic variable selection.  Stepwise variable selection has many statistical problems and the problems are worse when the covariates are collinear as they are in our case  (see this [link](https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/) for a review of the problems with stepwise variable selection). However I use it here to replicate Stergiou and Christou.  I will follow this with an example of other more robust approaches to model selection for linear regression.

Stergiou and Christou do not give specifics on how they implemented stepwise variable selection. Stepwise variable selection refers to a forward-backward search, however there are many ways we can implement this and different approaches give different answers.  I will illustrate a number of approaches to do this in R.

#### step() {-}

When using the stepAIC() function, we specify the starting model, the smallest model and the largest model.  The latter two are the scope of our search. We set direction equal to "both" to specify stepwise variable selection. The default is to use AIC for the selection criteria.

Let's start with a search that starts with a full model which has all the explanatory variables. The first argument to `step()` is the starting model.

```{r stepAIC}
library(MASS)
null <- lm(anchovy ~ 1, data=df)
full <- lm(anchovy ~ ., data=df)
stepAIC.model1 <- step(full, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0)
stepAIC.model1
```

The AIC for this model is `r round(AIC(stepAIC.model1),digits=1)`. This is a much larger model than that reported in Table 3 of Stergiou and Christou. The model in their Table 3 includes only Year, *Trachurus* catch, SST, and FIP.  The model selected by `stepAIC()` starting from the full model includes Year, *Trachurus* catch, air temperature, vertical wind, BOP, and HPP.

Let's repeat but start the search with the smallest model.

```{r stepAIC2}
stepAIC.model2 <- step(null, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0)
stepAIC.model2
```

This model has an AIC of `r round(AIC(stepAIC.model2),digits=1)`. This AIC is smaller (better), which illustrates that you need to be careful how you set up the search. This selected model is very similar to that in Table 3 except that air temperature instead of SST is selected.  Air temperature and SST are highly correlated, however.

The air temperature is removed from the best model if we use BIC as the model selection criteria. This is done by setting `k=log(n)` where $n$ is sample size.

```{r stepBIC}
stepAIC.model3 <- step(null, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0, k=log(nrow(df)))
stepAIC.model3
```

For sardines, the selected model is also similar to that in Table 3 with the addition of vertical wind to the model.  However, the p-value for vertical wind is large and Stergiou and Christou may have dropped it.

```{r stepAIC2.sardine}
null <- lm(sardine ~ 1, data=df)
full <- lm(sardine ~ ., data=df)
stepAIC.model3 <- step(null, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0)
summary(stepAIC.model3)
```

If we select using BIC, then only FIP is included which is the same model in their Table 3.
```{r stepBIC.sardine}
stepAIC.model4 <- step(null, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0, k=log(nrow(df)))
stepAIC.model4
```

#### leaps() {-}

We can use the leaps package to do a full search of the model space. The function `regsubsets()` will find the `nbest` models of size (number of explanatory variables) 1 to `nvmax`.  We can then plot these against a criteria.  leaps allows us to plot against BIC, Cp (asymptotically the same as AIC), $R^2$ and adjusted $R^2$.  Each row in the plot is a model. The dark shading shows which variables are in the model. On the y-axis, farther away from the x-axis is better, so the models (rows) at the top of the plot are the best models.

```{r leaps}
library(leaps)
models <- regsubsets(anchovy~., data = df, nvmax = 11,
                     method = "seqrep", nbest=1)
plot(models, scale="bic")
```
This plot shows that the best model under BIC has an intercept, Year, FIP and *Trachurus*.

`stepAIC()` uses AIC instead of the AICc (corrected for small sample size).  In our case, $n=23$ which is fairly small.  Using AICc would be better suited for such a small dataset.  leaps does not return AIC or AICc, but we can compute them.  Note that Cp asymptotically has the same ordering as AIC, but $n=23$ is small and it does not have the same ordering as AIC in our case.

First we use `summary()` to get a matrix showing the best model of each size.  This matrix shows what variable is in the best model of each size.
```{r summary.leaps}
smodels <- summary(models)
smodels$which
```

Next we compute AICc from BIC and find the line of the matrix above where AICc is the smallest. `k` is the number of parameters.
```{r summary.leaps}
k <- apply(smodels$which,1,sum)
AICc <- smodels$bic+k*(2+(2*k+2)/(23-k-1)-log(23))
rmin <- which(AICc==min(AICc))
smodels$which[rmin,]
```
This shows that AICc would pick the same model as BIC; one with an intercept, Year, FIP and *Trachurus*.

```{r leaps}
library(leaps)
models <- regsubsets(anchovy~., data = df, nvmax = 4,
                     method = "seqrep", nbest=3)
plot(models, scale="bic")
```


### Model selection with cross-validation

The problems with model selection using searching and selecting with some model fit criteria is that the selected model tends to be over-fit.  The p-values are not correct and the predictive value of the model is not optimal because of over-fitting.

```{r}
library(caret)
# Set seed for reproducibility
#set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 5)
# Train the model
step.model <- train(anchovy~., data = df,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax = 1:8),
                    trControl = train.control
                    )
step.model$results
summary(step.model$finalModel)
```

### Collinearity

Collinearity is near-linear relationships among the explanatory variable, which we saw in the `pairs()` plot. VAP and TOP were removed from the full set of explanatory variables, but we still have collinearity problems in our explanatory variables.

One way to see this is visually with the `pairs()` plot.  Another way is to compute the variance inflation factors (VIF). The variance inflation factor is an estimate of how much larger the variance of that variable coefficient estimate is compared to if the variable were uncorrelated with the other predictor variables in the model.  For example, if the VIF of variable $i$ is 10, then the standard error of the $\beta_i$ for variable $i$ is $\sqrt{10}=3.16$ times larger than if variable $i$ were uncorrelated with the other variables.  The rule of thumb is that any of the variables with VIF greater than 10 have collinearity problems.

The car package will compute VIFs for us.  Let's compute VIF for the full data set with the VAP, TOP and HPP added back in.  I removed these earlier since they are collinear.

```{r vif.car}
library(car)
full <- lm(anchovy ~ ., data=df.full)
vif(full)
```
This shows that Year, HPP and TOP have severe collinearity problems.

Collinearity cause many problems such as inflated standard errors of the coefficients and correspondingly unbiased but highly imprecise estimates of the coefficients, false p-values, and poor predictive accuracy of the model.

### Penalized regression: ridge regression

<!--
https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
-->

For this part, I am going to use the full data set with TOP and HPP added back in.  This is help show the problems caused by highly collinear variables.  I will z-score (demean and standardize variances to 1) so I can compare the effects of the different explanatory variables using the size of the coefficients.

```{r z.score}
dfz <- df.full
for(i in colnames(df)){
  pop_sd <- sd(df[,i])*sqrt((length(df[,i])-1)/(length(df[,i])))
  pop_mean <- mean(df[,i])
  dfz[,i] <- (df[,i]-pop_mean)/pop_sd
}
```

One approach to dealing with variance inflation that arises from collinearity and models with many explanatory variable is penalized regression.  First, let's look at ridge regression.  The basic idea with penalized regression is that you penalize coefficient estimates that are far from 0.  The true coefficients are (likely) not 0 so fundamentally this will lead to biased coefficient estimates but the idea is that the inflated variance of the coefficient estimates is the bigger problem.  

With ridge regression, we will assume that the coefficients have a mean of 0 and a variance of $1/\lambda$. This is our prior on the coefficients.  The $\beta_i$ are the most probable values given the data and the prior. Note, there are many other ways to derive ridge regression.  

We will use the glmnet package to fit the anchovy catch with ridge regression.  To fit with a ridge penalty, we set `alpha=0`.

```{r glmnet.ridge}
library(glmnet)
resp <- colnames(dfz)!="anchovy"
x <- as.matrix(dfz[,resp])
y <- as.matrix(dfz[,"anchovy"])
fit.ridge <- glmnet(x, y, family="gaussian", alpha=0)
```

We need to choose a value for the penalty parameter $\lambda$ (called `s` in `coef.glmnet()`). If $\lambda$ is large, then our prior is that the coefficients are very close to 0.  If our $\lambda$ is small, then our prior is less informative.

We can use cross-validation to choose $\lambda$.  This chooses a $\lambda$ that gives us the lowest out of sample errors. `cv.glmnet()` will do k-fold cross-validation and report the MSE.  We pick the $\lambda$ with the lowest MSE (`lambda.min`) or the largest value of $\lambda$ such that error is within 1 s.e. of the minimum (`lambda.1se`).  This value is computed via cross-validation so will vary.  We will take the average over a number of runs; here 20 for speed but 100 is better.

Once we have a best $\lambda$ to use, we can get the coefficients at that value.
```{r lambda.choice}
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=0)$lambda.min
s.best <- s/n
coef(fit.ridge, s=s.best)
```

I will plot the standardized coefficients for the ordinary least squares coefficients against the coefficients using ridge regression.

```{r coef.ridge, echo=FALSE}
full <- lm(anchovy ~ ., data=dfz)
coef.ridge <- as.vector(coef(fit.ridge, s=s.best))[-1]
coef.full <- coef(full)[-1]
labs <- rownames(coef(fit.ridge, s=s.best))[-1]
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.full,coef.ridge), names.arg=labs, 
        horiz=TRUE, las=2, beside=TRUE,
        col=c("aquamarine3","coral"))
legend("topright", c("ols","ridge"), pch=15, 
       col=c("aquamarine3","coral"), 
       bty="n")
par(op)
```

This shows the problem caused by the highly collinear TOP and HPP.  They have highly inflated coefficient estimates that are offset by an inflated Year coefficient (in the opposite direction).  This is why we need to evaluate collinearity in our variables before fitting a linear regression.

With ridge regression, all the estimates have shrunk towards 0 (as they should) but the collinear variables still have very large coefficients.

### Penalized regression: Lasso

In ridge regression, the coefficients will be shrunk towards 0 but none will be set to 0 (unless the OLS estimate happens to be 0).  Lasso is a type of regression that uses a penalty function where 0 is an option.  Lasso does a combination of variable selection and shrinkage.
 
We can do lasso with `glmnet()` by setting `alpha=1`.

```{r glmnet.lasso}
fit.lasso <- glmnet(x, y, family="gaussian", alpha=1)
```
We select the best $\lambda$ as we did for ridge regression using cross-validation.
```{r lambda.choice}
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=1)$lambda.min
s.best <- s/n
coef.lasso <- as.vector(coef(fit.lasso, s=s.best))[-1]
```

We can compare to the estimates from ridge and OLS and see that the model is now more similar the models we got from stepwise variable selection.
The main difference is that slp is included as a variable.

```{r coef.lasso.plot, echo=FALSE}
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.ridge, coef.lasso), 
        names.arg=labs, xlim=c(-0.2,1.5),
        horiz=TRUE, las=2, beside=TRUE,
        col=c("coral", "black"))
legend("topright", c("ridge", "lasso"), pch=15, 
       col=c("coral", "black"), 
       bty="n")
par(op)
```

Lasso has estimated a model that is similar to what we got with stepwise variable selection without removing the collinear variables from our data set.  Year, air temperature, slp and FIP are the most important variables.

### Penalized regression: Elastic net

<!--
https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
-->

Elastic net is uses both L1 and L2 regularization. Elastic regression generally works well when we have a big dataset.  We do not have a big dataset but we will try elastic net.  You can tune the amount of L1 and L2 mixing by adjusting `alpha` but for this example, we will just use `alpha=0.5`.

```{r glmnet.elastic.net}
fit.en <- glmnet(x, y, family="gaussian", alpha=0.5)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=0.5)$lambda.min
s.best <- s/n
coef.en <- as.vector(coef(fit.en, s=s.best))[-1]
```


```{r coef.en.plot, echo=FALSE}
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.ridge, coef.lasso, coef.en), 
        names.arg=labs, xlim=c(-0.2,1.5),
        horiz=TRUE, las=2, beside=TRUE,
        col=c("coral", "black", "red"))
legend("topright", c("ridge", "lasso", "elastic net"), pch=15, 
       col=c("coral", "black", "red"), 
       bty="n")
par(op)
```
As we might expect, elastic net is part way between the ridge regression model and the Lasso model.

### Model selection with prior knowledge

In Section 4.7.7 of  (Harrell 2015), a rule of thumb (based on shrinkage) for the number of predictors that can be used without overfitting is given: $(LR-p)/9$ where $LR$ is the likelihood ratio test $\chi^2$ of the full model against the null model with only intercept and $p$ is the number of variables in the full model.

```{r eval.shrinkage}
null <- lm(anchovy ~ 1, data=df)
full <- lm(anchovy ~ ., data=df)
a <- lrtest (null, full)
(a$Chisq[2]-a$Df[2])/9
```

This rule of thumb suggests that we could include six variables.  Let's try sardine, SST, BOP, HPP, and *Trachurus*.

```{r fit.prior}
fit.prior <- lm(anchovy ~ sardine+BOP+HPP+Trachurus, data=df.full)
```

## Comparing model prediction accuracy

### MREG of first or second differences

In the multivariate regression with ARMA errors, the response variable $x_t$ is not necessarily stationary since the covariates $c_t$'s need not be stationary.  If we wish to model the first or second differences of $x_t$, then we are potentially modeling a stationary process if differencing leads to a stationary process.
We need to think carefully about how we set up a multivariate regression if our response variable is stationary.

One recommendation is if $x_t$ is differenced, the same differencing is applied to the covariates.  The idea is if the response variable is stationary, we want to make sure that the independent variables are also stationary.  However, in a fisheries application $x_t - x_{t-1}$ often has a biological meaning, the yearly (or monthly or hourly) rate of change, and that rate of change is what one is trying explain with a covariate.  One would not necessarily expect the first difference to be stationary and one is trying to explain any trend in the one-step rate of change with some set of covariates.  On the other hand, if the response variable, the raw data or the first or second difference, is stationary then trying to explain its variability via a non-stationary covariate will clearly lead to the effect size of the covariates being zero.  We don't need to fit a model to tell us that.


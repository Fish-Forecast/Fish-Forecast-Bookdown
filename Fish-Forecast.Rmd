--- 
title: "Fisheries Catch Forecasting"
author: "Elizabeth Holmes"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: "fish-forecast/Fish-Forecast-Bookdown/"
description: "This book will show you how to model and forecast annual and seasonal fisheries catches using R and its time-series analysis functions and packages. Forecasting using time-varying regression, ARIMA (Box-Jenkins) models, and expoential smoothing models is demonstrated using real catch time series.  The entire process from data evaluation and diagnostics, model fitting, model selection and forecast evaluation is shown. The focus of the book is on univariate time series (annual or seasonal), however multivariate regression with autocorrelated errors and multivariate autoregressive models (MAR) are covered briefly.  For multivariate autoregressive models and multivariate autoregressive state-space models for fisheries and environmental sciences, see Holmes, Ward and Scheuerell (2018)."
cover-image: "images/cover.png"
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.height=5, fig.align="center", echo=TRUE, cache=FALSE)
library(huxtable)
# devtools::install_github("rstudio/fontawesome")
library(fontawesome)
library(ggplot2)
library(forecast)
library(tseries)
```

# Preface {-}

This book will show you how to use R to model and forecast catch time series using a variety of standard forecasting models. 

<img style="float: right" src="https://rverse-tutorials.github.io/Fish-Forecast-Training-Course/images/fish-forecast.jpg" width=20%>

- Time-varying regression
- Box-Jenkins (ARIMA) models
- Exponential smoothing
- Multivaritate regression with ARMA errors
- ARMA models with covariates (ARMAX)
- Seasonal ARIMA models
- Seasonal exponential smoothing models

In addition to model fitting, model diagnostics, forecast diagnostics and accuracy metrics will be covered along with uncertainty metrics.

The focus of this book is on analysis of univariate time series. However multivariate regression with autocorrelated errors and multivariate autoregressive models (MAR) will be covered more briefly.  For an indepth discussion of multivariate autoregressive models and multivariate autoregressive state-space models, see Holmes, Ward and Scheuerell (2018).

---

<a rel="license" href="http://unlicense.org/"><img alt="Unicense" style="border-width:0" src="http://unlicense.org/pd-icon.png" /></a><br />As a work of the [United States government](https://www.usa.gov/), this project is in the public domain within the United States of America.  Additionally, we waive copyright and related rights in the work worldwide through the Unlicense public domain dedication. <a rel="license" href="http://unlicense.org/">Unlicense public domain dedication</a>.


<!--chapter:end:index.Rmd-->

# Introduction

There are many approaches for forecasting from time series alone--meaning without any covariates or exogenous variables. Examples are the approaches used in the following papers.

**Stergiou and Christou 1996**

- Time-varying regression
- Box-Jenkins models, aka ARIMA models
- Multivariate time-series approaches
    - Harmonic regression
    - Dynamic regression
    - Vector autoregression (MAR)
- Exponential smoothing (2 variants)
- Exponential surplus yield model (FOX)

**Georgakarakos et al. 2006**

- Box-Jenkins models, aka ARIMA models
- Artificial neural networks (ANNs)
- Bayesian dynamic models

**Lawer 2016**

- Box-Jenkins models, aka ARIMA models
- Artificial neural networks (ANNs)
- Exponential Smoothing (6 variants)

This course will focus on three of these methods: time-varying regression, ARIMA models and Exponential smoothing models.  These will be shown with and without seasonality.  Methods which use covariates, or exogenous variables, will also be addressed.

## Stergiou and Christou 1996

These three methods will be demonstrated by replicating the work in Stergiou and Christou (1996) *Modelling and forecasting annual fisheries catches: comparison of regression, univariate and multivariate time series methods*. Fisheries Research 25: 105-136.

![](./figs/StergiouChristou1996.png)

### Hellenic landings data {#landingsdata}

We will use the annual landings data from Hellenic (Greek) waters (Figure \@ref(fig:greece)) that were used in Stergiou and Christou (1996).  Stergiou and Christou analyzed 16 species.  We will look at just two of the species: Anchovy and Sardine.  Stergiou and Christou used the data from 1964-1989.  We have the data up to 2007, but will focus mainly on 1964-1989 (the first half of the time series) to replicate Stergiou and Christou's analyses.


```{r greece, fig.cap="Location of the fishery.", echo=FALSE}
knitr::include_graphics("./figs/Greece.png")
```

The data are available in tables in yearly fishery survey reports published by the [Hellenic Statisical Authority](http://www.statistics.gr/greece-in-figures).

![](./figs/StatisticalReportCover.png)

The main landings data is in Table IV in these reports.

![](./figs/StatisticalReportTableIV.png)

## The landings data and covariates

The **FishForecast** package has the following data objects:

* **greeklandings** The 1964 to 2007 total landings data multiple species.  It is stored as a data frame, not ts object, with a year column, a species column and columns for landings in metric tons and log metric tons.
* **anchovy** and **sardine** A data frame for the landings (in log metric tons) of these species.  These are the example catch time series used in the chapters.  The data are 1964-2007, however Stergiou and Christou used 1964-1989 and the time series are subsetted to this time period for the examples.  These data frames have only year and log.metric.tons columns.
* **anchovyts** and **sardinets** A ts object for the yearly landings (in log metric tons) of these species.
* **anchovy87** and **sardine87** A subsetted data frame with Year <= 1987.  This is the training data used in Stergiou and Christou.
* **anchovy87ts** and **sardine87ts** A ts object for the yearly landings (in log metric tons) of these species for 1964-1987.
* **ecovsmean.mon** and **ecovsmean.year** The environmental covariates air temperature, pressure, sea surface temperature, vertical wind, and wind speed cubed average monthly and yearly over three 1 degree boxes in the study area.  See the chapter on covariates for details.
* **greekfish.cov** The fisheries covariates on number of boats, horsepower, and fishers.

Load the data by loading the **FishForecast** package and use only the 1964-1989 landings.  We use `subset()` to subset the landings data frame.  Not `window()` as that is a function for subsetting ts objects.

```{r f11-load_data, fig.align = "center", fig.height = 4, fig.width = 8}
require(FishForecast)
landings89 = subset(greeklandings, Year <= 1989)
ggplot(landings89, aes(x=Year, y=log.metric.tons)) +
  geom_line() + facet_wrap(~Species)
```

## ts objects

A ts object in R is a time series, univariate or multivariate, that has information on the major time step value (e.g. year) and the period of the minor time step, if any.  For example, if your data are monthly then the major time step is year, the minor time step is month and the period is 12 (12 months a year).  If you have daily data collected hourly then you major time step is day, minor time step is hour and period is 24 (24 hours per day).  If you have yearly data collected yearly, your major time step is year, your minor time step is also year, and the period is 1 (1 year per year).  You cannot have multiple minor time steps, for example monthly data collected hourly with daily and hourly periods specified. 

The data in a ts object cannot have any missing time steps.  For example, if your data were in a data frame with a column for year, you could have a missing year, say no row for year 1988, and the data sense would still 'make sense'.  The data in a ts object cannot have any missing 'rows'.  If there is no data for a particular year or year/month (if your data are monthly), then that data point must be entered as a NA.  You do not need a time step (e.g. year/month) column(s) for a ts object.  You only need the starting major time step and the starting minor time step (if not 1) and the period.  All the time values from each data point can be computed from those 2 pieces of information if there are no gaps in your time series.  Missing data are fine; they just have to be entered with a NA.

All the non-seasonal examples shown will work on a plain vector of numbers, and it it is not necessary to convert a non-seasonal time series into a ts object.  That said, if you do not convert to a ts object, you will miss out on all the plotting and subsetting functions that are written for ts objects.  Also when you do multivariate regression with covariates, having your data and covariates stored as a ts object will make regressing against lagged covariates (covariate values in the past) easier.


### `ts()` function

To convert a vector of numbers to a ts object, we use the `ts()` function.

```
ts(data = NA, start = 1, end = numeric(), frequency = 1)
```

`start` is a two number vector with the first major time step and the first minor time step.  If you only pass in one number, then it will use 1 (first minor time step) as the 2nd number in `start`.  `end` is specified in exactly the same way and you only need to specified `start` or `end`, not both.   `frequency` is the number of minor time steps per major time step.  If you do not pass this in, it will assume that `frequency=1`, i.e. no periods or season in your data.  

If you specify `frequency=4`, it will assume that the period is quarterly.  If you specify that `frequency=12`, it will assume that period is monthly.  This just affects the labeling of the minor time step columns and will print your data with 4 or 12 columns.  For other frequencies, the data will not be printed with columns for the minor time steps, but the information is there and plotting will use the major steps.

#### Examples {-}

Quarterly data

```{r ts.example1}
aa <- ts(1:24, start=c(1960,1), frequency=4)
aa
plot(aa, type="p")
```

Monthly data

```{r ts.example2}
aa <- ts(1:24, start=c(1960,1), frequency=12)
aa
plot(aa, type="p")
```

Biennial data

```{r ts.example3}
aa <- ts(1:24, start=c(1960,1), frequency=2)
aa
plot(aa, type="p")
```


### ggplot and ts objects

In some ways, plotting ts object is easy.  Just use `plot()` or `autoplot()` and it takes care of the time axis.  In other ways, it can be frustrating if you want to alter the defaults.

#### `autoplot()` {-}

`autoplot()` is a ggplot of the ts object.

```{r autoplot.ts}
aa <- ts(1:24, start=c(1960,1), frequency=12)
autoplot(aa) 
```
and you have access to the usual gglot functions.

```{r autoplot.ts2}
autoplot(aa) + 
  geom_point() +
  ylab("landings") + xlab("") +
  ggtitle("Anchovy landings")
```
Adding minor tick marks in ggplot is tedious (google if you want that) but adding vertical lines at your minor ticks is easy.

```{r autoplot.ts3}
aa <- ts(1:24, start=c(1960,1), frequency=12)
vline_breaks <-  seq(1960, 1962, by=1/12)
autoplot(aa) + 
  geom_vline(xintercept = vline_breaks, color ="blue") +
  geom_point() 
```


### Plotting using a data frame

Often it is easier to work with a data frame (or a tibble) with columns for your major and minor time steps. That way you are not locked into whatever choices the plotting and printing functions use for ts objects.  Many plotting functions work nicely with this type of data frame and you have full control over plotting and summarizing your data.  

To plot the x-axis, we need to add a date column in date format.  Knowing the right format to use for `as.Date()` will take some sleuthing on the internet.  The default is `1960-12-31` so if you get stuff you can always write your date in that format and use the default.   Here I use `1960Jan01` and specify the format for that.  I have used the `date_format()` function in the scales package to help format the dates on the x-axis.

```{r plot.df1}
aa <- data.frame(
  year=rep(1960:1961,each=12), 
  month = rep(month.abb,2),
  val=1:24)
aa$date <- as.Date(paste0(aa$year,aa$month,"01"),"%Y%b%d")
ggplot(aa, aes(x=date, y=val)) + geom_point() +
  scale_x_date(labels=scales::date_format("%b-%Y")) +
  ylab("landings") + xlab("")
```

<!--chapter:end:Forecasting-1-1-Introduction.Rmd-->

## Packages

We will be mainly be using the forecast [@R-forecast] and tseries [@R-tseries] packages, with the MARSS [@R-MARSS] package to implement ARMAX models.  However we will also use a variety of other packages especially for the multivariate regression chapter.  So that you can keep track of what package a function come from, I will use the `::` notation for functions that are *not* from the following standard packages:

* base R
* stats
* ggplot2

Thus to call function fun1 from package pack1, I will use `pack1::fun1()`.  This will make the code more verbose but you will be able to keep track of which function comes from what package.

To install the data used in this book along with all the needed packages, install the **FishForecast** package from GitHub. If you are on a Windows machine, you will need to install [Rtools](https://cran.rstudio.com/bin/windows/Rtools/) in order to install packages from GitHub.

To install a package from GitHub, install the **devtools** package and then run

```
library(devtools)
devtools::install_github("Fish-Forecast/FishForecast")
```

Calling 
```
library(FishForecast)
```
will then make the data objects available.


#### tidyverse and piping {-}

I will minimize the use of tidyverse and piping.  Although the latter can create much more concise code, for beginner R users and programmers, I think it will interfere with learning.  I may add the piped versions of the code later.  I am not going to be doing much 'data-wrangling'.  I will assume that your data are in the tidyverse format, though I will not be using tibbles explicitly.  Our data are quite simple, so this is not hard.  See the chapter on inputting your data. 

#### plotting packages {-}

I will use a combination of base plotting and ggplot2 [@R-ggplot2] plotting.  Doing a tutorial on basic plotting with ggplot2 may be helpful for the material.

## References

We will be using classic methods for catch forecasting discussed in the following reference papers:

* We are replicating the work in [@StergiouChristou1996]  ([Link](https://doi.org/10.1016/0165-7836(95)00389-4)). 
* These methods are also discussed in [@Lawer2016] ([PDF](http://dx.doi.org/10.4236/nr.2016.74018)).
* And in [@Georgakarakosetal2006] ([Link](https://doi.org/10.1016/j.fishres.2005.12.003)).  A PDF reprint is available on the [Hellenic Center for Marine Research website](http://arch.her.hcmr.gr/publ.htm).
* The chapter on modeling seasonal catch data will use models discussed in [@Stergiouetal1997] ([Link](https://doi.org/10.1016/S0165-7836(96)00482-1)). See their [ResearchGate page](https://www.researchgate.net/publication/222888388_Modelling_and_forecasting_monthly_fisheries_catches_Comparison_of_regression_univariate_and_multivariate_time_series_methods) for a PDF reprint.



```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'forecast', 'tseries', 'MARSS', 'urca', 'corrplot', 'olsrr', 'caret', 'car'), 'packages.bib')
```

<!--chapter:end:Forecasting-1-2-Packages.Rmd-->

# Time-varying regression

Time-varying regression is simply a linear regression where time is the explanatory variable:

$$log(catch) = \alpha + \beta t + \beta_2 t^2 + \dots + e_t$$
The error term ( $e_t$ ) was treated as an independent Normal error ( $\sim N(0, \sigma)$ ) in Stergiou and Christou (1996).  If that is not a reasonable assumption, then it is simple to fit a non-Gausian error model in R.

#### Order of the time polynomial {-}

The order of the polynomial of $t$ determines how the time axis (x-axis) relates to the overall trend in the data.  A 1st order polynomial ($\beta t$) will allow a linear relationship only.  A 2nd order polynomial($\beta_1 t + \beta_2 t^2$) will allow a convex or concave relationship with one peak.  3rd and 4th orders will allow more flexible relationships with more peaks.

```{r poly.plot, echo=FALSE,fig.height=4,fig.width=8,fig.align="center"}
par(mfrow=c(1,4))
tt=seq(-5,5,.01)
plot(tt,type="l",ylab="",xlab="")
title("1st order")
plot(tt^2,type="l",ylab="",xlab="")
title("2nd order")
plot(tt^3-3*tt^2-tt+3,ylim=c(-100,50),type="l",ylab="",xlab="")
title("3rd order")
plot(tt^4+2*tt^3-12*tt^2-2*tt+6,ylim=c(-100,100),type="l",ylab="",xlab="")
title("4th order")
```



<!--chapter:end:Forecasting-2-1-TV-Regression.Rmd-->

## Fitting

Fitting a time-varying regression is done with the `lm()` function.  For example, here is how to fit a 4th-order polynomial for time to the anchovy data.  We are fitting this model:

$$log(Anchovy) = \alpha + \beta t + \beta_2 t^2 + \beta_3 t^3 + \beta_4 t^4 + e_t$$

First load in the data by loading the **FishForecast** package.  `anchovy` is a data frame with year and log.metric.tons columns. `anchovy87` is the same data frame but with the years 1964 to 1987. These are the years that Stergio and Christou use for fitting their models.  They hold out 1988 and 1989 for forecast evaluation.

```{r f21-load_data}
require(FishForecast)
```

We need to add on a column for $t$ (and $t^2$, $t^3$, $t^4$) where the first year is $t=1$.  We could regress against year (so 1964 to 1987), but by convention, one regresses against 1 to the number of years or 0 to the number of years minus 1.  Stergiou and Christou did the former.

```{r tvreg.anchovy}
anchovy87$t = anchovy87$Year-1963
anchovy87$t2 = anchovy87$t^2
anchovy87$t3 = anchovy87$t^3
anchovy87$t4 = anchovy87$t^4
model <- lm(log.metric.tons ~ t + t2 + t3 + t4, data=anchovy87)
```

All our covariates are functions of $t$, so we do not actually need to add on the $t^2$, $t^3$ and $t^4$ to our data frame.  We can use the `I()` function.  This function is useful whenever you want to use a transformed value of a column of your data frame in your regression.

```{r tvreg.anchovy.I}
anchovy87$t = anchovy87$Year-1963
model <- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=anchovy87)
```

Let's look at the fit.

```{r}
summary(model)
```


### Orthogonal polynomials

None of the time effects are significant despite an obvious linear temporal trend to the data.  What's going on?  Well $t$, $t^2$, $t^3$ and $t^4$ are all highly correlated.  Fitting a linear regression with multiple highly correlated covariates will not get you anywhere unless perhaps all the covariates are needed to explain the data.  We will see the latter case for the sardine. In the anchovy case, multiple of the covariates could explain the linear-ish trend.

You could try fitting the first degree model $x_t = \alpha + \beta t + e_t$, then the second $x_t = \alpha + \beta_1 t + \beta_2 t^2 + e_t$, then the third.  This would reveal that in the first and second order fits, we get significant effects of time in our model.  However the correct way to do this would be to use orthogonal polynomials.

#### `poly()` function {-}

The `poly()` function creates orthogonal covariates for your polynomial.  What does that mean? Let's say you want to fit a model with a 2nd order polynomial of $t$.  It has $t$ and $t^2$, but using these as covariates directly lead to using two covariates that are highly correlated.  Instead we want a covariate that explains $t$ and another that explains the part of $t^2$ that cannot be explained by $t$.  `poly()` creates these orthogonal covariates.  The `poly()` function creates covariates with mean zero and identical variances.  Covariates with different means and variances makes it hard to compare the estimated effect sizes.  

```{r poly}
T1 = 1:24; T2=T1^2
c(mean(T1),mean(T2),cov(T1, T2))
T1 = poly(T1,2)[,1]; T2=poly(T1,2)[,2]
c(mean(T1),mean(T2),cov(T1, T2))
```

#### Using `poly()` to fit the anchovy data {-}

We saw in the anchovy fit that using $t$, $t^2$, $t^3$ and $t^4$ directly in the fit resulted in no significant estimated time effect despite a clear temporal trend in the data.  If we fit with `poly()` so that we do not use correlated time covariates, we see a different picture.

```{r tvreg.poly.anchovy}
model <- lm(log.metric.tons ~ poly(t,4), data=anchovy87)
summary(model)
```

### Residual diagnostics

We want to test if our residuals are temporally independent.  We can do this with the Ljung-Box test as Stergio and Christou do.  For the Ljung-Box test

* Null hypothesis is that the data are independent
* Alternate hypothesis is that the data are serially correlated

#### Example of the Ljung-Box test {-}

```{r example_LB_test}
Box.test(rnorm(100), type="Ljung-Box")
```

The null hypothesis is not rejected.  These are not serially correlated.


Stergio and Christou appear to use a lag of 14 for the test (this is a bit large for 24 data points).  The degrees of freedom is lag minus the number of estimated parameters in the model.  So for the Anchovy data, $df = 14 - 2$.

```{r}
x <- resid(model)
Box.test(x, lag = 14, type = "Ljung-Box", fitdf=2)
```

Compare to the values in the far right column in Table 4.  The null hypothesis of independence is rejected.

#### Breusch-Godfrey test {-}

Although Stergiou and Christou use the Ljung-Box test, the Breusch-Godfrey test is more standard for regression residuals.  The forecast package has the `checkresiduals()` function which will run this test and some diagnostic plots.

```{r}
forecast::checkresiduals(model)
```

### Compare to Stergiou and Christou

Stergiou and Christou (1996) fit time-varying regressions to the 1964-1987 data and show the results in Table 4.

![Table 4](./figs/SC1995Table4.png)

#### Compare anchovy fit to Stergiou and Christou {-}

Stergiou and Christou use a first order polynomial, linear relationship with time, for the anchovy data.  They do not state how they choose this over a 2nd order polynomial which also appears supported (see fit with `poly()` fit to the anchovy data).

```{r tvreg.anchovy2}
anchovy87$t = anchovy87$Year-1963
model <- lm(log.metric.tons ~ t, data=anchovy87)
```

The coefficients and adjusted R2 are similar to that shown in their Table 4.  The coefficients are not identical so there may be some differences in the data I extracted from the Greek statistical reports and those used in Stergiou and Christou.

```{r}
c(coef(model), summary(model)$adj.r.squared)
```

#### Compare sardine fit to Stergiou and Christou {-}

For the sardine (bottom row in Table 4), Stergio and Christou fit a 4th order polynomial.  With `poly()`, a 4th order time-varying regression model is fit to the sardine data as:

```{r tvreg.sardine}
sardine87$t = sardine87$Year-1963
model <- lm(log.metric.tons ~ poly(t,4), data=sardine87)
```

This indicates support for the 2nd, 3rd, and 4th orders but not the 1st (linear) part.


```{r poly.summary}
summary(model)
```

Stergiou and Christou appear to have used a raw polynomial model using $t$, $t^2$, $t^3$ and $t^4$ as the covariates instead of orthogonal polynomials.  To fit the model that they did, we use

```{r tvreg.sardine2}
model <- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=sardine87)
```

Using a model fit with the raw time covariates, the coefficients and adjusted R2 are similar to that shown in Table 4.

```{r}
c(coef(model), summary(model)$adj.r.squared)
```

The test for autocorrelation of the residuals is 

```{r}
x <- resid(model)
Box.test(x, lag = 14, type = "Ljung-Box", fitdf=5)
```

`fitdf` specifies the number of parameters estimated by the model.  In this case it is 5, intercept and 4 coefficients.

The p-value is less than 0.05 indicating that the residuals are temporally correlated.

### Summary

#### Why use time-varying regression? {-}

* It looks there is a simple time relationship.  If a high-order polynomial is required, that is a bad sign.

* Easy and fast

* Easy to explain

* You are only forecasting a few years ahead

* No assumptions required about 'stationarity'

#### Why not to use time-varying regression? {-}

* Autocorrelation is not modeled.  That autocorrelation may hold information for forecasting.

* You are only using temporal trend for forecasting (mean level).

* If you use a high-order polynomial, you might be modeling noise from a random walk.  That means interpreting the temporal pattern as having information when in fact it has none.

#### Is time-varying regression used? {-}

It seems pretty simple.  Is this used?  All the time.  Most "trend" analyses are a variant of time-varying regression.  If you fit a line to your data and report the trend or percent change, that's a time-varying regression.

<!--chapter:end:Forecasting-2-2-TV-Regression.Rmd-->

## Forecasting

Forecasting is easy in R once you have a fitted model.  Let's say for the anchovy, we fit the model

$$C_t = \alpha + \beta t + e_t$$
where $t$ starts at 1 (so 1964 is $t=1$ ).  To predict, predict the catch in year t, we use

$$C_t = \alpha + \beta t + e_t$$


Model fit:

```{r f22-tvreg.anchovy}
require(FishForecast)
anchovy87$t <- anchovy87$Year-1963
model <- lm(log.metric.tons ~ t, data=anchovy87)
coef(model)
```

For anchovy, the estimated $\alpha$ (Intercept) is `r coef(model)[1]` and $\beta$ is `r coef(model)[2]`.  We want to use these estimates to forecast 1988 ( $t=25$ ).

So the 1988 forecast is `r coef(model)[1]` + `r coef(model)[2]` $\times$ 25 :

```{r tvreg.forecast1}
coef(model)[1]+coef(model)[2]*25
```

log metric tons.


### The forecast package

The forecast package in R makes it easy to create forecasts with fitted models and to plot (some of) those forecasts.

For a TV Regression model, our `forecast()` call looks like

```{r TVregression.forecast2}
fr <- forecast::forecast(model, newdata = data.frame(t=25:29))
```

The dark grey bands are the 80% prediction intervals and the light grey are the 95% prediction intervals.

```{r plot.TVreg.forecast, fig.align = "center"}
plot(fr)
```


Anchovy forecasts from a higher order polynomial can similarly be made.  Let's fit a 4-th order polynomial.


$$C_t = \alpha + \beta_1 t + \beta_2 t^2 + \beta_3 t^3 + \beta_4 t^4 + e_t$$

To forecast with this model, we fit the model to estimate the $\beta$'s and then replace $t$ with $24$:

$$C_{1988} = \alpha + \beta_1 24 + \beta_2 24^2 + \beta_3 24^3 + \beta_4 24^4 + e_t$$

This is how to do that in R:

```{r f22-tvreg.sardine}
model <- lm(log.metric.tons ~ t + I(t^2) + I(t^3) + I(t^4), data=anchovy87)
fr <- forecast::forecast(model, newdata = data.frame(t=24:28))
fr
```


Unfortunately, forecast's `plot()` function for forecast objects does not recognize that there is only one predictor $t$ thus we cannot use forecast's plot function.

If you do this in R, it throws an error.
```{r plot.TVreg.forecast.bad}
try(plot(fr))
```

```
Error in plotlmforecast(x, PI = PI, shaded = shaded, shadecols = shadecols, : Forecast plot for regression models only available for a single predictor
```


```{r plot.TVreg.func, echo=FALSE}
plotforecasttv <- function(object, h=10, ylims=NULL){
  dat <- object$model
  dat$fitted <- object$fitted.values
  tlim <- (max(object$model$t)+1:h)
  pr95 <- predict(model, newdata = data.frame(t=(max(object$model$t)+1:h)), level=0.95, interval="prediction")
  pr80 <- predict(model, newdata = data.frame(t=(max(object$model$t)+1:h)), level=0.80, interval="prediction")
  pr95 <- as.data.frame(pr95); pr95$t <- tlim
  pr80 <- as.data.frame(pr80); pr80$t <- tlim
  if(is.null(ylims)) ylims <- c(min(dat[,1],pr95$lwr, pr80$lwr), max(dat[,1],pr95$upr, pr80$upr))
  p1 <- ggplot(dat, aes_string(x = colnames(dat)[2], y = colnames(dat)[1])) +
  theme_bw() +
  geom_point(color = "blue") + xlim(0,max(tlim)) + ylim(ylims) +
  geom_line(aes(x=t, y=fitted), color="red")
  
  p1 + 
    geom_ribbon(mapping=aes(x=t, ymin=lwr, ymax=upr), data=pr95, inherit.aes=FALSE, fill = "grey50") +
    geom_ribbon(mapping=aes(x=t, ymin=lwr, ymax=upr), data=pr80, inherit.aes=FALSE, fill = "grey75") +
    geom_line(aes(x=t, y=fit), pr95)
}
```

I created a function that you can use to plot time-varying regressions with polynomial $t$.  You will use this function in the lab.

```{r plot.TVreg.forecast2, fig.align = "center"}
plotforecasttv(model, ylims=c(8,17))
```


A feature of a time-varying regression with many polynomials is that it fits the data well, but the forecast quickly becomes uncertain due to uncertainty regarding the polynomial fit.  A simpler model can give forecasts that do not become rapidly uncertain.

The flip-side is that the simpler model may not capture the short-term trends very well and may suffer from autocorrelated residuals.



```{r tvreg.lm1}
model <- lm(log.metric.tons ~ t + I(t^2), data=anchovy87)
```


```{r plot.TVreg.lm1, fig.align = "center"}
plotforecasttv(model, ylims=c(8,17))
```


## Summary

* Time-varying regression is a simple approach to forecasting that allows a non-linear trend.
* The uncertainty in your forecast is determined by how much error there is between the fit an the data.
* Fit must be balanced against prediction uncertainty.
* R allows you to quickly fit models and compute the prediction intervals.

Careful thought must be given to selecting the polynomial order.

* Standard methods are available in R for order selection
* Using different orders for different data sets has prediction consequences



<!--chapter:end:Forecasting-2-3-TV-Regression.Rmd-->

# ARIMA Models

The basic idea in an ARMA model is that past values in the time series have information about the current state.  An AR model, the first part of ARMA, models the current state as a linear function of past values:

$$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + ... + \phi_p x_{t-p} + e_t$$


![](./figs/SpeciesPlot.jpeg)

## Overview

### Components of an ARIMA model

You will commonly see ARIMA models referred to as *Box-Jenkins* models.  This model has 3 components (p, d, q):

- **AR autoregressive**  $y_t$ depends on past values. The AR level is maximum lag $p$.

$$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + ... + \phi_p x_{t-p} + e_t$$

- **I differencing** $x_t$ may be a difference of the observed time series.  The number of differences is denoted $d$. First difference is $d=1$:

$$x_t = y_t - y_{t-1}$$

- **MA moving average**  The error $e_t$ can be a sum of a time series of independent random errors.  The maximum lag is denoted $q$.

$$e_t = \eta_t + \theta_1 \eta_{t-1} + \theta_2 \eta_{t-2} + ... + \theta_q \eta_{t-q},\quad \eta_t \sim N(0, \sigma)$$

#### Create some data from an AR(2) Model {-}

$$x_t = 0.5 x_{t-1} + 0.3 x_{t-2} + e_t$$

```{r arima.sim, fig.align="center"}
dat = arima.sim(n=1000, model=list(ar=c(.5,.3)))
plot(dat)
abline(h=0, col="red")
```

Compare AR(2) and random data.

```{r arimavsrn, fig.align="center", echo=FALSE}
par(mfrow=c(1,2))
plot(dat[1:500],type="l",ylab="dat")
abline(h=0, col="red")
title("ar(2)")
plot(rnorm(length(dat))[1:500],type="l",ylab="dat",xlab="Time")
abline(h=0, col="red")
title("random normal")
```


#### AR(2) is auto-correlated {-}

Plot the data at time $t$ against the data at time $t-1$

```{r arimavsrncorr, fig.align="center", echo=FALSE}
par(mfrow=c(1,2))
TT=length(dat)
plot(dat[2:TT],dat[1:(TT-1)],type="p")
title("ar(2)")
rn=rnorm(length(dat))
plot(rn[2:TT],rn[1:(TT-1)],type="p")
title("random normal")
```


### Box-Jenkins method

This refers to a step-by-step process of selecting a forecasting model.  You need to go through the steps otherwise you could end up fitting a nonsensical model or using fitting a sensible model with an algorithm that will not work on your data.

A. Model form selection

  1. Evaluate stationarity and seasonality
  2. Selection of the differencing level (d)
  3. Selection of the AR level (p)
  4. Selection of the MA level (q)

B. Parameter estimation

C. Model checking


### ACF and PACF functions

#### The ACF function {-}

The auto-correlation function (ACF) is the correlation between the data at time $t$ and $t+1$.  This is one of the basic diagnostic plots for time series data.

```{r acf, fig.align="center"}
acf(dat[1:50])
```

The ACF simply shows the correlation between all the data points that are lag $p$ apart.  Here are the correlations for points lag 1 and lag 10 apart.  `cor()` is the correlation function.

```{r corr}
cor(dat[2:TT], dat[1:(TT-1)])
cor(dat[11:TT], dat[1:(TT-10)])
```

The values match what we see in the ACF plot.

```{r acf2, fig.align="center", echo=FALSE}
par(mfrow=c(1,2))
plot(dat[1:50], type="b", pch="x")
title("dat[1:50]")
acf(dat,lag.max=25)
```

#### ACF for independent data {-}

Temporally independent data shows no significant autocorrelation.

```{r acf.random, fig.align="center",echo=FALSE}
rn <- rnorm(TT)
par(mfrow=c(1,2))
plot(rn[1:50], type="b", pch="x")
title("random 1:50")
acf(rn,lag.max=25)
```

#### PACF function {-}

In the ACF for the AR(2), we see that $x_t$ and $x_{t-3}$ are correlated even those the model for $x_t$ does not include $x_{t-3}$.  $x_{t-3}$ is correlated with $x_t$ indirectly because $x_{t-3}$ is directly correlated with $x_{t-2}$ and $x_{t-1}$ and these two are in turn directly correlated with $x_t$.  The partial autocorrelation function removes this indirect correlation.  Thus the only significant lags in the PACF should be the lags that appear in the process model.

For example, if the model is

#### Partial ACF for AR(2) {-}

$$x_t = 0.5 x_{t-1} + 0.3 x_{t-2} + e_t$$
then only the first two lags should be significant in the PACF.

```{r pacf.ar2, fig.align="center"}
pacf(dat)
```


#### Partial ACF for AR(1) {-}

Similarly if the process model is

$$x_t = 0.5 x_{t-1} + e_t$$

The PACF should only have significant values at lag 1.

```{r pacf.ar3, fig.align="center"}
dat <- arima.sim(TT, model=list(ar=c(.5)))
pacf(dat)
```


<!--chapter:end:Forecasting-3-1-ARMA-Intro.Rmd-->

## Stationarity

```{r f32-load_data, message=FALSE, warning=FALSE, echo=FALSE}
require(FishForecast)
```
The first two steps of the Box-Jenkins Method have to do with evaluating for stationarity and correcting for lack of stationarity in your data:

A. Model form selection

  1. **Evaluate stationarity and seasonality**
  2. **Selection of the differencing level (d)**
  3. Selection of the AR level (p)
  4. Selection of the MA level (q)

B. Parameter estimation

C. Model checking

### Definition

Stationarity means 'not changing in time' in the context of time-series models.  Typically we test the trend and variance, however more generally all statistical properties of a time-series is time-constant if the time series is 'stationary'.

Many ARMA models exhibit stationarity.  White noise is one type:
$$x_t = e_t, e_t \sim N(0,\sigma)$$

```{r fig.stationarity, fig.height = 4, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
require(gridExtra)
require(reshape2)

TT=100
y = rnorm(TT)
dat = data.frame(t=1:TT, y=y)
p1 = ggplot(dat, aes(x=t, y=y)) + geom_line() + 
  ggtitle("White Noise") + xlab("") + ylab("value")
ys = matrix(rnorm(TT*10),TT,10)
ys = data.frame(ys)
ys$id = 1:TT

ys2=reshape2::melt(ys, id.var="id")
p2 = ggplot(ys2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("The variance of a white noise process is steady")
gridExtra::grid.arrange(p1, p2, ncol = 1)
```


An AR-1 process with $b<1$
$$x_t = b x_{t-1} + e_t$$
is also stationary.

```{r fig.stationarity2, fig.height = 4, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
theta=0.8
nsim=10
ar1=as.vector(arima.sim(TT, model=list(ar=theta)))
dat = data.frame(t=1:TT, y=ar1)
p1 = ggplot(dat, aes(x=t, y=y)) + geom_line() + 
  ggtitle("AR-1") + xlab("") + ylab("value")
ys = matrix(0,TT,nsim)
for(i in 1:nsim) ys[,i]=as.vector(arima.sim(TT, model=list(ar=theta)))
ys = data.frame(ys)
ys$id = 1:TT

ys2=reshape2::melt(ys, id.var="id")
p2 = ggplot(ys2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("The variance of an AR-1 process is steady")
gridExtra::grid.arrange(p1, p2, ncol = 1)
```

#### Stationarity around a trend {-}

The processes shown above have mean 0 and a flat level.  We can also have stationarity around an non-zero level or stationarity around an linear trend. If $b=0$, we have white noise and if $b<1$ we have AR-1.

1. Non-zero mean: $x_t = \mu + b x_{t-1} + e_t$

2. Linear trend: $x_t = \mu + at + b x_{t-1} + e_t$

```{r fig.stationarity3, fig.height = 4, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
require(gridExtra)
intercept = .5
trend=.1
dat = data.frame(t=1:TT, wn=rnorm(TT))
dat$wni = dat$wn+intercept
dat$wnti = dat$wn + trend*(1:TT) + intercept
p1 = ggplot(dat, aes(x=t, y=wn)) + geom_line() + ggtitle("White noise")
p2 = ggplot(dat, aes(x=t, y=wni)) + geom_line() + ggtitle("with non-zero mean")
p3 = ggplot(dat, aes(x=t, y=wnti)) + geom_line() + ggtitle("with linear trend")

ar1 = rep(0,TT)
err = rnorm(TT)
for(i in 2:TT) ar1[i]=theta*ar1[i-1]+err[i]
dat = data.frame(t=1:TT, ar1=ar1)
dat$ar1i = dat$ar1+intercept
dat$ar1ti = dat$ar1 + trend*(1:TT) + intercept
p4 = ggplot(dat, aes(x=t, y=ar1)) + geom_line() + ggtitle("AR1")
p5 = ggplot(dat, aes(x=t, y=ar1i)) + geom_line() + ggtitle("with non-zero mean")
p6 = ggplot(dat, aes(x=t, y=ar1ti)) + geom_line() + ggtitle("with linear trend")

grid.arrange(p1, p4, p2, p5, p3, p6, ncol = 2)
```


### Non-stationarity

One of the most common forms of non-stationarity that is tested for is 'unit root', which means that the process is a random walk:
$$x_t = x_{t-1} + e_t$$ .

```{r fig.nonstationarity, fig.height = 4, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
require(reshape2)

rw = rep(0,TT)
for(i in 2:TT) rw[i]=rw[i-1]+err[i]
dat = data.frame(t=1:TT, rw=rw)
p1 = ggplot(dat, aes(x=t, y=rw)) + geom_line() + 
  ggtitle("Random Walk") + xlab("") + ylab("value")
rws = apply(matrix(rnorm(TT*nsim),TT,nsim),2,cumsum)
rws = data.frame(rws)
rws$id = 1:TT

rws2=melt(rws, id.var="id")
p2 = ggplot(rws2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("The variance of a random walk process grows in time")
grid.arrange(p1, p2, ncol = 1)
```

#### Non-stationarity with a trend {-}

Similar to the way we added an intecept and linear trend to the stationarity processes, we can do the same to the random walk.

1. Non-zero mean or intercept: $x_t = \mu + x_{t-1} + e_t$

2. Linear trend: $x_t = \mu + at + x_{t-1} + e_t$

The effects are fundamentally different however.  The addition of $\mu$ leads to a upward mean linear trend while the addition of $at$ leads to exponential growth.

```{r fig.stationarity4, fig.height = 4, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
require(gridExtra)
dat = data.frame(t=1:TT, y=cumsum(rnorm(TT)))
dat$yi = cumsum(rnorm(TT,intercept,1))
dat$yti = cumsum(rnorm(TT,intercept+trend*1:TT,1))
p1 = ggplot(dat, aes(x=t, y=y)) + geom_line() + ggtitle("Random Walk")
p2 = ggplot(dat, aes(x=t, y=yi)) + geom_line() + ggtitle("with non-zero mean added")
p3 = ggplot(dat, aes(x=t, y=yti)) + geom_line() + ggtitle("with linear trend added")

gridExtra::grid.arrange(p1, p2, p3, ncol = 1)
```


### Stationarity tests

Why is evaluating stationarity important? 

- Many AR models have a flat level or trend and time-constant variance.  If your data do not have those properties, you are fitting a model that is fundamentally inconsistent with your data.
- Many standard algorithms for fitting ARIMA models assume stationarity.  Note, you can fit ARIMA models without making this assumption, but you need to use the appropriate algorithm.

We will discuss three common approaches to evaluating stationarity:

- Visual test
- (Augmented) Dickey-Fuller test
- KPSS test


#### Visual test {-}

The visual test is simply looking at a plot of the data versus time.  Look for

- Change in the level over time.  Is the time series increasing or decreasing?  Does it appear to cycle?
- Change in the variance over time.  Do deviations away from the mean change over time, increase or decrease?


Here is a plot of the anchovy and sardine in Greek waters from 1965 to 1989.  The anchovies have an obvious non-stationary trend during this period.  The mean level is going up.  The sardines have a roughly stationary trend.  The variance (deviations away from the mean) appear to be roughly stationary, neither increasing or decreasing in time.

```{r fig.vis, fig.height = 4, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
dat = subset(greeklandings, Species %in% c("Anchovy", "Sardine") & 
               Year <= 1989)
dat$log.metric.tons = log(dat$metric.tons)
ggplot(dat, aes(x=Year, y=log.metric.tons)) +
  geom_line()+facet_wrap(~Species)
```

Although the logged anchovy time series is increasing, it appears to have an linear trend.


#### Dickey-Fuller test {-}

The Dickey=Fuller test (and Augmented Dickey-Fuller test) look for evidence that the time series has a unit root.  

The null hypothesis is that the time series has a unit root, that is, it has a random walk component.  

The alternative hypothesis is some variation of stationarity.  The test has three main verisons. 


Visually, the null and alternative hypotheses for the three Dickey-Fuller tests are the following.  It is hard to see but in the panels on the left, the variance around the trend is increasing and on the right, it is not.

```{r fig.df, fig.height = 6, fig.width = 8, fig.align = "center", echo=FALSE}
require(ggplot2)
require(gridExtra)
#####
ys = matrix(0,TT,nsim)
for(i in 2:TT) ys[i,]=ys[i-1,]+rnorm(nsim)
rws = data.frame(ys)
rws$id = 1:TT
library(reshape2)
rws2=melt(rws, id.var="id")
p1 = ggplot(rws2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("Null Non-stationary", subtitle="White noise")
ys = matrix(0,TT,nsim)
for(i in 2:TT) ys[i,]=theta*ys[i-1,]+rnorm(nsim)
ar1s = data.frame(ys)
ar1s$id = 1:TT
require(reshape2)
ar1s2=reshape2::melt(ar1s, id.var="id")
p2 = ggplot(ar1s2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("Alternate Stationary", subtitle="AR1")
#####
ys = matrix(intercept,TT,nsim)
for(i in 2:TT) ys[i,]=intercept+ys[i-1,]+rnorm(nsim)
rws = data.frame(ys)
rws$id = 1:TT
rws2=reshape2::melt(rws, id.var="id")
p3 = ggplot(rws2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("", subtitle="White noise + drift")
ys = matrix(intercept/(1-theta),TT,nsim)
for(i in 2:TT) ys[i,]=intercept+theta*ys[i-1,]+rnorm(nsim)
ar1s = data.frame(ys)
ar1s$id = 1:TT
ar1s2=reshape2::melt(ar1s, id.var="id")
p4 = ggplot(ar1s2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("Alternate", subtitle="AR1 + non-zero level")
#####
ys = matrix(intercept+trend*1,TT,nsim)
for(i in 2:TT) ys[i,]=intercept+trend*i+ys[i-1,]+rnorm(nsim)
rws = data.frame(ys)
rws$id = 1:TT
rws2=reshape2::melt(rws, id.var="id")
p5 = ggplot(rws2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("", subtitle="White noise + exponential drift")
ys = matrix((intercept+trend*1)/(1-theta),TT,nsim)
for(i in 2:TT) ys[i,]=intercept+trend*i+theta*ys[i-1,]+rnorm(nsim)
ar1s = data.frame(ys)
ar1s$id = 1:TT
ar1s2=reshape2::melt(ar1s, id.var="id")
p6 = ggplot(ar1s2, aes(x=id,y=value,group=variable)) +
  geom_line() + xlab("") + ylab("value") +
  ggtitle("Alternate", subtitle="AR1 + linear trend")
#####
gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
```

Mathematically, here are the null and alternative hypotheses.  In each, we are testing if $\delta=0$.

1. Null is a random walk with no drift
$x_t = x_{t-1}+e_t$

Alternative is a mean-reverting (stationary) process with zero mean.
$x_t = \delta x_{t-1}+e_t$

2. Null is a random walk with drift (linear STOCHASTIC trend)
$x_t = \mu + x_{t-1} + e_t$

Alternative is a mean-reverting (stationary) process with non-zero mean and no trend.
$x_t = \mu + \delta x_{t-1} + e_t$

3. Null is a random walk with exponential trend
$x_t = \mu + at + x_{t-1} + e_t$

Alternative is a mean-reverting (stationary) process with non-zero mean and linear DETERMINISTIC trend.
$x_t = \mu + at + \delta x_{t-1} + e_t$

#### Example: Dickey-Fuller test using adf.test() {-}

`adf.test()` in the tseries package will apply the Augemented Dickey-Fuller and report the p-value.  We want to reject the Dickey=Fuller null hypothesis of non-stationarity.  We will set `k=0` to apply the Dickey-Fuller test which tests for AR(1) stationarity.  The Augmented Dickey-Fuller tests for more general lag-p stationarity.

```
adf.test(x, alternative = c("stationary", "explosive"),
         k = trunc((length(x)-1)^(1/3)))
```

`x` is the time-series data in vector or ts form.  Here is how to apply this test to the anchovy data

```{r adf.test.anchovy1}
tseries::adf.test(anchovy87ts, k=0)
# or tseries::adf.test(anchovy87$log.metric.tons, k=0)
```

The null hypothesis is not rejected.  That is not what we want.

#### Example: Dickey-Fuller test using ur.df() {-}


The `urca` R package can also be used to apply the Dickey-Fuller tests.  Use `lags=0` for Dickey-Fuller which tests for AR(1) stationarity.  We will set `type="trend"` to deal with the trend seen in the anchovy data.  Note, `adf.test()` uses this type by default.

```
ur.df(y, type = c("none", "drift", "trend"), lags = 0)
```

```{r dickey.fuller, message=FALSE, warning=FALSE}
test = urca::ur.df(anchovy87ts, type="trend", lags=0)
test
```

`ur.df()` will report the test statistic.  You can look up the values of the test statistic for different $\alpha$ levels using `summary(test)` or `attr(test, "cval")`.  If the test statistic is less than the critical value for $\alpha$=0.05 ('5pct' in cval), it means the null hypothesis of non-stationarity is rejected.  For the Dickey-Fuller test, you do want to reject the null hypothesis.

The test statistic is 
```{r teststat}
attr(test, "teststat")
```
and the critical value at $\alpha = 0.05$ is 
```{r cval}
attr(test,"cval")
```
The statistic is larger than the critical value and thus the null hypothesis of non-stationarity is not rejected. That's not what we want.


#### Augmented Dickey-Fuller test {-}

The Dickey-Fuller test assumes that the stationary process is AR(1) (autoregressive lag-1).  The Augmented Dickey-Fuller test allows a general stationary process.  The idea of the test however is the same.

We can apply the Augmented Dickey-Fuller test with the `ur.df()` function or the `adf.test()` function in the `tseries` package.

```
adf.test(x, alternative = c("stationary", "explosive"),
         k = trunc((length(x)-1)^(1/3)))
```

The alternative is either stationary like $x_t = \delta x_{t-1} + \eta_t$ with $\delta<1$ or 'explosive' with $\delta>1$.  `k` is the number of lags which determines the number of time lags allowed in the autoregression.  `k` is generally determined by the length of your time series.

#### Example: Augmented Dickey-Fuller tests with adf.test() {-}

With the `tseries` package, we apply the Augmented Dickey-Fuller test with `adf.test()`.  This function uses the test where the alternative model is stationary around a linear trend: $x_t = \mu + at + \delta x_{t-1} + e_t$.

```{r dickey.fuller2, message=FALSE, warning=FALSE}
tseries::adf.test(anchovy87ts)
```

In both cases, we do not reject the null hypothesis that the data have a random walk.  Thus there is not support for these time series being stationary.

#### Example: Augmented Dickey-Fuller tests with ur.df() {-}

With the `urca` package, we apply the Augmented Dickey-Fuller test with `ur.df()`.  The defaults for `ur.df()` are different than for `adf.test()`.  

`ur.df()` allows you to specify which of the 3 alternative hypotheses you want: none (stationary around 0), drift (stationary around a non-zero intercept), trend (stationary around a linear trend).

Another difference is that by default, `ur.df()` uses a fixed lag of 1 while by default `adf.test()` selects the lag based on the length of the time series.


We will specify "trend" to make the test similar to `adf.test()`.  We will set the lags like `adf.test()` does also.

```{r dickey.fuller.ur.df, message=FALSE, warning=FALSE}
k = trunc((length(anchovy87ts)-1)^(1/3))
test = urca::ur.df(anchovy87ts, type="trend", lags=k)
test
```

The test statistic values are the same, but we need to look up the critical values with `summary(test)`.


#### KPSS test {-}

In the Dickey-Fuller test, the null hypothesis is the unit root, i.e. random walk.  Often times, there is not enough power to reject the null hypothesis.  A null hypothesis is accepted unless there is strong evidence against it.  

The Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test has as the null hypothesis that a time series is stationary around a level trend (or a linear trend). The alternative hypothesis for the KPSS test is a random walk.

The stationarity assumption is general; it does not assume a specific type of stationarity such as white noise.

If both KPSS and Dickey-Fuller tests support non-stationarity, then the stationarity assumption is not supported.

#### Example: KPSS tests {-}

```{r kpss.test, message=FALSE, warning=FALSE}
tseries::kpss.test(anchovy87ts, null="Trend")
```

Here `null="Trend"` was included to account for the increasing trend in the data.  The null hypothesis of stationarity is rejected.  Thus both the KPSS and Dickey-Fuller tests support the hypothesis that the anchovy time series is non-stationary.  That's not what we want.


### Differencing the data

Differencing the data is used to correct non-stationarity. Differencing means to create a new time series  $z_t = x_t - x_{t-1}$.  First order differencing means you do this once (so $z_t$) and second order differencing means you do this twice (so $z_t - z_{t-1}$).

The `diff()` function takes the first difference:

```{r diff1}
x <- diff(c(1,2,4,7,11))
x
```

The second difference is the first difference of the first difference.

```{r diff2}
diff(x)
```

Here is a plot of the anchovy data and its first difference.

```{r diff1.plot, fig.align="center", fig.height = 4, fig.width = 8}
par(mfrow=c(1,2))
plot(anchovy87ts, type="l")
title("Anchovy")
plot(diff(anchovy87ts), type="l")
title("Anchovy first difference")
```

Let's test the anchovy data with one difference using the KPSS test.

```{r diff.anchovy.test}
diff.anchovy = diff(anchovy87ts)
tseries::kpss.test(diff.anchovy)
```

The null hypothesis of stationairity is not rejected. That is good.

Let's test the first difference of the anchovy data using the Augmented Dickey-Fuller test.  We do the default test and allow it to chose the number of lags.

```{r test.dickey.fuller.diff1}
tseries::adf.test(diff.anchovy)
```

The null hypothesis of non-stationarity is rejected.  That is  what we want.  However, we differenced which removed the trend thus we are testing against a more general model than we need.  Let's test with an alternative hypothesis that has a non-zero mean but not trend.  We can do this with `ur.df()` and `type='drift'`.

```{r test.dickey.fuller.diff2}
test <- urca::ur.df(diff.anchovy, type="drift", lags=2)
```

The null hypothesis of NON-stationairity IS rejected. That is good.

The test statistic is 
```{r teststat.diff}
attr(test, "teststat")
```

and the critical value at $\alpha = 0.05$ is 
```{r cval.diff}
attr(test,"cval")
```


### Summary

Test stationarity before you fit a ARMA model.

- Visual test: is the time series flutuating about a level or a linear trend?

Yes or maybe?  Apply a "unit root" test.

- (Augmented) Dickey-Fuller test
- KPSS test

No or fails the unit root test.

- Apply differencing again and re-test.

Still not passing?

- Try a second difference.

Still not passing?

- ARMA model might not be the best choice. Or you may need to an adhoc detrend.


<!--chapter:end:Forecasting-3-2-ARMA-Stationarity.Rmd-->

## Model structure

```{r f33-load_data, echo=FALSE, message=FALSE, warning=FALSE}
require(FishForecast)
```

We are now at step A3 and A4 of the Box-Jenkins Method.  Note we did not address seasonality since we are working with yearly data.

A. Model form selection

  1. Evaluate stationarity and seasonality
  2. Selection of the differencing level (d)
  3. **Selection of the AR level (p)**
  4. **Selection of the MA level (q)**
    
B. Parameter estimation

C. Model checking

*Much of this will be automated when we use the forecast package*


### AR and MA lags

Step A3 is to determine the number of $p$ lags in the AR part of the model:

$$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + ... + \phi_p x_{t-p} + e_t$$

Step A4  is to determine the number of $q$ lags in the MA part of the model: 

$$e_t = \eta_t + \theta_1 \eta_{t-1} + \theta_2 \eta_{t-2} + ... + \theta_q \eta_{t-q},\quad \eta_t \sim N(0, \sigma)$$


### Model order

For an ARIMA model, the number of AR lags, number of differences, and number of MA lags is called the **model order** or just **order**.

Examples.  Note $e_t \sim N(0,\sigma)$

- order (0,0,0) white noise 

$$x_t = e_t$$

- order (1,0,0) AR-1 process 

$$x_t = \phi x_{t-1} + e_t$$

- order (0,0,1) MA-1 process

$$x_t = e_t + \theta e_{t-1}$$

- order (1,0,1) AR-1 MA-1 process

$$x_t = \phi x_{t-1} + e_t + \theta e_{t-1}$$

- order (0,1,0) random walk

$$x_t - x_{t-1} = e_t$$

which is the same as

$$x_t = x_{t-1} + e_t$$


### Choosing the AR and MA levels

#### Method #1 use the ACF and PACF functions {-}

The ACF plot shows you how the correlation between $x_t$ and $x_{t+p}$ decrease as $p$ increases.  The PACF plot shows you the same but removes the autocorrelation due to lags less that $p$.

```{r fig.acf.pacf, fig.height = 6, fig.width = 8, fig.align = "center", echo=FALSE}
par(mfrow=c(3,2))
#####
ys = arima.sim(n=1000, list(ar=.7))
acf(ys, main="")
title("AR-1 (p=1, q=0)")
pacf(ys, main="")

ys = arima.sim(n=1000, list(ma=.7))
acf(ys, main="")
title("MA-1 (p=0, q=1)")
pacf(ys, main="")

ys = arima.sim(n=1000, list(ar=.7, ma=.7))
acf(ys, main="")
title("ARMA (p=1, q=1)")
pacf(ys, main="")
```

If your ACF and PACF look like the top panel, it is AR-p.  The first lag where the PACF is below the dashed lines is the $p$ lag for your model.

```{r fig.acf, fig.height = 5, fig.width = 8, fig.align = "center", echo=FALSE}
par(mfrow=c(2,2))
#####
ys = arima.sim(n=1000, list(ar=.7))
acf(ys, main="")
title("AR-1 (p=1, q=0)")
pacf(ys, main="")

ys = arima.sim(n=1000, list(ar=c(.7,.2)))
acf(ys, main="")
title("AR-2 (p=2, q=0)")
pacf(ys, main="")
```

If it looks like the middle panel, it is MA-p.  The first lag where the ACF is below the dashed lines is the $q$ lag for your model.

```{r fig.pacf, fig.height = 5, fig.width = 8, fig.align = "center", echo=FALSE}
par(mfrow=c(2,2))
#####
ys = arima.sim(n=1000, list(ma=.7))
acf(ys, main="")
title("MA-1 (p=0, q=1)")
pacf(ys, main="")

ys = arima.sim(n=1000, list(ma=c(.7,.2)))
acf(ys, main="")
title("MA-2 (p=0, q=2)")
pacf(ys, main="")
```

If it looks like the bottom panel, it is ARMA and this approach doesn't work.

```{r fig.pacf.acf.model, fig.height = 5, fig.width = 8, fig.align = "center", echo=FALSE}
par(mfrow=c(2,2))
#####
ys = arima.sim(n=1000, list(ar=.7, ma=.7))
acf(ys, main="")
title("ARMA (p=1, q=1)")
pacf(ys, main="")

ys = arima.sim(n=1000, list(ar=c(.7,.2), ma=c(.7,.2)))
acf(ys, main="")
title("ARMA (p=2, q=2)")
pacf(ys, main="")
```

#### Method #2 Use formal model selection {-}

This weighs how well the model fits against how many parameters your model has.  We will use this approach. 

The `auto.arima()` function in the forecast package in R allows you to easily estimate the $p$ and $q$ for your ARMA model.  We will use the first difference of the anchovy data since our stationarity diagnostics indicated that a first difference makes our time series stationary.

```{r auto.arima.anchovy}
anchovy.diff1 = diff(anchovy87$log.metric.tons)
forecast::auto.arima(anchovy.diff1)
```

The output indicates that the 'best' model is a MA-1 with a non-zero mean.  "non-zero mean" means that the mean of our data (`anchovy.diff1`) is not zero.


`auto.arima()` will also estimate the amount of differencing needed.

```{r auto.arima3}
forecast::auto.arima(anchovy87ts)
```

The output indicates that the 'best' model is a MA-1 with first difference.  "with drift" means that the mean of our data (`anchovy87`) is not zero.  This is the same model but the jargon regarding the mean is different.


#### More examples {-}

Let's try fitting to some simulated data.  We will simulate with `arima.sim()`.  We will specify no differencing.

```{r fitting.example.1}
set.seed(100)
a1 = arima.sim(n=100, model=list(ar=c(.8,.1)))
forecast::auto.arima(a1, seasonal=FALSE, max.d=0)
```

The 'best-fit' model is simpler than the model used to simulate the data. 


#### How often is the 'true' model is chosen? {-}

Let's fit 100 simulated time series and see how often the 'true' model is chosen. By far the correct type of model is selected, AR-p, but usually a simpler model of AR-1 is chosen over AR-2 (correct) most of the time.

```{r fit.1000}
save.fits = rep(NA,100)
for(i in 1:100){
  a1 = arima.sim(n=100, model=list(ar=c(.8,.1)))
  fit = forecast::auto.arima(a1, seasonal=FALSE, max.d=0, max.q=0)
  save.fits[i] = paste0(fit$arma[1], "-", fit$arma[2])
}
table(save.fits)
```


### Trace = TRUE

You can see what models that `auto.arima()` tried using `trace=TRUE`.  The models are selected on AICc by default and the AICc value is shown next to the model.

```{r}
forecast::auto.arima(anchovy87ts, trace=TRUE)
```

### stepwise = FALSE

By default, step-wise selection is used and an approximation is used for the models tried in the model selection step.  For a final model selection, you should turn these off.

```{r}
forecast::auto.arima(anchovy87ts, stepwise=FALSE, approximation=FALSE)
```


### Summary

- Once you have dealt with stationarity, you need to determine the order of the model: the AR part and the MA part.

- Although you could simply use `auto.arima()`, it is best to run `acf()` and `pacf()` on your data to understand it better.

  - Does it look like a pure AR process?
    
- Also evaluate if there are reasons to assume a particular structure.  

  - Are you using an established model form, from say another paper?
  
  - Are you fitting to a process that is fundamentally AR only or AR + MA?

<!--chapter:end:Forecasting-3-3-ARMA-Model-Structure.Rmd-->

## Fitting ARIMA models

We are now at step B of the Box-Jenkins Method.  

A. Model form selection

  1. Evaluate stationarity and seasonality
  2. Selection of the differencing level (d)
  3. Selection of the AR level (p)
  4. Selection of the MA level (q)
  
B. **Parameter estimation**
C. **Model checking**


### Fitting with `auto.arima()`

`auto.arima()` (in the forecast package) has many arguments.

```
auto.arima(y, d = NA, D = NA, max.p = 5, max.q = 5, max.P = 2,
  max.Q = 2, max.order = 5, max.d = 2, max.D = 1, start.p = 2,
  start.q = 2, start.P = 1, start.Q = 1, stationary = FALSE,
  seasonal = TRUE, ic = c("aicc", "aic", "bic"), stepwise = TRUE,
  trace = FALSE, approximation = (length(x) > 150 | frequency(x) > 12),
  truncate = NULL, xreg = NULL, test = c("kpss", "adf", "pp"),
  seasonal.test = c("seas", "ocsb", "hegy", "ch"), allowdrift = TRUE,
  allowmean = TRUE, lambda = NULL, biasadj = FALSE, parallel = FALSE,
  num.cores = 2, x = y, ...)
```

When just getting started, we will focus just on a few of these.

* `trace` To print out the models that were tested.
* `stepwise` and `approximation` To use slower but better estimation when selecting model order.
* `test` The test to use to select the amount of differencing.

#### Load the data {-}

Load the data by loading the **FishForecast** package. 

```{r f34-load_data}
require(FishForecast)
```

`anchovy87ts` is a ts object of the log metric tons for 1964-1987.  We will use this for `auto.arima()` however we could also use `anchovy87$log.metric.tons`.  `anchovy87ts` is just

```
anchovy87ts <- ts(anchovy87, start=1964)
```


#### Fit to the anchovy data using `auto.arima()` {-}

```{r fit.anchovy}
fit <- forecast::auto.arima(anchovy87ts)
```

Here are the values for anchovy in Table 8 of Stergiou and Christou.

Model  | $\theta_1$ | drift (c) | R$^2$ | BIC | LB
------------- | --------| --- | --- | --- | --- | 
(0,1,1) | 0.563 | 0.064 | 0.83 | 1775 | 5.4

Here is the equivalent values from the best fit from  `auto.arima()`:

```{r echo=FALSE}
res <- resid(fit)
LB <- Box.test(res, type="Ljung-Box", lag=12, fitdf=2)$statistic
dat <- anchovy87$log.metric.tons
meany <- mean(dat, na.rm=TRUE)
r2 <- 1- sum(resid(fit)^2,na.rm=TRUE)/sum((dat-meany)^2,na.rm=TRUE)

df <- data.frame(Model="(0,1,1)", theta1=-1*coef(fit)[1], drift=coef(fit)[2], R2 = r2, BIC=fit$bic, LB=LB)
```

```{r echo=FALSE, results = 'asis'}
knitr::kable(df, row.names=FALSE, format='html')
```


Where do we find each of the components of Stergiou and Christou's Table 8?  

#### The parameter estimates {-}

We can extract the parameter estimates from a fitted object in R using `coef()`.

```{r}
coef(fit)
```

The `ma1` is the same as $\theta_1$ except its negative because of the way Stergiou and Christou write their MA models.  They write it as 

$$e_t = \eta_t - \theta_1 \eta_{t-1}$$

instead of the form that `auto.arima()` uses

$$e_t = \eta_t + \theta_1 \eta_{t-1}$$


#### Computing R2 {-}

This is not output as part of a arima fitted object so we need to compute it.

```{r compute.r2}
res <- resid(fit)
dat <- anchovy87$log.metric.tons
meany <- mean(dat, na.rm=TRUE)
r2 <- 1- sum(res^2,na.rm=TRUE)/sum((dat-meany)^2,na.rm=TRUE)
```

#### Ljung-Box statistic {-}

```{r compute.LB}
LB <- Box.test(res, type="Ljung-Box", lag=12, fitdf=2)$statistic
```
fitdf=2 is from the number of parameters estimated.

#### BIC {-}

BIC is in `fit$BIC`.  Why is BIC different? Because there is a missing constant, which is fairly common.  The absolute value of BIC is unimportant.  Only its value relative to other models that you tested is important.


### Outputting the models tested

Pass in `trace=TRUE` to see a list of the models tested in `auto.arima()`'s search.  By default `auto.arima()` uses AICc for model selection and the AICc values are shown.  Smaller is better for AICc and AICc values that are different by less than 2 have similar data support.  Look for any models with similar AICc to the best selected model.  You should consider that model also.

```{r fit.anchovy.trace}
forecast::auto.arima(anchovy87ts, trace=TRUE)
```


### Repeat with the sardine data

Stergiou and Christou sardine model (Table 8) is ARIMA(0,1,0):
$$x_t = x_{t-1}+e_t$$


The model selected by `auto.arima()` is ARIMA(0,0,1):
$$x_t = e_t + \theta_1 e_{t-1}$$

```{r forecast.sardine}
forecast::auto.arima(sardine87ts)
```


Why?  Stergiou and Christou used the Augmented Dickey-Fuller test to determine the amount of differencing needed while the default for `auto.arima()` is to use the KPSS test.

#### Repeat using `test='adf'` {-}

Now the selected model is the same.

```{r}
fit <- auto.arima(sardine87ts, test="adf")
fit
```

Compare the estimated values in Stergiou and Christou Table 8:

Model  | $\theta_1$ | drift (c) | R2 | BIC | LB
------------- | --------| --- | --- | --- | --- | 
(0,1,0) | NA | NA | 0.00 | 1396 | 22.2

versus from `auto.arima()`

```{r echo=FALSE}
res <- resid(fit)
LB <- Box.test(res, type="Ljung-Box", lag=12, fitdf=0)$statistic
meany <- mean(sardine, na.rm=TRUE)
r2 <- 1- sum(resid(fit)^2,na.rm=TRUE)/sum((sardine-meany)^2,na.rm=TRUE)

df <- data.frame(Model="(0,1,0)", theta1=-1*coef(fit)[1], drift=coef(fit)[2], R2 = r2, BIC=fit$bic, LB=LB)
```

```{r echo=FALSE, results = 'asis'}
knitr::kable(df, row.names=FALSE, format='html')
```


### Missing values

These functions work fine with missing values.  Missing values are denoted NA.

```{r miss.fit}
anchovy.miss <- anchovy87ts
anchovy.miss[10:14] <- NA
fit <- auto.arima(anchovy.miss)
fit
```


### Fit a specific ARIMA model

Sometimes you don't want to search, but rather fit an ARIMA model with a specific order.  Say you wanted to fit this model:
$$x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t$$
For that you can use `Arima()` in the forecast package:

```{r}
fit.AR2 <- forecast::Arima(anchovy87ts, order=c(2,0,0))
fit.AR2
```


### Model checking

* Plot your data

* Is the plot long-tailed (Chl, some types of fish data)? Take the logarithm.

* Fit model.

* Plot your residuals

* Check your residuals for stationarity, normality, and independence

Ideally your response variable will be unimodal.  If not, you are using an ARIMA model that doesn't produce data like yours.  While you could change the assumptions about the error distribution in the model, it will be easier to transform your data.

Look at histograms of your data:

```{r fig.width=10, echo=FALSE, fig.align="center"}
par(mfrow=c(1,2))
dat <- subset(greeklandings, Species=="Anchovy" & Year<=1987)
hist(dat$metric.tons, main="metric.tons", xlab="anchovy")
hist(dat$log.metric.tons, main="log.metric.tons",xlab="log anchovy")
```

Use `checkresiduals()` to do basic diagnostics.

```{r fig.align="center"}
fit <- forecast::auto.arima(anchovy87ts)
checkresiduals(fit)
```


### Workflow for non-seasonal data

* Go through Box-Jenkins Method to evaluate stationarity

* Plot the data and make decisions about transformations to make the data more unimodal

* Make some decisions about differencing and any other data transformations via the stationarity tests

* Use `auto.arima(data, trace=TRUE)` to evaluate what ARMA models best fit the data.  Fix the differencing if needed.

* Determine a set of candidate models.  Include a null model in the candidate list.  naive and naive with drift are typical nulls.

* Test candidate models for forecast performance with cross-validation (next lecture).


### Stepwise vs exhaustive model selection

Stepwise model selection is fast and useful if you need to explore many models and it takes awhile to fit each model.  Our models fit quickly and we don't have season in our models.  Though it will not make a difference for this particular dataset, in general set `stepwise=FALSE` to do a more thorough model search.

```{r fit.anchovy2}
forecast::auto.arima(anchovy87ts, stepwise=FALSE, approximation=FALSE)
```


### Summary

- `auto.arima()` in the forecast package is a good choice for selection and fitting of ARIMA models.

- `Arima()` is a good choice when you know the order (structure) of the model.

- You (may) need to know whether the mean of the data should be zero and whether it is stationary around a linear line.
    - `include.mean=TRUE` means the mean is not zero
    - `include.drift=TRUE` means fit a model that fluctuates around a trend (up or down)


<!--chapter:end:Forecasting-3-4-ARMA-Fitting.Rmd-->

## Forecasting


```{r f35-load_data, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(reshape2)
library(tseries)
library(forecast)
```

```{r f35-load_data2, message=FALSE, warning=FALSE, echo=FALSE}
require(FishForecast)
```


The basic idea of forecasting with an ARIMA model is the same as forecasting with a time-varying regressiion model.

We estimate a model and the parameters for that model.  For example, let's say we want to forecast with ARIMA(2,1,0) model:
$$y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + e_t$$
where $y_t$ is the first difference of our anchovy data.


Let's estimate the $\beta$'s for this model from the 1964-1987 anchovy data.

```{r}
fit <- Arima(anchovy87ts, order=c(2,1,0))
coef(fit)
```

So we will forecast with this model:

$$y_t = -0.3348 y_{t-1} - 0.1454 y_{t-2} + e_t$$

So to get our forecast for 1988, we do this

$$(y_{1988}-y_{1987}) = -0.3348 (y_{1987}-y_{1986}) - 0.1454 (y_{1986}-y_{1985})$$

Thus

$$y_{1988} = y_{1987}-0.3348 (y_{1987}-y_{1986}) - 0.1454 (y_{1986}-y_{1985})$$

Here is R code to do that:

```{r}
anchovy87ts[24]+coef(fit)[1]*(anchovy87ts[24]-anchovy87ts[23])+
  coef(fit)[2]*(anchovy87ts[23]-anchovy87ts[22])
```


### Forecasting with `forecast()`

`forecast(fit, h=h)` automates the forecast calculations for us.  `forecast()` takes a fitted object, `fit`,  from `arima()` and output forecasts for `h` time steps forward.  The upper and lower prediction intervals are also computed.

```{r}
fit <- forecast::auto.arima(sardine87ts, test="adf")
fr <- forecast::forecast(fit, h=5)
fr
```

We can plot our forecast with prediction intervals.  Here is the sardine forecast:

```{r}
plot(fr, xlab="Year")
```


#### Forecast for anchovy {-}

```{r}
fit <- forecast::auto.arima(anchovy87ts)
fr <- forecast::forecast(fit, h=5)
plot(fr)
```


#### Forecasts for chub mackerel {-}

We can repeat for other species.

```{r}
spp="Chub.mackerel"
dat <- subset(greeklandings, Species==spp & Year<=1987)$log.metric.tons
dat <- ts(dat, start=1964)
fit <- forecast::auto.arima(dat)
fr <- forecast::forecast(fit, h=5)
plot(fr, ylim=c(6,10))
```


### Missing values

Missing values are allowed for `arima()` and we can product forecasts with the same code.

```{r}
anchovy.miss <- anchovy87ts
anchovy.miss[10:14] <- NA
fit <- forecast::auto.arima(anchovy.miss)
fr <- forecast::forecast(fit, h=5)
plot(fr)
```


### Null forecast models

Whenever we are testing a forecast model or procedure we have developed, we should test against 'null' forecast models.  These are standard 'competing' forecast models.

* The Naive forecast
* The Naive forecast with drift
* The mean or average forecast

#### The "Naive" forecast {-}

The "naive" forecast is simply the last value observed.  If we want to prediction landings in 2019, the naive forecast would be the landings in 2018.  This is a difficult forecast to beat!  It has the advantage of having no parameters.

In forecast, we can fit this model with the `naive()` function.  Note this is the same as the `rwf()` function.

```{r}
fit.naive <- forecast::naive(anchovy87ts)
fr.naive <- forecast::forecast(fit.naive, h=5)
plot(fr.naive)
```


#### The "Naive" forecast with drift {-}

The "naive" forecast is equivalent to a random walk with no drift.  So this
$$x_t = x_{t-1} + e_t$$
As you saw with the anchovy fit, it doesn't allow an upward trend.  Let's make it a little more flexible by add `drift`.  This means we estimate one term, the trend.

$$x_t = \mu + x_{t-1} + e_t$$


```{r}
fit.rwf <- forecast::rwf(anchovy87ts, drift=TRUE)
fr.rwf <- forecast::forecast(fit.rwf, h=5)
plot(fr.rwf)
```


#### The "mean" forecast {-}

The "mean" forecast is simply the mean of the data.  If we want to prediction landings in 2019, the mean forecast would be the average of all our data.  This is a poor forecast typically.  It uses no information about the most recent values.

In forecast, we can fit this model with the `Arima()` function and `order=c(0,0,0)`.  This will fit this model:
$$x_t = e_t$$
where $e_t \sim N(\mu, \sigma)$.

```{r}
fit.mean <- forecast::Arima(anchovy87ts, order=c(0,0,0))
fr.mean <- forecast::forecast(fit.mean, h=5)
plot(fr.mean)
```

<!--chapter:end:Forecasting-3-5-ARMA-Forecasting.Rmd-->

```{r echo=FALSE}
require(FishForecast)
```

# Exponential smoothing models

The basic idea with an exponential smoothing model is that your forecast of $x$ at time $t$ is a smoothed function of past $x$ values.

$$\hat{x}_{t} = \alpha x_{t-1} + \alpha (1-\alpha)^2 x_{t-2} + \alpha (1-\alpha)^3 x_{t-3} + \dots$$
Although this looks similar to an AR model with a constraint on the $\beta$ terms, it is fundamentally different.  There is no process model and one is **not** assuming that
$$x_{t} = \alpha x_{t-1} + \alpha (1-\alpha)^2 x_{t-2} + \alpha (1-\alpha)^3 x_{t-3} + \dots + e_t$$
The goal is to find the $\alpha$ that minimizes $x_t - \hat{x}_t$, i.e. the forecast error.  The issues regarding stationarity do not arise because we are not fitting a stationary process model.  We are not fitting a process model at all.

## Overview

### Naive model

Let's start with a simple example, an exponential smoothing model with $\alpha=1$.  This is called the Naive model:
$$\hat{x}_{t} = x_{t-1}$$
For the naive model, our forecast is simply the value in the previous time step.  For example, a naive forecast of the anchovy landings in 1988 is the anchovy landings in 1987.
$$\hat{x}_{1988} = x_{1987}$$
This is the same as saying that we put 100% of the 'weight' on the most recent value and no weight on any value prior to that.
$$\hat{x}_{1988} = 1 \times x_{1987} + 0 \times x_{1986} + 0 \times x_{1985} + \dots$$
Past values in the time series have information about the current state, but only the most recent past value.

```{r echo=FALSE}
plot(1987:1964, c(1,rep(0,23)), lwd=2, ylab="weight", xlab="", type="l")
title("weight put on past values for 1988 forecast")
```

We can fit this with `forecast::Arima()`.

```{r rwf.fit}
fit.rwf <- forecast::Arima(anchovy87ts, order=c(0,1,0))
fr.rwf <- forecast::forecast(fit.rwf, h=5)
```

Alternatively we can fit with `rwf()` or `naive()` which are shortcuts for the above lines.  All fit the same model.

```{r rwf.fit2}
fr.rwf <- forecast::rwf(anchovy87ts, h=5)
fr.rwf <- forecast::naive(anchovy87ts, h=5)
```

A plot of the forecast shows the forecast and the prediction intervals.

```{r fig.height=4.5}
plot(fr.rwf)
```


### Exponential smoothing

The naive model is a bit extreme.  Often the values prior to the last value also have some information about future states.  But the 'information content' should decrease the farther in the past that we go.

```{r echo=FALSE}
alpha <- 0.5
plot(1987:1964, alpha*(1-alpha)^(1:24-1), lwd=2, ylab="weight", xlab="", type="l")
title("more weight put on more recent values\nfor 1988 forecast")
```


A *smoother* is another word for a filter, which in time series parlance means a weighted sum of sequential values in a time series:
$$w_1 x_t + w_2 x_{t-1} + w_3 x_{t-2} + \dots$$
An exponential smoother is a filter (weighted sum) where the weights decline exponentially (Figure \@ref(fig:ets.alpha)).

```{r ets.alpha, echo=FALSE, fig.cap="Weighting function for exponential smoothing filter. The shape is determined by $\alpha$."}
alpha <- 0.5
plot(-1:-24, alpha*(1-alpha)^(1:24-1), lwd=2, ylab="weight", xlab="lag", type="l")
title("weight on x at t + lag")
```


### Exponential smoothing model

A simple exponential smoothing model is like the naive model that just uses the last value to make the forecast, but instead of only using the last value it will use values farther in the past also. The weighting function falls off exponentially as shown above.  Our goal when fitting an exponential smoothing model is to find the the $\alpha$, which determines the shape of the weighting function (Figure \@ref(fig:ets.alpha2)), that minimizes the forecast errors.

```{r ets.alpha2, echo=FALSE, fig.cap="The size of $\alpha$ determines how past values affect the forecast."}
alpha <- 1
wts <- alpha*(1-alpha)^(0:23)
plot(1987:1964, wts/sum(wts), lwd=2, ylab="weight", xlab="", type="l")
alpha <- 0.5
wts <- alpha*(1-alpha)^(0:23)
lines(1987:1964, wts/sum(wts), lwd=2, col="blue")
alpha <- 0.05
wts <- alpha*(1-alpha)^(0:23)
lines(1987:1964, wts/sum(wts), lwd=2, col="red")
legend("topleft", c("alpha=1 like naive","alpha=0.5","alpha=0.05 like mean"),lwd=2, col=c("black","blue","red"))
title("more weight put on more recent values\nfor 1988 forecast")
```




<!--chapter:end:Forecasting-4-1-Exp-Smoothing.Rmd-->

## `ets()` function

The `ets()` function in the **forecast** package fits  exponential smoothing models and produces forecasts from the fitted models. It also includes functions for plotting forecasts.

Load the data by loading the **FishForecast** package.

```{r load_data_exp_smoothing}
require(FishForecast)
```

Fit the model.

```{r fit.ann}
fit <- forecast::ets(anchovy87ts, model="ANN")
```
`model="ANN"` specifies the simple exponential smoothing model.

Create a forecast for 5 time steps into the future.

```{r fr.ann}
fr <- forecast::forecast(fit, h=5)
```

Plot the forecast.
```{r}
plot(fr)
```

Look at the estimates

```{r}
fit
```

### The weighting function

The first coefficient of the ets fit is the $\alpha$ parameter for the weighting function.

```{r ann.weighting, fig.cap="Weighting function for the simple exponential smoothing model for anchovy."}
alpha <- coef(fit)[1]
wts <- alpha*(1-alpha)^(0:23)
plot(1987:1964, wts/sum(wts), lwd=2, ylab="weight", xlab="", type="l")
```


### Decomposing your model fit

Sometimes you would like to see the smoothed level that the model estimated. You can see that with `plot(fit)` or `autoplot(fit)`.

```{r fig.height=4, fig.cap="Decompositon of an ets fit."}
autoplot(fit)
```



<!--chapter:end:Forecasting-4-2-Exp-Smoothing.Rmd-->

## ETS with trend

The simple exponential model has a level that evolves over time, but there is no trend, a tendency to go up or down.  If a time series has a trend then we might want to include this in our forecast.

#### Naive model with drift {-}

The naive model with drift is a simple example of a model with level and trend.  This model uses the last observation as the forecast but includes a trend estimated from ALL the data. 

$$\hat{x}_{T+1} = x_T + \bar{b}$$
where $\bar{b}$ is the mean trend or change from one time step to the next ($x_t-x_{t-1}$).
$$\bar{b} = \frac{1}{1-T}\sum_{t=2}^T (x_t - x_{t-1})$$

We can fit this with `forecast::Arima()`.

```{r rwf.fit.w.drift}
fit.rwf <- forecast::Arima(anchovy87ts, order=c(0,1,0), include.drift=TRUE)
fr.rwf <- forecast::forecast(fit.rwf, h=5)
```

Alternatively we can fit with `rwf()` which is a shortcut for the above lines.

```{r rwf.fit2.w.drift}
fr.rwf <- forecast::rwf(anchovy87ts, h=5, drift=TRUE)
```

A plot of the forecast shows the forecast and the prediction intervals.

```{r fig.height=4.5}
plot(fr.rwf)
```

The trend seen in the blue line is estimated from the overall trend in ALL the data.

```{r}
coef(fit.rwf)
```

The trend from all the data is (last-first)/(number of steps).

```{r}
mean(diff(anchovy87ts))
```

The naive model with drift only use the latest data to choose the level for our forecast but uses all the data to choose the trend.  It would make more sense to weight the more recent trends more heavily.  


### Exponential smoothing model with trend

The exponential smoothing model has a level term which is an exponential weighting of past $x$ and a trend term  which is an exponential weighting of past trends $x_t - x_{t-1}$.

$$\hat{x}_{T+1} = l_T + b_T$$
where $b_T$ is a weighted average with the more recent trends given more weight.

$$b_T = \sum_{t=2}^T \beta (1-\beta)^{t-2}(x_t - x_{t-1})$$
The value of $\beta$ determines how much past trends affect the trend we use in our forecast. 

```{r echo=FALSE}
alpha <- 1
wts <- alpha*(1-alpha)^(0:23)
plot(1987:1964, wts/sum(wts), lwd=2, ylab="weight", xlab="", type="l")
alpha <- 0.5
wts <- alpha*(1-alpha)^(0:23)
lines(1987:1964, wts/sum(wts), lwd=2, col="blue")
alpha <- 0.05
wts <- alpha*(1-alpha)^(0:23)
lines(1987:1964, wts/sum(wts), lwd=2, col="red")
legend("topleft", c("beta=1","beta=0.5","beta=0.05 like naive"),lwd=2, col=c("black","blue","red"))
title("more weight put on more recent values\nfor 1988 forecast")
```

#### Fit with `ets()` {-}

To fit an exponential smoothing model with trend, we use `model="AAN".

```{r}
fit <- forecast::ets(anchovy87ts, model="AAN")
fr <- forecast::forecast(fit, h=5)
plot(fr)
```

Passing in "AAN", specifies that the model must have a trend. We can also let `ets()` choose whether or not to include a trend by passing in "AZN".

Here is a summary of the simple ETS models and the model code for each.

model  | "ZZZ" | alternate function |
------------- | ------------- | --------- |
exponential smoothing no trend | "ANN" | `ses()` |
exponential smoothing with trend  | "AAN" | `holt()` |
exponential smoothing choose trend  | "AZN" | NA |

The alternate function does exactly the same fitting.  It is just a 'shortcut'.  


### Produce forecast using a previous fit

Sometimes you want to estimate a forecasting model from one dataset and use that model to forecast another dataset or another area.  Here is how to do that.

This is the fit to the 1964-1987 data:

```{r}
fit1 <- forecast::ets(anchovy87ts, model="ANN")
```

Use that model with the 2000-2007 data and produce a forecast:

```{r fit.new.ann}
dat <- subset(greeklandings, Species=="Anchovy" & Year>=2000 & Year<=2007)
dat <- ts(dat$log.metric.tons, start=2000)
fit2 <- forecast::ets(dat, model=fit1)
fr2 <- forecast::forecast(fit2, h=5)
```

```{r}
plot(fr2)
```



<!--chapter:end:Forecasting-4-3-Exp-Smoothing.Rmd-->

## Forecast performance

We can evaluate the forecast performance with forecasts of our test data or we can use all the data and use time-series cross-validation.

Let's start with the former.

```{r echo=FALSE}
traindat <- window(anchovyts,1964,1987)
testdat <- window(anchovyts,1988,1989)
```


### Test forecast performance

#### Test against a test data set {-}

We will fit an an exponential smoothing model with trend to the training data and make a forecast for the years that we 'held out'. 

```{r fig.height=4}
fit1 <- forecast::ets(traindat, model="AAN")
h=length(testdat)
fr <- forecast::forecast(fit1, h=h)
plot(fr)
points(testdat, pch=2, col="red")
legend("topleft", c("forecast","actual"), pch=c(20,2), col=c("blue","red"))
```


We can calculate a variety of forecast error metrics with

```{r}
forecast::accuracy(fr, testdat)
```

We would now repeat this for all the models in our candidate set and choose the model with the best forecast performance.


#### Test using time-series cross-validation {-}

Another approach is to use all the data and test a series of forecasts made by fitting the model to different lengths of the data.

In this approach, we don't have test data.  Instead we will use all the data for fitting and for forecast testing.

We will redefine `traindat` as all our Anchovy data.

```{r subset.anch, echo=FALSE}
traindat <- window(anchovyts,1964,1989)
```

#### tsCV() function {-}

We will use the `tsCV()` function.  We need to define a function that returns a forecast.

```{r def.far2}
far2 <- function(x, h, model){
  fit <- ets(x, model=model)
  forecast(fit, h=h)
  }
```

Now we can use `tsCV()` to run our `far2()` function to a series of training data sets.  We will specify that a NEW ets model be estimated for each training set.  We are not using the weighting estimated for the whole data set but estimating the weighting new for each set.

The `e` are our forecast errors for all the forecasts that we did with the data.

```{r}
e <- forecast::tsCV(traindat, far2, h=1, model="AAN")
e
```

Let's look at the first few `e` so we see exactly with `tsCV()` is doing.

```{r}
e[2]
```

This uses training data from $t=1$ to $t=2$ so fits an ets to the first two data points alone.  Then it creates a forecast for $t=3$ and compares that forecast to the actual value observed for $t=3$.

```{r}
TT <- 2 # end of the temp training data
temp <- traindat[1:TT]
fit.temp <- forecast::ets(temp, model="AAN")
fr.temp <- forecast::forecast(fit.temp, h=1)
traindat[TT+1] - fr.temp$mean
```

```{r}
e[3]
```

This uses training data from $t=1$ to $t=2$ so fits an ets to the first two data points alone.  Then it creates a forecast for $t=3$ and compares that forecast to the actual value observed for $t=3$.

```{r}
TT <- 3 # end of the temp training data
temp <- traindat[1:TT]
fit.temp <- forecast::ets(temp, model="AAN")
fr.temp <- forecast::forecast(fit.temp, h=1)
traindat[TT+1] - fr.temp$mean
```

#### Forecast accuracy metrics {-}

Once we have the errors from `tsCV()`, we can compute forecast accuracy metrics.

RMSE: root mean squared error
```{r rmse}
rmse <- sqrt(mean(e^2, na.rm=TRUE))
```

MAE: mean absolute error
```{r mae}
mae <- mean(abs(e), na.rm=TRUE)
```


### Testing a specific ets model

By specifying `model="AAN"`, we estimated a new ets model (meaning new weighting) for each training set used.  We might want to specify that we use only the weighting we estimated for the full data set.

We do this by passing in a fit to `model`.

The `e` are our forecast errors for all the forecasts that we did with the data. `fit1` below is the ets estimated from all the data 1964 to 1989.  Note, the code will produce a warning that it is estimating the initial value and just using the weighting.  That is what we want.

```{r, message=FALSE, warning=FALSE}
fit1 <- forecast::ets(traindat, model="AAN")
e <- forecast::tsCV(traindat, far2, h=1, model=fit1)
e
```


<!--chapter:end:Forecasting-4-4-Exp-Smoothing.Rmd-->

## Further Reading

This chapter covers a small sample of the simpler ETS models that can be fit. There are many other types of more complex ETS models and the `ets()` function will fit these also.  Rob J Hyndman (lead on the forecast package)  and George Athanasopoulos have an excellent online text on practical forecasting and exponential smoothing. Read [their chapter](https://otexts.org/fpp2/expsmooth.html) on exponential smoothing to learn more about these models and how to use them.


<!--chapter:end:Forecasting-4-5-Exp-Smoothing.Rmd-->

# Testing forecast accuracy {#perf-testing}

Once you have found a set of possible forecast models, you are ready to compare forecasts from a variety of models and choose a forecast model. To quantify the forecast performance, we need to create forecasts for data that we have so that we can compare the forecast to actual data.  There are two approaches to this: holding out data for testing and cross-validation.
  
```{r f36-load_data, message=FALSE, warning=FALSE, echo=FALSE}
require(ggplot2)
require(FishForecast)
```


## Training set/test set

One approach is to 'hold out' some of your data as the test data and did not use it at all in your fitting.  To measure the forecast performance, you fit to your training data and test the forecast against the data in the test set.  This is the approach that Stergiou and Christou used.

Stergiou and Christou used 1964-1987 as their training data and tested their forecasts against 1988 and 1989.  

```{r echo=FALSE}
traindat <- window(anchovyts, 1964, 1987)
testdat <- window(anchovyts, 1988, 1989)
```

### Forecast versus actual

We will fit to the training data and make a forecast for the test data.  We can then compare the forecast to the actual values in the test data.

```{r}
fit1 <- forecast::auto.arima(traindat)
fr <- forecast::forecast(fit1, h=2)
fr
```

Plot the forecast and compare to the actual values in 1988 and 1989.

```{r}
plot(fr)
points(testdat, pch=2, col="red")
legend("topleft", c("forecast","actual"), pch=c(20,2), col=c("blue","red"))
```


## Cross-Validation

An alternate approach to is to use cross-validation.  This approach uses windows or shorter segments of the whole time series to make a series of single forecasts.  We can use either a variable length or a fixed length window.

### Variable window

For the variable length window approach applied to  the Anchovy time series, we would fit the model 1964-1973 and forecast 1974, then 1964-1974 and forecast 1975, then 1964-1975 and forecast 1976, and continue up to 1964-1988 and forecast 1989.  This would create 16 forecasts which we would compare to the actual landings.  The window is 'variable' because the length of the time series used for fitting the model, keeps increasing by 1.

```{r cv.sliding, echo=FALSE}
p <- list()
for(i in 1:9){
  p[[i]]<-ggplot(subset(greeklandings, Species=="Anchovy"&Year<1974+i), aes(x=Year, y=log.metric.tons))+geom_point()+ylab("landings")+xlab("")+xlim(1964,1990)+ylim(8,12)+
    geom_point(data=subset(greeklandings, Species=="Anchovy"&Year==1974+i),aes(x=Year,y=log.metric.tons),color="red") +
    ggtitle(paste0("forecast ",i))
}
gridExtra::grid.arrange(
  p[[1]],p[[2]],p[[3]],p[[4]],p[[5]],p[[6]],p[[7]],p[[8]],p[[9]],nrow=3,
  top = grid::textGrob("Cross-validation: sliding window", gp=grid::gpar(fontsize=20,font=3))
)
```

### Fixed window

Another approach uses a fixed window.  For example, a 10-year window.

```{r cv.fixed, echo=FALSE}
p <- list()
for(i in 1:9){
  p[[i]]<-ggplot(subset(greeklandings, Species=="Anchovy"&Year>=1964+i-1&Year<1974+i), aes(x=Year, y=log.metric.tons))+geom_point()+ylab("landings")+xlab("")+xlim(1964,1990)+ylim(8,12)+
    geom_point(data=subset(greeklandings, Species=="Anchovy"&Year==1974+i),aes(x=Year,y=log.metric.tons),color="red") +
    ggtitle(paste0("forecast ",i))
}
gridExtra::grid.arrange(
  p[[1]],p[[2]],p[[3]],p[[4]],p[[5]],p[[6]],p[[7]],p[[8]],p[[9]],nrow=3,
  top = grid::textGrob("Cross-validation: fixed window", gp=grid::gpar(fontsize=20,font=3))
)
```

### Cross-validation farther into the future

Sometimes it makes more sense to test the performance for forecasts that are farther in the future.  For example, if the data from your catch surveys takes some time to process, then you might need to make forecasts that are farther than 1 year from your last data point.

In that case, there is a gap between your training data and your test data point.

```{r cv.sliding.4plot, echo=FALSE}
p <- list()
for(i in 1:9){
  p[[i]]<-ggplot(subset(greeklandings, Species=="Anchovy"&Year<1974+i), aes(x=Year, y=log.metric.tons))+geom_point()+ylab("landings")+xlab("")+xlim(1964,1990)+ylim(8,12)+
    geom_point(data=subset(greeklandings, Species=="Anchovy"&Year==1974+i+3),aes(x=Year,y=log.metric.tons),color="red") +
    ggtitle(paste0("forecast ",i))
}
gridExtra::grid.arrange(
  p[[1]],p[[2]],p[[3]],p[[4]],p[[5]],p[[6]],p[[7]],p[[8]],p[[9]],nrow=3,
  top = grid::textGrob("Cross-validation: 4 step ahead forecast", gp=grid::gpar(fontsize=20,font=3))
)
```

<!--chapter:end:Forecasting-5-1-Performance-Testing.Rmd-->

## Metrics

How to we quantify the difference between the forecast and the actual values in the test data set?

Let's take the example of a training set/test set.

```{r echo=FALSE}
traindat <- window(anchovyts, 1964, 1987)
testdat <- window(anchovyts, 1988, 1989)
fit1 <- forecast::auto.arima(traindat)
fr <- forecast::forecast(fit1, h=2)
```

The forecast errors are the difference between the test data and the forecasts.

```{r}
fr.err <- testdat - fr$mean
fr.err
```


### `accuracy()` function

The `accuracy()` function in forecast provides many different metrics such as mean error, root mean square error, mean absolute error, mean percentage error, mean absolute percentage error.  It requires a forecast object and a test data set that is the same length.

```{r}
accuracy(fr, testdat)
```

The metrics are:

#### ME Mean err {-}

```{r}
me <- mean(fr.err)
me
```

#### RMSE Root mean squared error {-}

```{r}
rmse <- sqrt(mean(fr.err^2))
rmse
```

#### MAE Mean absolute error {-}

```{r}
mae <- mean(abs(fr.err))
mae
```


#### MPE Mean percentage error {-}

```{r}
fr.pe <- 100*fr.err/testdat
mpe <- mean(fr.pe)
mpe
```

#### MAPE Mean absolute percentage error {-}

```{r}
mape <- mean(abs(fr.pe))
mape
```

```{r}
accuracy(fr, testdat)[,1:5]
```

```{r}
c(me, rmse, mae, mpe, mape)
```


### Test multiple models

Now that you have some metrics for forecast accuracy, you can compute these for all the models in your candidate set.

```{r}
# The model picked by auto.arima
fit1 <- forecast::Arima(traindat, order=c(0,1,1))
fr1 <- forecast::forecast(fit1, h=2)
test1 <- forecast::accuracy(fr1, testdat)[2,1:5]

# AR-1
fit2 <- forecast::Arima(traindat, order=c(1,1,0))
fr2 <- forecast::forecast(fit2, h=2)
test2 <- forecast::accuracy(fr2, testdat)[2,1:5]

# Naive model with drift
fit3 <- forecast::rwf(traindat, drift=TRUE)
fr3 <- forecast::forecast(fit3, h=2)
test3 <- forecast::accuracy(fr3, testdat)[2,1:5]
```

#### Show a summary {-}

```{r results='asis', echo=FALSE}
sum.tests <- rbind(test1, test2, test3)
row.names(sum.tests) <- c("(0,1,1)","(1,1,0)","Naive")
sum.tests <- format(sum.tests, digits=3)
knitr::kable(sum.tests, format="html")
```


### Cross-validation

Computing forecast errors and performance metrics with time series cross-validation is similar to the training set/test test approach.

The first step to using the `tsCV()` function is to define the function that returns a forecast for your model.  Your function needs to take `x`, a time series, and `h` the length of the forecast.  You can also have other arguments if needed.  Here is an example function for a forecast from an ARIMA model.

```{r}
fun <- function(x, h, order){
  forecast::forecast(Arima(x, order=order), h=h)
}
```

We pass this into the `tsCV()` function.  `tsCV()` requires our dataset and our forecast function.  The arguments after the forecast function are those we included in our `fun` definition.  `tsCV()` returns a time series of errors.

```{r}
e <- forecast::tsCV(traindat, fun, h=1, order=c(0,1,1))
```

We then can compute performance metrics from these errors.

```{r}
tscv1 <- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE))
tscv1
```

#### Cross-validation farther in future {-}

Compare accuracy of forecasts 1 year out versus 4 years out.  If `h` is greater than 1, then the errors are returned as a matrix with each `h` in a column.  Column 4 is the forecast, 4 years out.

```{r cv.sliding.4}
e <- forecast::tsCV(traindat, fun, h=4, order=c(0,1,1))[,4]
#RMSE
tscv4 <- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE))
rbind(tscv1, tscv4)
```

As we would expect, forecast errors are higher when we make forecasts farther into the future.

#### Cross-validation with a fixed window {-}

Compare accuracy of forecasts with a fixed 10-year window and 1-year out forecasts.

```{r fixed.cv.1}
e <- forecast::tsCV(traindat, fun, h=1, order=c(0,1,1), window=10)
#RMSE
tscvf1 <- c(ME=mean(e, na.rm=TRUE), RMSE=sqrt(mean(e^2, na.rm=TRUE)), MAE=mean(abs(e), na.rm=TRUE))
tscvf1
```

#### All the forecasts tests together {-}

Here are all 4 types of forecasts tests together.  There is not right approach.  Time series cross-validation has the advantage that you test many more forecasts and use all your data.

```{r results='asis'}
comp.tab <- rbind(train.test=test1[c("ME","RMSE","MAE")],
      tsCV.variable1=tscv1,
      tsCV.variable4=tscv4,
      tsCV.fixed1=tscvf1)
knitr::kable(comp.tab, format="html")
```



<!--chapter:end:Forecasting-5-2-Metrics.Rmd-->

## Candidate model set

Once you have explored a variety of forecasting models you can come up with a candidate set of models along with a set of null models.

Here is our candidate models for the anchovy along with the code to fit and create a forecast from each model.

* Exponential smoothing model with trend

```
fit <- forecast::ets(traindat, model="AAN")
fr <- forecast::forecast(fit, h=1)
```

* Exponential smoothing model no trend

```
fit <- forecast::ets(traindat, model="ANN")
fr <- forecast::forecast(fit, h=1)
```

* ARIMA(0,1,1) with drift (best)

```
fit <- forecast::Arima(traindat, order(0,1,1), include.drift=TRUE)
fr <- forecast::forecast(fit, h=1)
```

* ARIMA(2,1,0) with drift (within 2 AIC of best)

```
fit <- forecast::Arima(traindat, order(2,1,0), include.drift=TRUE)
fr <- forecast::forecast(fit, h=1)
```

* Time-varying regression with linear time

```
traindat$t <- 1:24
fit <- lm(log.metric.tons ~ t, data=traindat)
fr <- forecast::forecast(fit, newdata=data.frame(t=25))
```

We also need to include null models in our candidate set.

#### Null models {-}

* Naive no trend

```
fit <- forecast::Arima(traindat, order(0,1,0))
fr <- forecast::forecast(fit, h=1)
# or simply
fr <- forecast::rwf(traindat, h=1)
```

* Naive with trend

```
fit <- forecast::Arima(traindat, order(0,1,0), include.drift=TRUE)
fr <- forecast::forecast(fit, h=1)
# or simply
fr <- forecast::rwf(traindat, drift=TRUE, h=1)
```

* Average or mean

```
fit <- forecast::Arima(traindat, order(0,0,0))
fr <- forecast::forecast(fit, h=1)
```

<!--chapter:end:Forecasting-5-3-Candidate-Model-Sets.Rmd-->

## Testing the candidate model set

With a set of candidate models, we can prepare tables showing the forecast performance for each model.

For each model, we will do the same steps:

1. Fit the model

2. Create forecasts for a test data set or use cross-validation

3. Compute forecast accuracy metrics for the forecasts

Note when you compare models, you can use both 'training data/test data' and use time-series cross-validation, but report the metrics in separate columns.  Example, 'RMSE from tsCV' and 'RMSE from test data'.


### Fit each of our candidate models

We will define the training data as 1964 to 1987 and the test data as 1988 and 1989. The full data is 1964 to 1989.

```{r read_data}
fulldat <- window(anchovyts, 1964, 1989)
traindat <- window(anchovyts, 1964, 1987)
testdat <- window(anchovyts, 1988, 1989)
```

We will store our fits and forecasts in a list for easy access. `fun.list` is the function to pass to `tsCV()`.

```{r}
fit.list <- list()
fr.list <- list()
fun.list <- list()
n.fr <- length(testdat)
```

For each model, we will fit, forecast, and define a forecast function.

* Exponential smoothing model with trend

```{r}
modelname <- "ETS w trend"
fit <- ets(traindat, model="AAN")
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ forecast(ets(x, model="AAN"), h=h) }
```

* Exponential smoothing model no trend

```{r}
modelname <- "ETS no trend"
fit <- ets(traindat, model="ANN")
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ forecast(ets(x, model="ANN"), h=h) }
```

* ARIMA(0,1,1) with drift (best)

```{r}
modelname <- "ARIMA(0,1,1) w drift"
fit <- Arima(traindat, order=c(0,1,1), include.drift=TRUE)
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ forecast(Arima(x, order=c(0,1,1), include.drift=TRUE),h=h) }
```

* ARIMA(2,1,0) with drift (within 2 AIC of best)

```{r}
modelname <- "ARIMA(2,1,0) w drift"
fit <- Arima(traindat, order=c(2,1,0), include.drift=TRUE)
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ forecast(Arima(x, order=c(2,1,0), include.drift=TRUE),h=h) }
```

* Time-varying regression with linear time

```{r}
TT <- length(traindat)
#make a data.frame for lm
dat <- data.frame(log.metric.tons=traindat, t=1:TT)
modelname <- "TV linear regression"
fit <- lm(log.metric.tons ~ t, data=dat)
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, newdata=data.frame(t=TT+1:n.fr))
fun.list[[modelname]] <- function(x, h){ 
  TT <- length(x)
  dat <- data.frame(log.metric.tons=x, t=1:TT)
  ft <- lm(log.metric.tons ~ t, data=dat)
  forecast(ft, newdata=data.frame(t=TT+h))
  }
```

* Naive no trend

```{r}
modelname <- "Naive"
fit <- Arima(traindat, order=c(0,1,0))
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ rwf(x,h=h) }
```

* Naive with trend

```{r}
modelname <- "Naive w trend"
fit <- Arima(traindat, order=c(0,1,0), include.drift=TRUE)
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ rwf(x, drift=TRUE, h=h) }
```

* Average or mean

```{r}
modelname <- "Average"
fit <- Arima(traindat, order=c(0,0,0))
fit.list[[modelname]] <- fit
fr.list[[modelname]] <- forecast(fit, h=n.fr)
fun.list[[modelname]] <- function(x, h){ forecast(Arima(x, order=c(0,0,0)),h=h) }
```

## Models fit

Now we can use `names()` to see the models that we have fit.  If we want to add more, we use the code above as a template.

```{r}
modelnames <- names(fit.list)
modelnames
```


### Metrics for each model

We will run the models and compute the forecast metrics for each and put in a table.

```{r}
restab <- data.frame(model=modelnames, RMSE=NA, ME=NA, tsCV.RMSE=NA, AIC=NA, BIC=NA, stringsAsFactors = FALSE)
for(i in modelnames){
  fit <- fit.list[[i]]
  fr <- fr.list[[i]]
  restab$RMSE[restab$model==i] <- accuracy(fr, testdat)["Test set","RMSE"]
  restab$ME[restab$model==i] <- accuracy(fr, testdat)["Test set","ME"]
  e <- tsCV(traindat, fun.list[[i]], h=1)
  restab$tsCV.RMSE[restab$model==i] <- sqrt(mean(e^2, na.rm=TRUE))
  restab$AIC[restab$model==i] <- AIC(fit)
  restab$BIC[restab$model==i] <- BIC(fit)
}
```

Add on $\Delta$AIC and $\Delta$BIC.  Sort by $\Delta$AIC and format to have 3 digits.
```{r}
restab$DeltaAIC <- restab$AIC-min(restab$AIC)
restab$DeltaBIC <- restab$BIC-min(restab$BIC)
restab <- restab[order(restab$DeltaAIC),]
resfor <- format(restab, digits=3, trim=TRUE)
```

Bold the minimum values in each column so they are easy to spot.
```{r}
for(i in colnames(resfor)){
  if(class(restab[,i])=="character") next
  if(i!="ME") testval <- restab[,i] else testval <- abs(restab[,i])
  theminrow <- which(testval==min(testval))
  resfor[theminrow, i] <- paste0("**",resfor[theminrow,i],"**")
}
```

This is the table of FORECAST performance metrics.  Not how well it fits the data, but how well it forecasts out of the data.  RSME and ME are for the 2 data points in 1988 and 1989 that were held out for testing.  tsCV.RMSE is the RSME for the time-series crossvalidation that makes a series of forecasts for each point in the data.  AIC and BIC are information criteria, which are a measure of data support for each model.

```{r}
knitr::kable(resfor)
```

<!--chapter:end:Forecasting-5-4-Candidate-Model-Sets-Test.Rmd-->


```{r f60-load_packages, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
```

# Covariates

Often we want to explain the variability in our data using covariates or exogenous variables.  We may want to do this in order to create forecasts using information from the covariates in time step $t-1$ or $t$ to help forecast at time $t$.  Or we may want to understand what causes variability in our data in order to help understand the underlying process.

We can include covariates in the time-varying regression model and the ARIMA models.  We cannot include covariates in an exponential smoothing model.  That doesn't make sense as a exponential model is a type of filter of the data not a 'process' model.

In this chapter, I show a number of approaches for including covariates in a multivariate regression model (MREG) with temporally independent errors. This is not a time series model per se, but rather a multivariate regression applied to time-ordered data. MREG models with auto-regressive errors and auto-regressive models with covariates will be addressed in a separate chapter.  

I illustrate a variety of approaches for developing a set of covariate for a MREG model.  The first approach is variable selection, which was the approach used by Stergiou and Christou for their MREG models (\@ref(MREGVAR)). The other approaches are penalized regression (\@ref(MREGPR)), relative importance metrics (\@ref(MREGRELPO)), and orthogonalization (\@ref(MREGORTHO)). These approaches all deal with the problem of selecting a set of covariates to include in your model.  

Before discussing models with covariates, I will show a variety of approaches for evaluating the collinearity in your covariate set. Collinearity will dramatically affect your inferences concerning the effect of your covariates and needs to be assessed before you begin modeling.

## Covariates used in Stergiou and Christou

Stergiou and Christou used five environmental covariates: air temperature (air), sea-level pressure (slp), sea surface temperature (sst), vertical wind speed (vwnd), and wind speed cubed (wspd3).  I downloaded monthly values for these covariates from the three 1 degree boxes used by Stergiou and Christou from the ICOADS database. I then computed a yearly average over all months in the three boxes.  These yearly average environmental covariates are in `covsmean.year`, which is part of `landings` in the **FishForecast** package.

```{r f37-load_data}
require(FishForecast)
colnames(ecovsmean.year)
```

The covariates are those in Stergiou and Christou with the following differences. I used the ICOADS data not the COADS data.  The boxes are 1 degree but on 1 degree centers not 0.5 centers.  Thus the box is 39.5-40.5 not 39-40.  ICOADS does not include 'vertical wind'.  I used NS winds which may be different.  The code to download the ICOADS data is in the appendix.

In addition to the environmental covariates, Stergiou and Christou used many covariates of fishing effort for trawlers, purse seiners, beach seiners, other coastal boats and demersal (sum of trawlers, beach seiners and other coastal boats).  For each fishery type, they used data on number of fishers (FI), number of boats (BO), total engine horse power (HP), total boat tonnage (TO).  They also used an economic variable: value (VA) of catch for trawlers, purse seiners, beach seiners, other coastal boats. These fishery covariates were extracted from the Greek Statistical Reports  (\@ref(landingsdata)).

```{r fish.cov}
colnames(greekfish.cov)
```
For anchovy, the fishery effort metrics from the purse seine fishery were used. Lastly, biological covariates were included which were the landings of other species.  Stergiou and Christou state (page 118) that the other species modeled by VAR (page 114) was included. This would imply that sardine was used as an explanatory variable.  However in Table 3 (page 119), it appears that *Trachurus* (Horse mackerel) was included.  It is not clear if sardine was also included but not chosen as an important variable.  I included *Trachurus* and not sardine as the biological explanatory variable.

### Preparing the data frame {-}

```{r cov.dataframe, echo=FALSE}
# response
df <- data.frame(anchovy=anchovy$log.metric.tons,
                 Year=anchovy$Year)
Year1 <- df$Year[1]
Year2 <- df$Year[length(df$Year)]
df <- subset(df, Year>=Year1+1 & Year<=Year2)
# biological covariates
df.bio <- subset(greeklandings, Species=="Horse.mackerel")[,c("Year","log.metric.tons")]
df.bio <- subset(df.bio, Year>=Year1 & Year<=Year2-1)[,-1,drop=FALSE] # [,-1] to remove year
colnames(df.bio) <- "Trachurus"
# environmental covariates
ecovsmean.year[,"vwnd.m/s"]<- abs(ecovsmean.year[,"vwnd.m/s"])
df.env <- log(subset(ecovsmean.year, Year>=Year1 & Year<=Year2-1)[,-1])
# fishing effort
df.fish <- log(subset(greekfish.cov, Year>=Year1 & Year<=Year2-1)[,-1])
purse.cols <- stringr::str_detect(colnames(df.fish),"Purse.seiners")
df.fish <- df.fish[,purse.cols]
df.fish <- df.fish[!(colnames(df.fish)=="Purse.seiners.VAP")]
# assemble
df <- data.frame(
            df, df.bio, df.env, df.fish
            )
df$Year <- df$Year-df$Year[1]+1
colnames(df) <- sapply(colnames(df), function(x){rev(stringr::str_split(x,"Purse.seiners.")[[1]])[1]})
colnames(df) <- sapply(colnames(df), function(x){stringr::str_split(x,"[.]")[[1]][1]})
df <- df[,colnames(df)!="VAP"]
# all the data to 2007
df.full <- df
# only training data
df <- subset(df, Year>=1965-1964 & Year<=1987-1964)
save(df, df.full, file="MREG_Data.RData")
```

We will model anchovy landings as the response variable.  The covariates are lagged by one year, following Stergiou and Christou.  This means that the catch in year $t$ is regressed against the covariates in year $t-1$. We set up our data frame as follows. We use the 1965 to 1987 catch data as the response. We use 1964 to 1986, so year prior, for all the explanatory variables and we log transform the explanatory variables (following Stergiou and Christou). We use $t$ 1 to 23 as a "year" covariate. Our data frame will have the following columns:
```{r df.columns}
colnames(df)
```
In total, there are `r ncol(df)-1` covariates and `r nrow(df)` years of data---which is not much data per explanatory variable.  Section \@ref(cov.df) shows the R code to create the `df` data frame with the response variable and all the explanatory variables. 

For most of the analyses, we will use the untransformed variables, however for some analyses, we will want the effect sizes (the estimated $\beta$'s) to be on the same scale.  For these analyses, we will use the z-scored variables, which will be stored in data frame `dfz`.  z-scoring removes the mean and normalizes the variance to 1. Here is a loop to demean and rescale our data frame.

```{r z.score}
dfz <- df
n <- nrow(df)
for(i in colnames(df)){
  pop_sd <- sd(df[,i])*sqrt((n-1)/n)
  pop_mean <- mean(df[,i])
  dfz[,i] <- (df[,i]-pop_mean)/pop_sd
}
```

The function `scale()` will also do a scaling to the unbiased variance instead of the sample variance (divide by $n-1$ instead of $n$) and will return a matrix. We will use `dfz` which is scaled to the sample variance as we will need this for the chapter on Principal Components Regression.

```{r scale, eval=FALSE}
df.scale <- as.data.frame(scale(df))
```

<hr>

### Creating the data frame for model fitting {#cov.df}

Code to make the `df` data frame used in the model fitting functions.

```{r ref.label="cov.dataframe",eval=FALSE}
```

<!--chapter:end:Forecasting-6-0-Covariates.Rmd-->

## Collinearity {#collinear}

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->

Collinearity is near-linear relationships among the explanatory variables. Collinearity causes many problems such as inflated standard errors of the coefficients and correspondingly unbiased but highly imprecise estimates of the coefficients, false p-values, and poor predictive accuracy of the model. Thus it is important to evaluate the level of collinearity in your explanatory variables.

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(car)
library(Hmisc)
library(corrplot)
library(olsrr)
```

#### Pairs plot {-}

One way to see this is visually is with the `pairs()` plot.  A pairs plot of fishing effort covariates reveals high correlations between Year, HPP and TOP.

```{r pairs.fish}
pairs(df[,c(2,9:12)])
```

The environmental covariates look generally ok.
```{r pairs.env}
pairs(df[,c(2,4:8)])
```

Another way that we can visualize the problem is by looking at the correlation matrix using the **corrplot** package.

```{r}
library(corrplot)
X <- as.matrix(df[,colnames(df)!="anchovy"])
corrplot::corrplot(cor(X))
```

#### Variance inflation factors {-}

Another way is to look for collinearity is to compute the variance inflation factors (VIF). The variance inflation factor is an estimate of how much larger the variance of a coefficient estimate is compared to if the variable were uncorrelated with the other explanatory variables in the model.  If the VIF of variable $i$ is $z$, then the standard error of the $\beta_i$ for variable $i$ is $\sqrt{z}$ times larger than if variable $i$ were uncorrelated with the other variables. For example, if VIF=10, the standard error of the coefficient estimate is 3.16 times larger (inflated).  The rule of thumb is that any of the variables with VIF greater than 10 have collinearity problems.

The `vif()` function in the **car** package will compute VIFs for us.  

```{r vif.car}
full <- lm(anchovy ~ ., data=df)
car::vif(full)
```

The `ols_vif_tol()` function in the **olsrr** [package](https://cran.r-project.org/package=olsrr) also computes the VIF.

```{r vif.olsrr}
olsrr::ols_vif_tol(full)
```

This shows that Year, HPP and TOP have severe collinearity problems, and BOP and *Trachusus* also have collinearity issues, though lesser.

#### Condition indices {-}

Condition indices are computed from the eigenvalues of the correlation matrix of the variates. The size of the index will be greatly affected by whether you have standardized the variance of your covariates, unlike the other tests described here.

$$ci = \sqrt{max(eigenvalue)/eigenvalue}$$

```{r}
vars <- as.matrix(dfz[,-1])
res <- eigen(crossprod(vars))$values
sqrt(max(res)/res)
```

```{r}
vars <- as.matrix(dfz[,-1])
res <- eigen(crossprod(vars))$values
sqrt(max(res)/res)
```

See the information from the olsrr package on [condition indices](https://cran.r-project.org/web/packages/olsrr/vignettes/regression_diagnostics.html) on how to use condition indices to spot collinearity.  Basically you are looking for condition indices greater than 30 whether the proportion of variance for the covariate is greater than 0.5.  In the table below, this criteria identifies Year, BOP, and TOP. Note that the test was done with the standardized covariates (`dfz`).

```{r}
model <- lm(anchovy ~ ., data=dfz)
round(olsrr::ols_eigen_cindex(model), digit=2)
```

#### redun() {-}

The **Hmisc** library also has a redundancy function (`redun()`) that can help identify which variables are redundant.  This identifies variables that can be explained with an $R^2>0.9$ by a linear (or non-linear) combination of other variables.  We are fitting a linear model, so we set `nk=0` to force `redun()` to only look at linear combinations.

We use `redun()` only on the explanatory variables and thus remove the first column, which is our response variable (anchovy).

```{r Hmisc.redun}
a <- Hmisc::redun(~ .,data=df[,-1], nk=0)
a$Out
```

This indicates that TOP and HPP can be explained by the other variables.

### Effect of collinearity

One thing that happens when we have collinearity is that we will get "complementary" (negative matched by positive) and very large coefficients in the variables that are collinear. We see this when we fit a linear regression with all the variables. I use the z-scored data so that the effect sizes (x-axis) are on the same scale.

```{r coef.full, echo=FALSE}
fit.full <- lm(anchovy ~ ., data=dfz)
coef.full <- coef(fit.full)[-1]
labs <- names(coef(fit.full))[-1]
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.full), names.arg=labs, 
        horiz=TRUE, las=2, beside=TRUE,
        col=c("aquamarine3"))
legend("topright", c("ols"), pch=15, 
       col=c("aquamarine3"), 
       bty="n")
par(op)
```

The Year coefficients is very large and the TOP and HPP coefficients are negative and very large.
If we look at the fit, we see the at the standard errors for Year, TOP and HPP are very large.  The p-value for Year is significant, however in the presence of severe collinearity, reported p-values should not be trusted.

```{r full.summary}
summary(fit.full)
```

Stergiou and Christou do not state how (if at all) they address the collinearity in the explanatory variables, but it is clearly present. In the next chapter, I will show how to develop a multivariate regression model using variable selection. This is the approach used by Stergiou and Christou. Keep in mind that variable selection will not perform well when there is collinearity in your covariates and that variable selection is prone to over-fitting and selecting covariates due to chance.




<!--chapter:end:Forecasting-6-1-colinearity.Rmd-->

## Variable selection {#MREGVAR}

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->

In this chapter, I will illustrate developing a forecasting model using a multivariate regression (MREG).  I will show the variable selection approach that Stergiou and Christou used to develop MREG models. More background on the methods discussed in this chapter can be found in the references in the endnotes. [^MSEGref1]&nbsp;[^MSEGref2]&nbsp;[^MSEGref3]&nbsp;[^MSEGref4]&nbsp;[^ISLcv].

[^MSEGref1]: [Model selection essentials in R](http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/)

[^MSEGref2]: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.

[^MSEGref3]: Harrell, Frank. 2015. Regression modeling strategies. Springer.

[^MSEGref4]: Raftery, A. E. 1995. Bayesian model selection in social research. Sociological Methodology, 25, 111-163.


A multivariate linear regression model with Gaussian errors takes the form:

\begin{equation}
\begin{gathered}
x_t = \alpha + \phi_1 c_{t,1} + \phi_2 c_{t,2} + \dots + e_t \\
e_t \sim N(0,\sigma)
\end{gathered}
\end{equation}

In R, we can fit this model with `lm()`, which uses ordinary least squares (OLS). For model selection (determining what explanatory variables to include), there are a variety of approaches we can take.  I will show approaches that use a few different packages.

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(MASS)
library(car)
library(glmnet)
library(stringr)
library(caret)
library(leaps)
library(forecast)
library(olsrr)
```

### Model selection with stepwise variable selection {#stepwise-sel}

<!--
http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/
-->

Stergiou and Christou state that the covariates to include were selected with stepwise variable selection.  Stepwise variable selection is a type of automatic variable selection.  Stepwise variable selection has many statistical problems and the problems are worse when the covariates are collinear as they are in our case  (see this [link](https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/) for a review of the problems with stepwise variable selection). The gist of the problem is one of over-fitting. A stepwise selection procedure will tend to choose variables that, by chance, have large coefficients. With only 23 data points and high collinearity, this is likely to be a rather large problem for our dataset. As we saw, collinearity tends to cause very large positive effect sizes offset by large negative effect sizes. However I use stepwise variable selection here to replicate Stergiou and Christou.  I will follow this with an example of other more robust approaches to model selection for linear regression.

Stergiou and Christou do not give specifics on how they implemented stepwise variable selection. Stepwise variable selection refers to a forward-backward search, however there are many ways we can implement this and different approaches give different answers.  The starting model in particular will have a large effect on the ending model. 

#### step() {-}

When using the `step()` function in the stats package (and the related `stepAIC()` function in the MASS package) , we specify the starting model and the scope of the search, i.e., the smallest model and the largest model.  We set direction equal to "both" to specify stepwise variable selection. We also need to specify the selection criteria.  The default is to use AIC.

Let's start with a search that starts with a full model which has all the explanatory variables. The first argument to `step()` is the starting model and `scope` specifies the maximum and minimum models as a list. `direction="both"` is stepwise variable selection. `trace=0` turns off the reporting.

```{r stepAIC}
null <- lm(anchovy ~ 1, data=df)
full <- lm(anchovy ~ ., data=df)
step.full <- step(full, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0)
step.full
```

We can also apply `step()` with the caret package:
```{r stepAIC.caret}
step.caret <- caret::train(anchovy ~ ., data = df,
                    method = "lmStepAIC",
                    direction = "both",
                    trace = FALSE
)
step.caret$finalModel
```
Note that `method="lmStepAIC"` in the `train()` function will always start with the full model.

The AIC for this model is `r round(AIC(step.full),digits=1)`. This is a larger model than that reported in Table 3 (page 119) of Stergiou and Christou. The model in Table 3 includes only Year, *Trachurus* catch, SST, and FIP.  The model selected by `step()` starting from the full model includes Year, *Trachurus* catch, air temperature, vertical wind, BOP, FIP and TOP.

Let's repeat but start the search with the smallest model.
```{r stepAIC2}
null <- lm(anchovy ~ 1, data=df)
full <- lm(anchovy ~ ., data=df)
step.null <- step(null, 
                   scope=list(lower=null, upper=full),
                   direction="both", trace = 0)
step.null
```

This model has an AIC of `r round(AIC(step.null),digits=1)`. This AIC is larger (worse), which illustrates that you need to be careful how you set up the search. This selected model is very similar to that in Table 3 except that air temperature instead of SST is selected.  Air temperature and SST are correlated, however.

The air temperature is removed from the best model if we use BIC as the model selection criteria. This is done by setting `k=log(n)` where $n$ is sample size.

```{r stepBIC}
step.null.bic <- step(null, 
                       scope=list(lower=null, upper=full),
                       direction="both", trace = 0,
                       k=log(nrow(df)))
step.null.bic
```

We can also do stepwise variable selection using the leaps package.  However, the algorithm or starting model is different than for `step()` and the results are correspondingly different.  The top row in the plot shows the included (black) variables: Year, Trachurus, air, vwnd, FIP. The results are similar to `step()` starting from the full model but not identical.  See the next section for a brief introduction to the leaps package.
```{r leaps.step}
models <- leaps::regsubsets(anchovy~., data = df, nvmax =11,
                     method = "seqrep", nbest=1)
plot(models, scale="bic")
```

#### leaps() {-}

<!--
Raftery, A. E. (1995). Bayesian Model Selection in Social Research. Sociological Methodology, 25, 111-163.
-->

We can use the leaps package to do a full search of the model space. The function `leaps::regsubsets()` will find the `nbest` models of size (number of explanatory variables) 1 to `nvmax` using different types of searches: exhaustive, forward, backward, and stepwise variable selection.  We can then plot these best models of each size against a criteria. such as BIC.  leaps allows us to plot against BIC, Cp (asymptotically the same as AIC and LOOCV), $R^2$ and adjusted $R^2$.  Each row in the plot is a model. The dark shading shows which variables are in the model. On the y-axis, farther away from the x-axis is better, so the models (rows) at the top of the plot are the best models.

Let's start with an exhaustive search and show only the best model of each size, where size is the number of explanatory variables in the model.

```{r leaps, fig.height=5}
models <- leaps::regsubsets(anchovy~., data = df, 
                     nvmax = 11, nbest=1,
                     method = "exhaustive")
plot(models, scale="bic")
```
We see that when we use BIC as the selection criteria, the best model has Year, *Trachurus*, and FIP.

Let's look at more than one model for each model size.  Let's take the top 3 models for each model size and look at their BICs.

```{r leaps2, fig.height=8}
models <- leaps::regsubsets(anchovy~., data = df, 
                     nvmax = 11, nbest=3,
                     method = "exhaustive")
plot(models, scale="bic")
```

We can plot the BIC for each size of model also. 
```{r leaps.bic.plot, fig.height=5}
smodels = summary(models)
nvar <- apply(smodels$which,1,sum)-1
plot(nvar, smodels$bic, xlab = "Number of Variables", ylab = "BIC")
min.bic <- which.min(smodels$bic)
points(nvar[min.bic], smodels$bic[min.bic], pch = 20, col = "red")
abline(h = smodels$bic[min.bic]+2, lty=2)
```

These two plots show that there are many models within 2 of the top model.  All the best models have Year and FIP, but there are many different 3rd and 4th variables that can be added and give a similar BIC.  Interesting SST does not appear in any of the top models, while it was selected by Stergiou and Christou.  This suggests that they computed the yearly SST values slightly differently than I did.  My remote sensing data source was slightly different and that might be the cause.

#### Comparison of models chosen by AIC, AICc and BIC

`step()` uses AIC instead of the AICc (corrected for small sample size).  In our case, $n=23$ is fairly small and using AICc would be better suited for such a small dataset.  leaps does not return AIC or AICc, but we can compute them.  Note that Mallow's Cp asymptotically has the same ordering as AIC, but $n=23$ is small and it does not have the same ordering as AIC in our case.

First we use `summary()` to get a matrix showing the best model of each size.  This matrix shows what variable is in the best model of each size. Note that this best model does not depend on the metric (BIC, AIC, etc) because we are looking at models with the same number of variables.  The metric affects the penalty for different number of variables and thus only affects the models choice when we compare models of different sizes.
```{r summary.leaps}
models <- leaps::regsubsets(anchovy~., data = df, 
                     nvmax = 11, nbest=1,
                     method = "exhaustive")
smodels <- summary(models)
head(smodels$which[,1:10])
```

Next we compute AIC and AICc from BIC. `k` is the number of parameters. We need to add one more parameter for the estimated variance.

```{r leaps.aicc}
k <- apply(smodels$which,1,sum)+1
mod.aicc <- smodels$bic+k*(2+(2*k+2)/(23-k-1))-log(23)*k
mod.aic <- smodels$bic+k*2-log(23)*k
```
Now we will plot the metrics for each model size. BIC, AICc and Mallow's Cp all chose models with an intercept and 3 variables:  Year, *Trachurus* and FIP.  AIC selects a much larger model, however with $n=23$, AICc would be a better choice. 
```{r leaps.aicc.plot, echo=FALSE}
nvar <- k-2
val <- smodels$bic-min(smodels$bic)
plot(nvar, val, xlab = "Number of Variables", ylab = "BIC", type="l", lwd=2, ylim=c(0,20))
min.pt <- which.min(val)
points(nvar[min.pt], val[min.pt], pch = 19, col = "red")
val <- mod.aicc-min(mod.aicc)
lines(nvar, val, lwd=2, lty=2)
min.pt <- which.min(val)
points(nvar[min.pt], val[min.pt], pch = 15, col = "red")
val <- mod.aic-min(mod.aic)
lines(nvar, val, lwd=2, lty=3)
min.pt <- which.min(val)
points(nvar[min.pt], val[min.pt], pch = 17, col = "red")
val <- smodels$cp-min(smodels$cp)
lines(nvar, val,lwd=2, lty=4)
min.pt <- which.min(val)
points(nvar[min.pt], val[min.pt], pch = 18, col = "red")
legend("topleft",c("BIC","AICc","AIC","Cp"),lty=c(1,2,3,4),pch=c(19,15,17,18))
```

To  find the best model, find the row of the `smodels` matrix  where AICc is the smallest.  For example, here is the best model with AICc.

```{r leaps.aicc.best}
rmin <- which(mod.aicc==min(mod.aicc))
colnames(smodels$which)[smodels$which[rmin,]]
```

In comparison, the best model with AIC is larger.
```{r leaps.aic.best}
rmin <- which(mod.aic==min(mod.aic))
colnames(smodels$which)[smodels$which[rmin,]]
```

#### **olsrr** package {-}

The **olsrr** [package](https://CRAN.R-project.org/package=olsrr) provides a variety of tools for multivariate regression models, including functions for variable selection. The **olsrr** functions include nice table and plot outputs. The functions are a bit more user friendly and the package include very clear vignettes that illustrate the functions. The package includes functions for all subsets, forward and backward selection with a variety of different selection metrics.

Here is an example of one of the functions. This is for all subsets selection.

```{r olsrr.step, cache=TRUE}
ols_step_best_subset(full)
```


### Model selection with cross-validation[^ISLcv]

[^ISLcv]: This section and the R code was adapted from and influenced by [Chapter 6 material](https://lagunita.stanford.edu/c4x/HumanitiesSciences/StatLearning/asset/ch6.html) in Introduction to Statistical Learning by James, Witten, Hastie and Tibshirani. They have an online course and (free) textbook at http://www-bcf.usc.edu/~gareth/ISL/ .

<!--
https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about

Introduction to Statistical Learning
http://www-bcf.usc.edu/~gareth/ISL/

https://lagunita.stanford.edu/c4x/HumanitiesSciences/StatLearning/asset/ch6.html
-->

Variable selection (forward, backward, stepwise) is known to overfit models and variables will be chosen that just happen to have high correlation with your response variable for your particular dataset.  The result is models with low out-of-sample predictive accuracy.  Cross-validation is a way to try to deal with that problem.

Model selection with cross-validation estimates the out-of-sample predictive performance of a *process* for building a model. So for example, you could use cross-validation to ask the question, "If I select a best model with AIC does that approach led to models with better predictive performance over selecting a best model with BIC?". 

The basic idea behind cross-validation is that part of the data is used for fitting (training) the model and the left-out data is used for assessing predictions.  You predict the left-out data and compare the actual data to the predictions. There are two common types of cross-validation: leave-one-out cross-validation (LOOCV) and k-fold cross-validation.

Leave-one-out cross-validation (LOOCV) is a cross-validation where you leave one data point out, fit to the rest of the data, predict the left out data point, and compute the prediction error with prediction minus actual data value.  This is repeated for all data points.  So you will have $n$ prediction errors if you have $n$ data points.  From these errors, you can compute various statistics.  Root mean squared error (RMSE), mean squared error (MSE), and mean absolute error (MAE) are common.

k-fold cross-validation is a cross-validation where you divide the data into k equal fractions.  The model is fit k times: each fraction is treated as a test data set and the other k-1 fractions are used as the training data.  When the model is fit, you predict the data in the test data and compute the prediction errors.  Then you'll compute the statistics (RMSE, MSE, etc) from the errors from all k training sets.  There are many different ways you can split your data into k fractions.  Thus one often repeats this process many times and uses the average.  This is called repeated cross-validation.

#### Example code {-}

Let's see an example of this using models fit via stepwise variable selection using `leaps::regsubsets()`.

Let's start by defining a `predict` function for `regsubsets` objects[^predictfun].

[^predictfun]: This function was copied from the Introduction to Statistical Learning material.

```{r predict.regsubsets}
predict.regsubsets <- function(object, newdata, id, ...) {
    form <- as.formula(object$call[[2]])
    mat <- model.matrix(form, newdata)
    coefi <- leaps:::coef.regsubsets(object, id = id)
    mat[, names(coefi)] %*% coefi
}
```

Next we set up a matrix that defines the folds. Each row has numbers 1 to k (folds) which specify which data points are in the test set.  The other (non-k) data points will be the training set.  Each row of `folds` is a different replicate of the repeated cross-validation.

```{r folds.rows}
nfolds <- 5
nreps <- 20
folds <- matrix(NA, nreps, nrow(df))
for(i in 1:nreps) 
  folds[i,] <- sample(rep(1:nfolds, length = nrow(df)))
```

Now we can use `df[folds[r,]==k]` to specify the test data for the k-th fold of the r-th replicate.  And `df[folds[r,]!=k]` is the training dataset for the k-th fold of the r-th replicate. The **fold** jargon is just another word for group.  We divide the data into k groups and we call each group a **fold**. 

Next we set up a matrix to hold the prediction errors.  We will have prediction errors for each fold, each replicate, and each variable (columns).

```{r cv.err.mat}
nvmax <- 8
cv.errors <- matrix(0, nreps*nfolds, nvmax)
```

Now, we step through each replicate and each fold in each replicate.  We find the best fit with `regsubsets()` applied to the *training set* for that replicate.  Then we predict using that best fit to the test data for that replicate.  We compute the errors (prediction minus data) and store.  When we are done, we compute the RMSE (or whatever metric we want).

```{r cv.code}
for(r in 1:nreps){
 for (k in 1:nfolds) {
  traindat <- df[folds[r,]!=k,]
  testdat <- df[folds[r,]==k,]
  best.fit <- leaps::regsubsets(anchovy ~ ., data=traindat, nvmax = nvmax, method = "seqrep")
  for (i in 1:nvmax) {
    pred <- predict.regsubsets(best.fit, testdat, id = i)
    cv.errors[r+(k-1)*nreps, i] <- 
      mean((testdat$anchovy - pred)^2)
    }
 }
}
rmse.cv <- sqrt(apply(cv.errors, 2, mean, na.rm=TRUE))
plot(1:nvmax, rmse.cv, pch = 19, type = "b",xlab="Number of Variables", ylab="RMSE")
```

The model size with the best predictive performance is smaller, intercept plus 2 variables instead of intercept plus 3 variables.  This suggests that we should constrain our model size to 2 variables (plus intercept).  Note, that with a 5-fold cross-validation, we were fitting the models to 19 data points instead of 23.  However, even with a 23-fold cross-validation (Leave One Out CV), a model with 2 variables has the lowest RMSE.

The best fit 2 variable model has Year and FIP.
```{r best.fit.2}
best.fit <- leaps::regsubsets(anchovy ~ ., data=traindat, nvmax = 2, method = "seqrep")
tmp <- summary(best.fit)$which
colnames(tmp)[tmp[2,]]
```


#### Cross-validation with caret package {-}

The the `train()` function in the **caret** package allows us to fit and cross-validate model sets easily.  `trainControl` specifies the type of cross-validation and `tuneGrid` specifies the parameter over which cross-validation will be done (in this case the size of the model). 

```{r caret.plot}
library(caret)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number=5, repeats=20)
# Train the model
step.model <- train(anchovy~., data = df,
                    method = "leapSeq",
                    tuneGrid = data.frame(nvmax = 1:nvmax),
                    trControl = train.control
                    )
plot(step.model$results$RMSE, pch = 19, type = "b", ylab="RMSE")
```
The `$results` part of the output shows us the cross-validation metrics. Best depends on the metric we use.  A 2-parameter model is best for all the error metrics except R-squared. 

```{r caret.results}
step.model$results
```

The best 2-parameter model has Year and FIP.
```{r caret.1b}
coef(step.model$finalModel, id=2)
```

<!--chapter:end:Forecasting-6-2-MREG-variable-selection.Rmd-->

## Penalized regression {#MREGPR}

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->


<!--
https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
-->

The problem with model selection using searching and selecting with some model fit criteria is that the selected model tends to be over-fit---even when using cross-validation.  The predictive value of the model is not optimal because of over-fitting. Another approach to dealing with variance inflation that arises from collinearity and models with many explanatory variable is penalized regression.  The basic idea with penalized regression is that you penalize coefficient estimates that are far from 0.  The true coefficients are (likely) not 0 so fundamentally this will lead to biased coefficient estimates but the idea is that the inflated variance of the coefficient estimates is the bigger problem.  

### Ridge Regression


First, let's look at ridge regression.  With ridge regression, we will assume that the coefficients have a mean of 0 and a variance of $1/\lambda$. This is our prior on the coefficients.  The $\beta_i$ are the most probable values given the data and the prior. Note, there are many other ways to derive ridge regression.  

We will use the glmnet package to fit the anchovy catch with ridge regression.  To fit with a ridge penalty, we set `alpha=0`.

```{r glmnet.ridge}
library(glmnet)
resp <- colnames(dfz)!="anchovy"
x <- as.matrix(dfz[,resp])
y <- as.matrix(dfz[,"anchovy"])
fit.ridge <- glmnet(x, y, family="gaussian", alpha=0)
```

We need to choose a value for the penalty parameter $\lambda$ (called `s` in `coef.glmnet()`). If $\lambda$ is large, then our prior is that the coefficients are very close to 0.  If our $\lambda$ is small, then our prior is less informative.

We can use cross-validation to choose $\lambda$.  This chooses a $\lambda$ that gives us the lowest out of sample errors. `cv.glmnet()` will do k-fold cross-validation and report the MSE.  We pick the $\lambda$ with the lowest MSE (`lambda.min`) or the largest value of $\lambda$ such that error is within 1 s.e. of the minimum (`lambda.1se`).  This value is computed via cross-validation so will vary.  We will take the average over a number of runs; here 20 for speed but 100 is better.

Once we have a best $\lambda$ to use, we can get the coefficients at that value.
```{r lambda.choice.ridge}
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=0)$lambda.min
s.best.ridge <- s/n
coef(fit.ridge, s=s.best.ridge)
```

I will plot the standardized coefficients for the ordinary least squares coefficients against the coefficients using ridge regression.

```{r coef.ridge, echo=FALSE}
full <- lm(anchovy ~ ., data=dfz)
coef.ridge <- as.vector(coef(fit.ridge, s=s.best.ridge))[-1]
coef.full <- coef(full)[-1]
labs <- rownames(coef(fit.ridge, s=s.best.ridge))[-1]
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.full,coef.ridge), names.arg=labs, 
        horiz=TRUE, las=2, beside=TRUE,
        col=c("aquamarine3","coral"))
legend("topright", c("ols","ridge"), pch=15, 
       col=c("aquamarine3","coral"), 
       bty="n")
par(op)
```

This shows the problem caused by the highly collinear TOP and HPP.  They have highly inflated coefficient estimates that are offset by an inflated Year coefficient (in the opposite direction).  This is why we need to evaluate collinearity in our variables before fitting a linear regression.

With ridge regression, all the estimates have shrunk towards 0 (as they should) but the collinear variables still have very large coefficients.

### Lasso

In ridge regression, the coefficients will be shrunk towards 0 but none will be set to 0 (unless the OLS estimate happens to be 0).  Lasso is a type of regression that uses a penalty function where 0 is an option.  Lasso does a combination of variable selection and shrinkage.
 
We can do lasso with `glmnet()` by setting `alpha=1`.

```{r glmnet.lasso}
fit.lasso <- glmnet(x, y, family="gaussian", alpha=1)
```
We select the best $\lambda$ as we did for ridge regression using cross-validation.
```{r lambda.choice.lasso}
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=1)$lambda.min
s.best.lasso <- s/n
coef.lasso <- as.vector(coef(fit.lasso, s=s.best.lasso))[-1]
```

We can compare to the estimates from ridge and OLS and see that the model is now more similar the models we got from stepwise variable selection.
The main difference is that slp and air are included as variables.

```{r coef.lasso.plot, echo=FALSE}
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.ridge, coef.lasso), 
        names.arg=labs, xlim=c(-0.2,1.5),
        horiz=TRUE, las=2, beside=TRUE,
        col=c("coral", "black"))
legend("topright", c("ridge", "lasso"), pch=15, 
       col=c("coral", "black"), 
       bty="n")
par(op)
```

Lasso has estimated a model that is similar to what we got with stepwise variable selection without removing the collinear variables from our data set. 

### Elastic net

<!--
https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
-->

Elastic net is uses both L1 and L2 regularization. Elastic regression generally works well when we have a big dataset.  We do not have a big dataset but we will try elastic net.  You can tune the amount of L1 and L2 mixing by adjusting `alpha` but for this example, we will just use `alpha=0.5`.

```{r glmnet.elastic.net}
fit.en <- glmnet(x, y, family="gaussian", alpha=0.5)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=0.5)$lambda.min
s.best.el <- s/n
coef.en <- as.vector(coef(fit.en, s=s.best.el))[-1]
```


```{r coef.en.plot, echo=FALSE}
op <- par(mar=c(5, 7, 4, 2) + 0.1)
barplot(rbind(coef.ridge, coef.lasso, coef.en), 
        names.arg=labs, xlim=c(-0.2,1.5),
        horiz=TRUE, las=2, beside=TRUE,
        col=c("coral", "black", "red"))
legend("topright", c("ridge", "lasso", "elastic net"), pch=15, 
       col=c("coral", "black", "red"), 
       bty="n")
par(op)
```
As we might expect, elastic net is part way between the ridge regression model and the Lasso model.

<!--chapter:end:Forecasting-6-3-MSEG-penalized-regression.Rmd-->

## Relative importance metrics {#MREGRELPO}

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->

Another approach to linear regression with multiple collinear regressors is to compute relative importance metrics[^relimp]. The **relaimpo** package will compute the relative importance metrics and provides plotting.

[^relimp]: Groemping, U. (2006) Relative Importance for Linear Regression in R: The Package relaimpo Journal of Statistical Software 17, Issue 1. Downloadable at http://www.jstatsoft.org/v17/i01

This gives a somewhat different picture with year, Trachurus and the effort metrics most important while the environmental variables have low importance.

```{r relimp}
reli <- relaimpo::calc.relimp(anchovy~.,data=df)
plot(reli)
```

The pattern remains the same without Year as a response variable.

```{r relimp.noYear}
reli <- relaimpo::calc.relimp(anchovy~.-Year,data=df)
plot(reli)
```



<!--chapter:end:Forecasting-6-4-MREG-relative-importance.Rmd-->

## Orthogonalization {#MREGORTHO}

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->

The last approach that we will discuss for dealing with collinearity is orthogonalization. With this technique, we replace the set of collinear covariates $X$  with a set of orthogonal, i.e. independent, covariates $Z$, which are linear combinations of the original, collinear, covariates. Principal Components Analysis (PCA) is an example of using orthogonalization to deal with collinear covariates. Another example is when we use the `poly()` function to do polynomial regression. In this case, we are replacing a set of collinear variates,  $x$, $x^2$, $x^3$, etc., with a set of orthogonal covariates. 

The ways that you can create a set of orthogonal covariates is not unique. There are many different sets of orthogonal covariates. We will show three ways you might create your orthogonal set: PCA, Gram-Schmidt Orthogonalization, and residuals from linear regressions. Note, it is important to use standardized covariates with the mean removed and variance scaled to 1 when doing orthogonalization. Thus we will use `dfz` instead of `df`.

#### Principal component regression

Principal component regression is a linear regression in which you transform your collinear covariates using the othogonal variates created by Principal Components Analysis (PCA). PCA uses an orthogonal set of variates $Z$ in which the first variate accounts for as much of the variability in the variate dataset as possible, the second accounts for as much of the remaining variance as possible, the third accounts for as much of the variance remaining after the first two, etc., etc. Each variate is orthogonal to the preceding variate. 

Singular value decomposition is a standard way to compute $Z$ for PCA.


$$Z = XV$$
where $V$ is the right singular vector from the singular value decomposition of the matrix of covariates ($X=UDV'$).  The $V$ is the 'loadings' matrix in a PCA. The orthogonal covariates $Z$ are linear combinations of the original, collinear, covariates. PCA covariates have the nice feature that they are ordered in terms of the amount of variance they explain, but the orthogonal variates, called axes in a PCA, can be a bit hard to interpret.

Let's see an example with our data set. First we will create a matrix of our collinear variates.  We need to use the scaled variates.

```{r otho1}
X <- as.matrix(dfz[,colnames(dfz)!="anchovy"])
```

We create our orthogonal PCA variates using the `svd()` function which does a singular value decomposition. We will re-label the variates as 'principal components (PC)'. A corrplot shows that our variates are now uncorrelated.

```{r}
loadings <- svd(X)$v
rownames(loadings) <- colnames(X)
Z <- X%*%loadings
colnames(Z) <- paste0("PC", 1:ncol(Z))
corrplot(cor(Z))
```
These new variates are linear combinations of the original variates.  The "loadings" indicate the weight of each original variate in the new variate ("principal component").

```{r}
library(reshape2)
meltR = melt(loadings)
ggplot(meltR, aes(x=Var1, y = value)) + 
  geom_bar(stat="identity") + 
  coord_flip() + 
  facet_wrap(. ~ Var2) +
  ggtitle("Loadings")
```

The $Z$ matrix gives us a set of orthogonal variates, but some of them do not explain much of the variance. We know this should be the case because we have collinearity in our data. The singular values (which are square root of the eigenvalues of $X^\top X$) show how much of the variance in $X$ explained by each pricipal component (column in $Z$). In the plot, the singular values were of $X/\sqrt{n}$ so that $X^\top X$ is the correlation matrix.  The average singular value for a correlation matrix is 1.  With this scaling, any singlular value much less than one is small.

```{r}
sing.val <- svd(X/sqrt(n))$d
plot(sing.val, xlab="axis", ylab="singular value")
abline(h=1, col="red")
```

We could run a linear regression with all the 11 orthogonal variates (principal components), but that would not be helpful. The point of orthogonalization is to find a smaller set of variates that explains the structure in the larger set of collinear variates. Based on the singular value plot, we will use the first 2 components. These 2 capture a fair bit of the variability in the anchovy catch.

```{r}
dfpca <- data.frame(anchovy=dfz$anchovy, Z[,1:2])
pcalm <- lm(anchovy ~ ., data=dfpca)
summary(pcalm)
```

We can also plot the anchovy catch, broken into small, medium, and large, against the 2 components and see that the 2 components do separate these broad catch levels.

```{r}
library(ggplot2)
library(grid)
library(gridExtra)
df_pca <- prcomp(X)
df_out <- as.data.frame(df_pca$x)
df_out$group <- cut(dfz$anchovy,3)
p<-ggplot(df_out,aes(x=PC1,y=PC2,color=group ))
p<-p+geom_point()
p
```

We can recover the effect sizes for the original variates from $V\tilde{\beta}$. This gives a similar picture as the other methods we used. Year, TOP/HPP/BOP and Trachurus come out as important.

```{r}
coef(pcalm)
eff <- data.frame(value=svd(X)$v[,1:2] %*% coef(pcalm)[-1], var=colnames(X))
ggplot(eff, aes(x=var, y = value)) + 
  geom_bar(stat="identity") + 
  coord_flip() +
  ggtitle("Effects on scaled original vars")
```


Principal Components Regression (PCR) is a method for dimension reduction, like Lasso regression or variable selection, but your new principal components can be hard to interpret because they are linear combinations of the original variates. In addition, if you are trying to understand if a particular variate improves your model, then PCR is going to help you. Another approach for creating orthogonal variates is Gram-Schmidt orthogalization and this can help study the effect of adding specific variates.

#### Gram-Schmidt Orthogonalization

The Gram-Schmidt orthogonalization treats the variates in a specific order. The first orthogonal variate will be the first variate, the second othogonal variate will be the variation in the second variate that is not explained by the first, the third will be the variation in the third variate that is not explained by the first two, etc. This makes your orthogonal variates easier to interpret if they have some natural or desired ordering. For example, let's say we want to study if adding TOP to our model help explain variance over what is already explained by Year. Putting both TOP and Year in a model won't be helpful because they are highly correlated and we'll just get effect sizes that offset each other (one negative, one positive) with high standard errors.  Instead, we'll add Year and then add a second variate that is the variability in TOP that is not explained by Year. Why not add TOP first? The ordering is up to you and depends on the specific question you are asking.  In this case, we asking what TOP adds to a model with Year not what Year adds to a model with TOP.

We create the 
Let $Z$ be the matrix of orthogonal variates and $X$ be the matrix of original, collinear, covariates. The first column of $Z$ is $Z_1 = X_1$.
The second column of $Z$ is 
$$Z_2 = X_2 - Z_1(Z_1^\top Z_1)Z_1^\top X_2.$$
The third column of $Z$ is 
$$Z_3 = X_3 - Z_1(Z_1^\top Z_1)Z_1^\top X_3 - Z_2(Z_2^\top Z_2)Z_2^\top X_3.$$

Here is R code to create the first 3 columns of the Z matrix. The cross-product of Z is diagonal, indicating that our new variates are orthogonal.

```{r}
pr <- function(y, x){ x%*%solve(t(x)%*%x)%*%t(x)%*%y }
Z <- cbind(X[,1], 0, 0)
Z[,2] <- X[,2] - pr(X[,2], Z[,1])
Z[,3] <- X[,3] - pr(X[,3], Z[,1]) - pr(X[,3], Z[,2]) 
zapsmall(crossprod(Z))
```

To create our orthogonal variates, we have to give some thought to the ordering. Also not all the variates in our example are collinear. So we don't need to do Gram-Schimdt Orthogonalization on all the variates.  From the variance inflation factors, Year, HPP, TOP, BOP and *Trachusus* have the worst collinearity problems.

```{r vif.car2}
full <- lm(anchovy ~ ., data=df)
car::vif(full)
```

We'll do Gram-Schmidt orthogonalization on these 5. First let's resort our variates to put these 5 first.

```{r}
pr <- function(y, x){ x%*%solve(t(x)%*%x)%*%t(x)%*%y }
Z <- X[,c("Year","BOP","HPP","TOP","Trachurus","FIP","air","slp","sst","vwnd","wspd3")]
Z[,2] <- X[,2] - pr(X[,2], Z[,1])
Z[,3] <- X[,3] - pr(X[,3], Z[,1]) - pr(X[,3], Z[,2]) 
zapsmall(crossprod(Z))
```


<!--chapter:end:Forecasting-6-5-MREG-othogonalization.Rmd-->

## Prediction accuracy {#MREGPREDICT}

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->

We could use cross-validation compare prediction accuracy if we had a pre-defined set of models to compare.  In our case, we do not have a set of models but rather a set of "number of variables" and the specific variables to include in that number are determined using the fit to the data (in some fashion).  We cannot use variable selection (any sort) with our full dataset to chose the variables and then turn around and use cross-validation with the same dataset to test the out-of-sample prediction accuracy.  Anytime you double-use your data like that, you will have severe bias problems.

<!--
Discussion of this issue
https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751
-->

Instead, we will test our models using sets of years that we held out for testing, i.e. that were not used for fitting the model or selecting variates.  We will use the following test years: 1988 and 1989 as was used in Stergiou and Christou and 1988-1992 (five years).  We will use the performance testing procedure in Chapter \@ref(perf-testing).

#### Computing the prediction error for a model {-}

First we set up the test data frames.
```{r testdata2, echo=FALSE}
# 1988 and 1989 are rows 24 & 25
testdata2 <- df.full[24:25,]
```

We can then compute the RMSE for the predictions from one of our linear regression models.  Let's use the model selected by `step()` using AIC as the metric and stepwise variable regression starting from a full model, `step.full`.
```{r errs}
fr <- predict(step.full, newdata=testdata2)
err <- fr - testdata2$anchovy
sqrt(mean(err^2))
```

We could also use `forecast()` in the forecast package to compute predictions and then use `accuracy()` to compute the prediction metrics for the test data.
```{r}
fr <- forecast::forecast(step.full, newdata=testdata2)
forecast::accuracy(fr, testdata2)
```

#### Comparing the predictions for a suite of models {-}

Let's compare a suite of models and compare predictions for the full out-of-sample data that we have: 1988 to 2007.
```{r}
fr.list <- list()
testdat <- testdata.full <- df.full[24:nrow(df.full),]
n.fr <- length(testdat)
```

Then we fit the three best lm models chosen via stepwise regression, exhaustive search or cross-validation:
```{r linear.reg.model.set}
modelname <- "Year+FIP"
fit <- lm(anchovy~Year+FIP, data=df)
fr.list[[modelname]] <- predict(fit, newdata=testdat)

modelname <- "Year+Trachurus+FIP"
fit <- lm(anchovy~Year+Trachurus+FIP, data=df)
fr.list[[modelname]] <- predict(fit, newdata=testdat)

modelname <- "6 variables"
fit <- lm(anchovy~Year+air+vwnd+BOP+FIP+TOP, data=df)
fr.list[[modelname]] <- predict(fit, newdata=testdat)
```

Then we add the forecasts for Ridge Regression.
```{r}
library(glmnet)
resp <- colnames(df)!="anchovy"
x <- as.matrix(df[,resp])
y <- as.matrix(df[,"anchovy"])
fit <- glmnet(x, y, family="gaussian", alpha=0)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=0)$lambda.min
s.best <- s/n

modelname <- "Ridge Regression"
newx <- as.matrix(testdat[,resp])
fr.list[[modelname]] <- predict(fit, newx=newx, s=s.best)
```
LASSO regression,
```{r}
fit <- glmnet(x, y, family="gaussian", alpha=1)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=1)$lambda.min
s.best <- s/n

modelname <- "LASSO Regression"
newx <- as.matrix(testdat[,resp])
fr.list[[modelname]] <- predict(fit, newx=newx, s=s.best)
```

and elastic net regression.
```{r}
fit <- glmnet(x, y, family="gaussian", alpha=0.5)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=0.5)$lambda.min
s.best <- s/n

modelname <- "Elastic net Regression"
newx <- as.matrix(testdat[,resp])
fr.list[[modelname]] <- predict(fit, newx=newx, s=s.best)
```

Now we can create a table
```{r model.comp.table, results="asis"}
restab <- as.data.frame(matrix(NA,1,21))
#restab <- data.frame(model="", stringsAsFactors=FALSE)
for(i in 1:length(fr.list)){
  err <- fr.list[[i]]-testdat$anchovy
  restab[i,2:(length(err)+1)] <- sqrt(cumsum(err^2)/1:length(err))
  restab[i,1] <- names(fr.list)[i]
}
tmp <- restab[,c(1,6,11,16,21)]
colnames(tmp) <- c("model","5 yrs", "10 yrs", "15 yrs", "20 yrs")
knitr::kable(tmp)
```


If we plot the forecasts with the 1965-1987 data (open circles) and the 1988-2007 data (solid circles), we see that the forecasts continue the upward trend in the data while the data level off. 

```{r echo=FALSE}
plot(Year1+df$Year, df$anchovy, ylim=c(8,12), xlim=c(Year1,Year2), ylab="Anchovy log catch", xlab="", bty="L")
for(i in 1:length(fr.list)){
  lines(Year1+testdat$Year,fr.list[[i]], lty=i, col=i)
}
points(Year1+testdat$Year,testdat$anchovy, pch=20)
legend("topleft", names(fr.list), cex=.5, col=1:6, lty=1:6, bty="n")
```

This illustrates a problem with using "Year" as a covariate.  This covariate is deterministically increasing. If it is included in the model, then the forecasts will have an upward or downward trend.  When using environmental, biological and effort covariates, one hopes that the covariates explain the trends in the data.  It would be wiser to not use "Year" as a covariate.

LASSO regression with no year,
```{r}
resp <- colnames(df)!="anchovy" & colnames(df)!="Year"
x <- as.matrix(df[,resp])
y <- as.matrix(df[,"anchovy"])
fit.lasso <- glmnet(x, y, family="gaussian", alpha=1)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=1)$lambda.min
s.best.lasso <- s/n

modelname <- "LASSO Reg no Year"
newx <- as.matrix(testdat[,resp])
fr.list[[modelname]] <- predict(fit.lasso, newx=newx, s=s.best.lasso)
```

Ridge regression with no year,
```{r}
resp <- colnames(df)!="anchovy" & colnames(df)!="Year"
x <- as.matrix(df[,resp])
y <- as.matrix(df[,"anchovy"])
fit.ridge <- glmnet(x, y, family="gaussian", alpha=0)
n <- 20; s <- 0
for(i in 1:n) s <- s + cv.glmnet(x, y, nfolds=5, alpha=1)$lambda.min
s.best.ridge <- s/n

modelname <- "Ridge Reg no Year"
newx <- as.matrix(testdat[,resp])
fr.list[[modelname]] <- predict(fit.ridge, newx=newx, s=s.best.ridge)
```

Now we can create a table
```{r model.comp.table2, results="asis", echo=FALSE}
restab <- as.data.frame(matrix(NA,1,21))
#restab <- data.frame(model="", stringsAsFactors=FALSE)
for(i in 1:length(fr.list)){
  err <- fr.list[[i]]-testdat$anchovy
  restab[i,2:(length(err)+1)] <- sqrt(cumsum(err^2)/1:length(err))
  restab[i,1] <- names(fr.list)[i]
}
tmp <- restab[,c(1,6,11,16,21)]
colnames(tmp) <- c("model","5 yrs", "10 yrs", "15 yrs", "20 yrs")
knitr::kable(tmp[7:8,])
```

```{r echo=FALSE}
plot(Year1+df$Year, df$anchovy, ylim=c(8,12), xlim=c(Year1,Year2), ylab="Anchovy log catch", xlab="", bty="L")
for(i in 7:length(fr.list)){
  lines(Year1+testdat$Year,fr.list[[i]], lty=i, col=i)
}
points(Year1+testdat$Year,testdat$anchovy, pch=20)
legend("topleft", names(fr.list)[7:8], cex=.5, col=7:8, lty=7:8, bty="n")
```
Without "Year", the model predicts 1988 well (using 1987 covariates) but then has a large jump upward after which is has a similar "flat-ish" trend as seen after 1989.  What happened in 1988 (the covariate year affecting 1989)?  The horsepower covariate, along with BOP (total boats) and TOP (boat tonnage), have a sudden upward jump in 1988. This is seen in all the fisheries.  This suggests that in 1988 either a large number of new boats entered all the fisheries or what boats were counted as "purse seiners" was changed. Upon looking at the covariates, it seems that something changed in the recording from 1988 to 1996.

```{r echo=FALSE}
par(mfrow=c(2,2),mar=c(2,2,2,2))
plot(greekfish.cov$Year,greekfish.cov$Beach.seiners.BOB,ylab="",xlab="")
title("Beach Seiner Boats")
abline(v=1988)
abline(v=1996)
plot(greekfish.cov$Year,greekfish.cov$Trawlers.BOT,ylab="",xlab="")
title("Trawler Boats")
abline(v=1988)
abline(v=1996)
plot(greekfish.cov$Year,greekfish.cov$Other.BOC,ylab="",xlab="")
title("Other Boats")
abline(v=1988)
abline(v=1996)
plot(greekfish.cov$Year,greekfish.cov$Purse.seiners.BOP,ylab="",xlab="")
title("Purse Seiner Boats")
abline(v=1988)
abline(v=1996)

```

If we correct that jump from 1988 to 1989 (subtract the jump from all data 1989 onward), the Lasso and Ridge predictions without year look considerably better.

```{r echo=FALSE}
testdatc <- testdat
testdatc$BOP[(testdat$Year+1964)>1988]<-testdatc$BOP[(testdat$Year+1964)>1988]-0.083
testdatc$HPP[(testdat$Year+1964)>1988]<-testdatc$HPP[(testdat$Year+1964)>1988]-0.28285
testdatc$TOP[(testdat$Year+1964)>1988]<-testdatc$TOP[(testdat$Year+1964)>1988]-0.295
save(df, df.full, testdatc, file="MREG_Data.RData")
```

```{r echo=FALSE}
modelname <- "LASSO Reg no Year jump corrected"
newx <- as.matrix(testdatc[,resp])
fr.list[[modelname]] <- predict(fit.lasso, newx=newx, s=s.best.lasso)

modelname <- "Ridge Reg no Year jump corrected"
newx <- as.matrix(testdatc[,resp])
fr.list[[modelname]] <- predict(fit.ridge, newx=newx, s=s.best.ridge)
```

```{r echo=FALSE}
mnames <- c("LASSO Reg no Year jump corrected", "Ridge Reg no Year jump corrected")
plot(Year1+df$Year, df$anchovy, ylim=c(8,12), xlim=c(Year1,Year2), ylab="Anchovy log catch", xlab="", bty="L")
for(i in 1:2){
  lines(Year1+testdat$Year,fr.list[[mnames[i]]], lty=i, col=i)
}
points(Year1+testdat$Year,testdat$anchovy, pch=20)
legend("topleft", mnames, cex=.5, col=1:2, lty=1:2, bty="n")
```


<!--chapter:end:Forecasting-6-6-MREG-prediction-error.Rmd-->

## Discussion

<!--
if(file.exists("Fish-Forecast.Rmd")) file.remove("Fish-Forecast.Rmd")
bookdown::preview_chapter("Forecasting-6-2-Covariates-MSEG.Rmd")
-->

This chapter illustrates a variety of approaches for "variable selection".  This is the situation where one has a large number of covariates and one wants to chose the covariates that produce the best predictions.  Following Stergiou and Christou, I used mainly linear regressions with variables selected with stepwise variable selection.

Keep in mind that stepwise variable selection is generally considered data-dredging and a reviewer who is statistician will almost certainly find fault with this approach. Penalized regression is a more accepted approach for developing a regression model with many covariates.  Part of the appeal of penalized regression is that it is robust to collinearity in your covariates.  Stepwise variable regression is not robust to collinearity.

Cross-validation is an approach for testing a *process* of building a model. In the case of the anchovy data, a model with only two covariates, Year and number of fishers, was selected via cross-validation as having the best (lowest) predictive error.  This is considerable smaller than the best model via stepwise variable selection.

When we tested the models against data completely held out of the analysis and model development (1988-2007), we discovered a number of problems. 1) Using "Year" as a covariate is a bad idea since it is deterministically linear upward. and 2) There is a problem with the effort data between 1988 and 1996.  There is a jump in the effort data.

We used variable selection or penalized regression to select weighting on a large set of covariates.  Another approach is to develop a set of covariates from your knowledge of the system and use only covariates that are thought to be important.  In Section 4.7.7 of  (Harrell 2015), a rule of thumb (based on shrinkage) for the number of predictors that can be used without overfitting is given by: $(LR-p)/9$ where $LR$ is the likelihood ratio test $\chi^2$ of the full model against the null model with only intercept and $p$ is the number of variables in the full model.

```{r eval.shrinkage}
null <- lm(anchovy ~ 1, data=df)
full <- lm(anchovy ~ ., data=df)
a <- lmtest::lrtest(null, full)
(a$Chisq[2]-a$Df[2])/9
```

This rule of thumb suggests that we could include six variables. Another approach to model building would be to select environmental and biological variables based on the known biology of anchovy and to select one effort variable or a composite "effort" based on a combination of the effort variables.


<!--chapter:end:Forecasting-6-7-MREG-discussion.Rmd-->

<!--
## Autocorrelated Data in Regression Models

Create some simulated data for a linear regression against one predictor variable:
$$y_i = \alpha + \beta x_i + e_i \\e_i \sim N(0,\sigma)$$
The errors $e_i$ are i.i.d.  These $y_i$ perfectly fit the assumption in a generic linear regression model.

```{r}
n <- 100
alpha <- 1
beta <- 2
cov <- rnorm(n)
err <- rnorm(n, 0, 1)
y <- alpha + beta*cov + err
plot(cov, y)
```

Now fit a linear regression and look at the standard errors for the parameter estimates.

```{r}
dat <- data.frame(y=y, x=cov)
fit <- lm(y~x, data=dat)
summary(fit)
```

Let's imagine that each $i$ is a site where we have measured $y_i$ and $x_i$.  Let's imagine that instead of measuring $y$ and $x$ once at each site, we measure $y$ and $x$ three times.  Let's imagine also that the error $e$ is site dependent, i.e. that there is a $e_i$ for each site, and the $x$ are the same at each site.  So that $y_{i,1}=y_{i,2}=y_{i,3}$.  Obviously, the $y$ are not independent of each other now.  The $y_{i,j}$ are all exactly the same.  Let's see what happens when we estimate the parameters.

We'll create our data by replicating $y$ and $x$ three times.  We'll randomize the order too.
```{r}
ord <- sample(3*n) #random order
y3 <- c(y,y,y)[ord]
cov3 <- c(cov, cov, cov)[ord]
dat3 <- data.frame(y=y3, x=cov3)
fit3 <- lm(y~x, data=dat3)
summary(fit3)
```

The standard errors of our parameters have gone down.  But that is not correct since we do not actually have $3\times n$ data points.  We really have $n$ data points.

We can see the problem in a histogram of the residuals and a plot of residuals against covariate value.

```{r}
par(mfrow=c(1,2))
resids <- residuals(fit3)
hist(resids, main="not normal")
plot(resids[order(cov3)], main="resids vs cov")
```

The residuals also do not pass a normality test---although that can happen for many reasons.

```{r}
shapiro.test(residuals(fit))
```

This was an extreme example where $e_{i,1}=e_{i,2}=e_{i,3}$, but the same problem would arise if the $e_i$ were less than perfectly correlated.

### Linear regression of autoregressive data

A similar problem arises when our data are a time-series and are autocorrelated---in addition to being a function of the covariate.  

$$y_t = \alpha + \beta x_t + \phi y_{t-1} + e_t$$
If $\phi$ is close to 0, then there is not much autocorrelation and we don't see a problem in the ACF.

```{r}
phi <- 0.1
yar <- y[2:n]+phi*y[1:(n-1)]
acf(yar)
```

If $\phi$ is close to 1, then the autocorrelation is apparent.


```{r}
phi <- 0.8
yar <- y[2:n]+phi*y[1:(n-1)]
acf(yar)
```

If $\phi$ is close to 1, our relation between $y$ and $x$ also looks peculiar and non-linear.

```{r}
par(mfrow=c(1,2))
plot(y,cov, main="uncorrelated y")
plot(yar, cov[2:n], main="correlated y")
```

Now fit:

```{r}
datar <- data.frame(y=yar, x=cov[2:n])
fitar <- lm(y ~ x, data=datar)
summary(fitar)
```


Let's fit to $y_{1:n-1}$ and $yar$

### Autocorrelated predictor variable

```{r}
covar <- as.vector(arima.sim(n, model=list(ar=0.7)))
y2 <- alpha + beta*covar + err
dat2 <- data.frame(y=y2, x=covar)
```
-->

<!--chapter:end:Forecasting-6-9-Covariates-AR-data-not-used.Rmd-->


```{r f70-load_packages, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
```

# AR models with covariates


<!--chapter:end:Forecasting-7-0-AR-Covariates.Rmd-->

## MREG with ARMA errors {#MREGARMA}

```{r f62-load_packages, message=FALSE, warning=FALSE}
library(ggplot2)
library(forecast)
library(astsa)
library(nlme)
```


The `stats::arima()` and `forecast::auto.arima()` functions with argument `xreg` fit a multivariate linear regression with ARMA errors.  Note, this is not what is termed a ARMAX model. ARMAX models will be addressed in Section \@ref(ARMAX).  

The model fitted when `xreg` is passed in is:

\begin{equation}
\begin{gathered}
x_t = \alpha + \phi_1 c_{t,1} + \phi_2 c_{t,2} + \dots + z_t \\
z_t = \beta_1 z_{t-1} + \dots + \beta_p z_{t-p} + e_t + \theta_1 e_{t-1} + \dots + \theta_q e_{t-q}\\
e_t \sim N(0,\sigma)
\end{gathered}
\end{equation}
where `xreg` is  matrix with $c_{t,1}$ in column 1, $c_{t-2}$ in column 2, etc.  $z_t$ are the ARMA errors.

### Example: fitting with auto.arima

Let's fit two of the best multivariate regression models from Section \@ref(stepwise-sel) with ARMA errors.  We can use `auto.arima` to search for an ARMA model for the residuals.

```{r mreg.auto.arima}
xreg <- as.matrix(df[,c("Year","FIP")])
forecast::auto.arima(df$anchovy, xreg=xreg)
```

The esimated model is a "Regression with ARIMA(0,0,0) errors" which indicates no autoregressive or moving average pattern in the residuals.  We can also see this by looking at an ACF plot of the residuals.


```{r mreg.acf}
lm(anchovy~Year+FIP,data=df) %>%
  resid %>%
  acf
```
The same pattern is seen with the models with more variables.

```{r mreg.arma.2}
xreg <- as.matrix(df[,c("Year","Trachurus","FIP")])
forecast::auto.arima(df$anchovy, xreg=xreg)
```

### Example: fitting with arima and sarima

If we want to fit a specific ARMA model, for example an AR(1) model for the residuals, we can use `arima`.

```{r mreg.arima}
xreg <- as.matrix(df[,c("Year","FIP")])
arima(df$anchovy, xreg=xreg, order = c(1,0,0))
```

We can also use the `sarima` function in the **astsa** package.  This plots a nice diagnostics plot with the fit.

```{r mreg.sarima}
xreg <- as.matrix(df[,c("Year","FIP")])
astsa::sarima(df$anchovy, 1, 0, 0, xreg=xreg)
```
### Example: fitting with gls

We can also fit multivariate regression with autocorrelated errors with the nlme package and function `gls()`. The default fitting method is REML, and to get the same results as `arima()`, we need to specify `method="ML"`.

```{r mreg.arma.gls}
mod <- gls(anchovy~Year+FIP, data=df, correlation=corAR1(form=~1), method="ML")
summary(mod)
```

You can also fit an AR(2) or ARMA with `gls()`:

```{r mreg.arma.gls2}
mod <- gls(anchovy~Year+FIP, data=df, correlation=corARMA(form = ~1,p=2,q=0), method="ML")
summary(mod)
```

### MREG of first or second differences

In the multivariate regression with ARMA errors, the response variable $x_t$ is not necessarily stationary since the covariates $c_t$'s need not be stationary.  If we wish to model the first or second differences of $x_t$, then we are potentially modeling a stationary process if differencing leads to a stationary process.
We need to think carefully about how we set up a multivariate regression if our response variable is stationary.

One recommendation is if $x_t$ is differenced, the same differencing is applied to the covariates.  The idea is if the response variable is stationary, we want to make sure that the independent variables are also stationary.  However, in a fisheries application $x_t - x_{t-1}$ often has a biological meaning, the yearly (or monthly or hourly) rate of change, and that rate of change is what one is trying explain with a covariate.  One would not necessarily expect the first difference to be stationary and one is trying to explain any trend in the one-step rate of change with some set of covariates.  On the other hand, if the response variable, the raw data or the first or second difference, is stationary then trying to explain its variability via a non-stationary covariate will clearly lead to the effect size of the covariates being zero.  We don't need to fit a model to tell us that.

### Discussion

R provides many different functions and packages for fitting a multivariate regression with autoregressive errors.  In the case of the anchovy time series, the errors are not autoregressive. In general, the first step to determining whether a model with correlated errors is required is to look at diagnostics for the residuals.  Select a model (see previous section) and then examine the residuals for evidence of autocorrelation. However another approach is to include a model with autocorrelated errors in your model set and compare via model selection. If this latter approach is taken, you must be careful to that the model selection criteria (AIC, BIC etc) are comparable. If you use functions from different packages, they authors have often left off a constant in their model selection criteria formulas. If you need to use different packages, you will carefully test the model selection criteria from the same model with different functions and adjust for the missing constants.

<!--chapter:end:Forecasting-7-1-MREG-ARMA-errors.Rmd-->

## ARMAX Models {#ARMAX}

```{r f72-load_packages, message=FALSE, warning=FALSE}
library(MARSS)
```


The `stats::arima()` and `forecast::auto.arima()` functions with argument `xreg` fit a multivariate linear regression with ARMA errors.  Note, this is not what is termed a ARMAX model. ARMAX models will be addressed separately.  

[^Hyndsight]:https://robjhyndman.com/hyndsight/arimax/

The model fitted when `xreg` is passed in is:

\begin{equation}
\begin{gathered}
x_t = \alpha + \phi_1 c_{t,1} + \phi_2 c_{t,2} + \dots + z_t \\
z_t = \beta_1 z_{t-1} + \dots + \beta_p z_{t-p} + e_t + \theta_1 e_{t-1} + \dots + \theta_q e_{t-q}\\
e_t \sim N(0,\sigma)
\end{gathered}
\end{equation}
where `xreg` is  matrix with $c_{t,1}$ in column 1, $c_{t-2}$ in column 2, etc.  $z_t$ are the ARMA errors.



### Discussion

R provides many different functions and packages for fitting a multivariate regression with autoregressive errors.  In the case of the anchovy time series, the errors are not autoregressive. In general, the first step to determining whether a model with correlated errors is required is to look at diagnostics for the residuals.  Select a model (see previous section) and then examine the residuals for evidence of autocorrelation. However another approach is to include a model with autocorrelated errors in your model set and compare via model selection. If this latter approach is taken, you must be careful to that the model selection criteria (AIC, BIC etc) are comparable. If you use functions from different packages, they authors have often left off a constant in their model selection criteria formulas. If you need to use different packages, you will carefully test the model selection criteria from the same model with different functions and adjust for the missing constants.

<!--chapter:end:Forecasting-7-2-ARMAX.Rmd-->

# Seasonality

To work with seasonal data, we need to turn our data into a ts object, which is a "time-series" object in R. This will allow us to specify the seasonality.  It is important that we do not leave out any data in our time series.  You data should look like so

```
  Year   Month  metric.tons
 2018   1           1
 2018   2           2
 2018   3           3
 ...   
 2019   1           4
 2019   2           6
 2019   3          NA
```

The months are in order and the years are in order.

## Chinook data

We will illustrate the analysis of seasonal catch data using a data set of monthly chinook salmon from Washington state.


### Load the chinook salmon data set {-}

This is in the **FishForecast** package.
```{r f80-load_packages}
require(FishForecast)
head(chinook.month)
```


The data are monthly and start in January 1990.  To make this into a ts object do

```{r}
chinookts <- ts(chinook.month$log.metric.tons, start=c(1990,1), frequency=12)
```
`start` is the year and month and frequency is the number of months in the year.  If we had quarterly data that started in 2nd quarter of 1990, our call would be
```
ts(chinook$log.metric.tons, start=c(1990,2), frequency=4)
```
If we had daily data starting on hour 5 of day 10 and each row was an hour, our call would be
```
ts(chinook$log.metric.tons, start=c(10,5), frequency=24)
```

Use `?ts` to see more examples of how to set up ts objects.


### Plot seasonal data {-}

Now that we have specified our seasonal data as a ts object, it is easy to plot because R knows what the season is.

```{r}
plot(chinookts)
```


## Seasonal Exponential Smoothing Model

Now we add a few more lines to our ETS table of models:

model  | "ZZZ" | alternate function |
------------- | ------------- | --------- |
exponential smoothing no trend | "ANN" | `ses()` |
exponential smoothing with trend  | "AAN" | `holt()` |
exponential smoothing with season no trend  | "ANA" | NA |
exponential smoothing with season and trend  | "AAA" | NA |
estimate best trend and season model | "ZZZ" | NA |

Unfortunately `ets()` will not handle missing values and will find the longest continuous piece of our data and use that.


```{r}
library(forecast)
traindat <- window(chinookts, c(1990,1), c(1999,12))
fit <- forecast::ets(traindat, model="AAA")
fr <- forecast::forecast(fit, h=24)
plot(fr)
points(window(chinookts, c(1996,1), c(1996,12)))
```


### Force seasonality to evolve more

If we plot the decomposition, we see the the seasonal component is not changing over time, unlike the actual data.  The bar on the right, alerts us that the scale on the 3rd panel is much smaller.

```{r fig.height=4}
autoplot(fit)
```

Pass in a high `gamma` (the season weighting) to force the seasonality to evolve.
```{r fig.height=4}
fit <- forecast::ets(traindat, model="AAA", gamma=0.4)
autoplot(fit)
```

---

## Seasonal ARIMA model

`auto.arima()` will recognize that our data has season and fit a seasonal ARIMA model to our data by default.  Let's use the data that `ets()` used.  This is shorter than our training data and is Oct 1990 to Dec 1995.  The data used by `ets()` is returned in `fit$x`.

We will redefine the training data to be the longest segment with no missing values.

```{r}
traindat <- window(chinookts, c(1990,10), c(1995,12))
testdat <- window(chinookts, c(1996,1), c(1996,12))
fit <- forecast::auto.arima(traindat)
fr <- forecast::forecast(fit, h=12)
plot(fr)
points(testdat)
```


## Missing values

Unlike for an exponential smoothing model, missing values are ok when fitting a seasonal ARIMA model

```{r}
fulldat <- window(chinookts, c(1990,1), c(1999,12))
fit <- forecast::auto.arima(fulldat)
fr <- forecast::forecast(fit, h=12)
plot(fr)
```


## Forecast evaluation

We can compute the forecast performance metrics as usual. 

```{r}
fit.ets <- forecast::ets(traindat, model="AAA")
fr <- forecast::forecast(fit.ets, h=12)
```
Look at the forecast so you know what years and months to include in your test data.  Pull those 12 months out of your data using the `window()` function.

```{r}
testdat <- window(chinookts, c(1996,1), c(1996,12))
```
Use `accuracy()` to get the forecast error metrics.
```{r}
forecast::accuracy(fr, testdat)
```


We can do the same for the ARIMA model.

```{r}
fit <- forecast::auto.arima(traindat)
fr <- forecast::forecast(fit, h=12)
forecast::accuracy(fr, testdat)
```



<!--chapter:end:Forecasting-8-0-Seasonality.Rmd-->

`r if (knitr::is_html_output()){ '
# References {-}
<div id="refs"></div>
'}`

<!--chapter:end:Forecasting-9-0-References.Rmd-->

# (APPENDIX) Appendix {-}

# Inputting data

This chapter will illustrate how to input data that is stored in csv files in various common formats.

### one response variable {-}

If your data look like this:

```
  Year Species metric.tons
 2018,   Fish1,           1
 2019,   Fish1,           2
 2018,   Fish2,           3
 2019,   Fish2,           4
 2018,   Fish3,           6
 2019,   Fish4,          NA
```
with this code:

```
test <- read.csv("Data/test.csv", stringsAsFactors = FALSE)
save(test, file="test.RData")
```

---

### Many response variables {-}

Read in a file where the data are in columns.  If your data look like this with each species (or site) across the columns:

```
Year,Anchovy,Sardine,Chub mackerel,Horse mackerel,Mackerel,Jack Mackerel
1964,5449.2,12984.4,1720.7,4022.4,NA,NA
1965,4263.5,10611.1,1278.5,4158.3,NA,NA
1966,5146.4,11437.8,802.6,3012.1,NA,NA
```

Use this code:

```
test <- read.csv("Data/test.csv", stringsAsFactors = FALSE)
reshape2::melt(test, id="Year", value.name="metric.tons", variable.name="Species")
save(test, file="test.RData")
```

---

### Many response variables, two time variables {-}

If your data also have, say, a month (or qtr) column, use this code:

```
Year,Month,Anchovy,Sardine,Chub mackerel,Horse mackerel,Mackerel,Jack Mackerel
1964,1,5449.2,12984.4,1720.7,4022.4,NA,NA
1964,2,4263.5,10611.1,1278.5,4158.3,NA,NA
1964,3,5146.4,11437.8,802.6,3012.1,NA,NA
```
Use this code:

```
test <- read.csv("Data/test.csv", stringsAsFactors = FALSE)
reshape2::melt(test, id=c("Year","Month"), value.name="metric.tons", variable.name="Species")
save(test, file="test.RData")
```

---

### One response variable, multiple explanatory variables {-}

```
Year, Anchovy, SST,  Mackerel
1964, 5449.2,  24.4, 1720.7
1965, 4263.5,  30.1, 1278.5
1966, 5146.4,  23.8,  802.6
```

Use this code:

```
test <- read.csv("Data/test.csv", stringsAsFactors = FALSE)
save(test, file="test.RData")
```

Use this `lm()` model (or gam() etc):

```
fit <- lm(Anchovy ~ SST + Mackerel, data=test)
```

<!--chapter:end:Forecasting-9-1-Inputting-Data.Rmd-->

# Downloading ICOADS covariates

The covariates are those in Stergiou and Christou except that NS winds might not be vertical wind.  I used the ICOADS data not the COADSs.  The boxes are 1 degree but on 1 degree centers not 0.5 centers.  Thus box is 39.5-40.5 not 39-40.


The following is the code used to download the covariates from the NOAA ERDDAP server. It creates a list with the monthly data for each box.

```{r, eval=FALSE}
library(RCurl)
library(XML)
library(stringr)
lat <- c(39,39,40)
lon <- c(24,25,25)
covs <- list()
for(i in 1:3){
  loc <- paste0("[(",lat[i],".5):1:(",lat[i],".5)][(",lon[i],".5):1:(",lon[i],".5)]")
  url <- paste0("https://coastwatch.pfeg.noaa.gov/erddap/griddap/esrlIcoads1ge.htmlTable?air[(1964-01-01):1:(2018-08-01T00:00:00Z)]",loc,",slp[(1964-01-01):1:(2018-08-01T00:00:00Z)]",loc,",sst[(1964-01-01):1:(2018-08-01T00:00:00Z)]",loc,",vwnd[(1964-01-01):1:(2018-08-01T00:00:00Z)]",loc,",wspd3[(1964-01-01):1:(2018-08-01T00:00:00Z)]",loc)
doc <- getURL(url)
cov <- readHTMLTable(doc, which=2, stringsAsFactors=FALSE)
coln <- paste0(colnames(cov),".",cov[1,])
coln <- str_replace(coln, "\n", "")
coln <- str_replace_all(coln, "[*]", "")
cov <- cov[-1,]
colnames(cov) <- coln
cov[,1] <- as.Date(cov[,1])
for(j in 2:dim(cov)[2]) cov[,j] <- as.numeric(cov[,j])
covs[[i]] <- cov
}
```

Now create the monthly and yearly means.

```{r eval=FALSE}
covsmean <- covs[[1]]
for(j in 2:dim(cov)[2])
  covsmean[,j] <- apply(cbind(covs[[1]][,j], covs[[2]][,j], covs[[3]][,j]),1,mean,na.rm=TRUE)
covsmean <- covsmean[,c(-2,-3)]
covsmean$Year <- as.factor(format(cov[,1],"%Y"))
covsmean.mon <- covsmean
covsmean.year <- data.frame(Year=unique(covsmean$Year))
for(j in 2:(dim(covsmean)[2]-1)) covsmean.year <- cbind(covsmean.year, tapply(covsmean[,j], covsmean$Year, mean, na.rm=TRUE))
colnames(covsmean.year) <- c("Year",colnames(covsmean)[2:(dim(covsmean)[2]-1)])
```



<!--chapter:end:Forecasting-9-2-Downloading-ICOADS.Rmd-->

